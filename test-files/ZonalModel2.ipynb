{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ZonalModel2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "djgc1AVtrAdy",
        "q_J2CFR8rljZ"
      ],
      "authorship_tag": "ABX9TyMlf90jTjjIboNsN/u3U6QO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ctorney/learning-to-simulate-tf2/blob/main/test-files/ZonalModel2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3TcOyZ_njXA"
      },
      "source": [
        "!pip install \"graph_nets>=1.1\" \"dm-sonnet>=2.0.0b0\" \"tensorflow_probability\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XemapxUWfYJZ",
        "cellView": "form"
      },
      "source": [
        "#@title ### Imports\n",
        "\n",
        "import numpy as np\n",
        "from math import *\n",
        "import sys\n",
        "import random\n",
        "import time\n",
        "\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "import functools\n",
        "\n",
        "from sklearn import neighbors\n",
        "\n",
        "from typing import Callable\n",
        "\n",
        "import graph_nets as gn\n",
        "import sonnet as snt\n",
        "import collections\n",
        "\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation, rc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djgc1AVtrAdy"
      },
      "source": [
        "# Generate Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_lpv__-npkr",
        "cellView": "form"
      },
      "source": [
        "#@title ### Zonal Model\n",
        "\n",
        "def get_record(group_id,pos,vel):\n",
        "    feature = { 'group_id': tf.train.Feature(int64_list=tf.train.Int64List(value=[group_id])),\n",
        "                'pos': tf.train.Feature(bytes_list=tf.train.BytesList(value=[pos.numpy()])),\n",
        "                'vel': tf.train.Feature(bytes_list=tf.train.BytesList(value=[vel.numpy()]))\n",
        "                }\n",
        "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "\n",
        "\n",
        "class zonal_model:\n",
        "    def __init__(self, N, timesteps, discard, repeat, L, dt, save_interval,train_directory='train_datasets', valid_directory='valid_datasets', disable_progress=False):\n",
        "        self.N = N\n",
        "        self.timesteps = timesteps\n",
        "        self.discard = discard\n",
        "        self.B = repeat  # repeat for B batches\n",
        "        self.L = L\n",
        "        self.dt = dt\n",
        "        self.save_interval = save_interval\n",
        "        \n",
        "        self.micro_state = np.zeros((self.B, (self.timesteps - self.discard)//self.save_interval, N, 4),dtype=np.float32)\n",
        "\n",
        "        self.sim_counter=0\n",
        "\n",
        "        if not os.path.exists(train_directory):\n",
        "            os.makedirs(train_directory)\n",
        "\n",
        "        if not os.path.exists(valid_directory):\n",
        "            os.makedirs(valid_directory)\n",
        "\n",
        "        self.train_directory = train_directory\n",
        "        self.valid_directory = valid_directory\n",
        "\n",
        "        # turn progress bar on or off\n",
        "        self.disable_progress = disable_progress\n",
        "\n",
        "        self.valid_fraction = 0.1\n",
        "        \n",
        "    def initialise_state(self):\n",
        "\n",
        "        self.positions = tf.random.uniform((self.B,self.N,2),0.5*self.L, 0.5*self.L+20) #0,self.L)\n",
        "        #self.positions = tf.random.uniform((self.B,self.N,2),0, self.L) \n",
        "        self.angles = tf.random.uniform((self.B,self.N,1), 0, 2*pi) #\n",
        "        \n",
        "\n",
        "\n",
        "    def run_sim(self, *params):\n",
        "\n",
        "        eta, Ra, Ro, Rr, vs, va, sigma = params\n",
        "        \n",
        "        record_file = self.train_directory + '/microstates-' + str(self.sim_counter) + '.tfrecords'\n",
        "        self.writer = tf.io.TFRecordWriter(record_file) \n",
        "        \n",
        "        valid_file = self.valid_directory + '/microstates-' + str(self.sim_counter) + '.tfrecords'\n",
        "        self.validwriter = tf.io.TFRecordWriter(valid_file) \n",
        "        \n",
        "        # tensorflow function to run an update step\n",
        "        @tf.function\n",
        "        def update_tf(X, A):\n",
        "            cos_A = tf.math.cos(A)\n",
        "            sin_A = tf.math.sin(A)\n",
        "\n",
        "\n",
        "            Xx = tf.expand_dims(X[...,0],-1)\n",
        "            dx = -Xx + tf.linalg.matrix_transpose(Xx)\n",
        "            dx = tf.where(dx>0.5*self.L, dx-self.L, dx)\n",
        "            dx = tf.where(dx<-0.5*self.L, dx+self.L, dx)\n",
        "\n",
        "            Xy = tf.expand_dims(X[...,1],-1)\n",
        "            dy = -Xy + tf.linalg.matrix_transpose(Xy)\n",
        "            dy = tf.where(dy>0.5*self.L, dy-self.L, dy)\n",
        "            dy = tf.where(dy<-0.5*self.L, dy+self.L, dy)\n",
        "\n",
        "\n",
        "            angle_to_neigh = tf.math.atan2(dy, dx)\n",
        "            cos_N = tf.math.cos(angle_to_neigh)\n",
        "            sin_N = tf.math.sin(angle_to_neigh)\n",
        "            rel_angle_to_neigh = angle_to_neigh - A\n",
        "            rel_angle_to_neigh = tf.math.atan2(tf.math.sin(rel_angle_to_neigh), tf.math.cos(rel_angle_to_neigh))\n",
        "            \n",
        "            dist = tf.math.sqrt(tf.square(dx)+tf.square(dy))\n",
        "    \n",
        "            # repulsion \n",
        "            rep_x = tf.where(dist<=Rr, -dx, tf.zeros_like(dx))\n",
        "            rep_x = tf.where(rel_angle_to_neigh<0.5*va, rep_x, tf.zeros_like(rep_x))\n",
        "            rep_x = tf.where(rel_angle_to_neigh>-0.5*va, rep_x, tf.zeros_like(rep_x))\n",
        "            rep_x = tf.math.divide_no_nan(rep_x,tf.math.square(dist))\n",
        "            rep_x = tf.reduce_sum(rep_x,axis=2)\n",
        "\n",
        "            rep_y = tf.where(dist<=Rr, -dy, tf.zeros_like(dy))\n",
        "            rep_y = tf.where(rel_angle_to_neigh<0.5*va, rep_y, tf.zeros_like(rep_y))\n",
        "            rep_y = tf.where(rel_angle_to_neigh>-0.5*va, rep_y, tf.zeros_like(rep_y))\n",
        "            rep_y = tf.math.divide_no_nan(rep_y,tf.math.square(dist))\n",
        "            rep_y = tf.reduce_sum(rep_y,axis=2)\n",
        "\n",
        "            # alignment \n",
        "            align_x = tf.where(dist<=Ro, cos_A, tf.zeros_like(cos_A))\n",
        "            align_x = tf.where(rel_angle_to_neigh<0.5*va, align_x, tf.zeros_like(align_x))\n",
        "            align_x = tf.where(rel_angle_to_neigh>-0.5*va, align_x, tf.zeros_like(align_x))\n",
        "            align_x = tf.reduce_sum(align_x,axis=1)\n",
        "            \n",
        "            align_y = tf.where(dist<=Ro, sin_A, tf.zeros_like(sin_A))\n",
        "            align_y = tf.where(rel_angle_to_neigh<0.5*va, align_y, tf.zeros_like(align_y))\n",
        "            align_y = tf.where(rel_angle_to_neigh>-0.5*va, align_y, tf.zeros_like(align_y))\n",
        "            align_y = tf.reduce_sum(align_y,axis=1)\n",
        "\n",
        "            al_norm = tf.math.sqrt(align_x**2+align_y**2)\n",
        "            align_x = tf.math.divide_no_nan(align_x,al_norm)\n",
        "            align_y = tf.math.divide_no_nan(align_y,al_norm)\n",
        "\n",
        "            # attractive interactions\n",
        "            attr_x = tf.where(dist<=Ra, dx, tf.zeros_like(dx))\n",
        "            attr_x = tf.where(rel_angle_to_neigh<0.5*va, attr_x, tf.zeros_like(attr_x))\n",
        "            attr_x = tf.where(rel_angle_to_neigh>-0.5*va, attr_x, tf.zeros_like(attr_x))\n",
        "            attr_x = tf.reduce_sum(attr_x,axis=2)\n",
        "\n",
        "            attr_y = tf.where(dist<=Ra, dy, tf.zeros_like(dy))\n",
        "            attr_y = tf.where(rel_angle_to_neigh<0.5*va, attr_y, tf.zeros_like(attr_y))\n",
        "            attr_y = tf.where(rel_angle_to_neigh>-0.5*va, attr_y, tf.zeros_like(attr_y))\n",
        "            attr_y = tf.reduce_sum(attr_y,axis=2)\n",
        "\n",
        "            at_norm = tf.math.sqrt(attr_x**2+attr_y**2)\n",
        "            attr_x = tf.math.divide_no_nan(attr_x,at_norm)\n",
        "            attr_y = tf.math.divide_no_nan(attr_y,at_norm)\n",
        "\n",
        "            # combine angles and convert to desired angle change\n",
        "            social_x = rep_x + align_x + attr_x\n",
        "            social_y = rep_y + align_y + attr_y\n",
        "\n",
        "            d_angle = tf.math.atan2(social_y,social_x)\n",
        "            d_angle = tf.expand_dims(d_angle,-1)\n",
        "\n",
        "            \n",
        "            d_angle = tf.math.atan2((1-eta)*tf.math.sin(d_angle) + eta*sin_A, (1-eta)*tf.math.cos(d_angle) + eta*cos_A)\n",
        "\n",
        "            d_angle = d_angle - A\n",
        "            d_angle = tf.where(d_angle>pi, d_angle-2*pi, d_angle)\n",
        "            d_angle = tf.where(d_angle<-pi, d_angle+2*pi, d_angle)\n",
        "\n",
        "\n",
        "            # add perception noise\n",
        "            noise = tf.random.normal(shape=(self.B,self.N,1),mean=0,stddev=sigma*(self.dt**0.5))\n",
        "            d_angle = d_angle + noise\n",
        "            \n",
        "            # restrict to maximum turning angle\n",
        "            #d_angle = tf.where(tf.math.abs(d_angle)>eta*self.dt, tf.math.sign(d_angle)*eta*self.dt, d_angle)\n",
        "            \n",
        "            # rotate headings\n",
        "            A = A + d_angle\n",
        "            \n",
        "            # update positions\n",
        "            velocity = self.dt*vs*tf.concat([tf.cos(A),tf.sin(A)],axis=-1)\n",
        "            X += velocity\n",
        "\n",
        "            # add periodic boundary conditions\n",
        "            A = tf.where(A<-pi,  A+2*pi, A)\n",
        "            A = tf.where(A>pi, A-2*pi, A)\n",
        "\n",
        "            X = tf.where(X>self.L, X-self.L, X)\n",
        "            X = tf.where(X<0, X+self.L, X)\n",
        "\n",
        "            X = tf.where(X>self.L, X-self.L, X)\n",
        "            X = tf.where(X<0, X+self.L, X)\n",
        "\n",
        "            return X, A\n",
        "            \n",
        "        self.initialise_state()\n",
        "\n",
        "        counter=0\n",
        "        for i in tqdm(range(self.timesteps),disable=self.disable_progress):\n",
        "            self.positions, self.angles = update_tf(self.positions,  self.angles)\n",
        "            if i>=self.discard:\n",
        "                if i%self.save_interval==0:\n",
        "                    # store in an array in case we want to visualise\n",
        "                    self.micro_state[:,counter,:,0:2] = self.positions.numpy()\n",
        "                    self.micro_state[:,counter,:,2:3] = np.cos(self.angles.numpy())\n",
        "                    self.micro_state[:,counter,:,3:4] = np.sin(self.angles.numpy())\n",
        "                        \n",
        "                    \n",
        "\n",
        "                    counter = counter + 1\n",
        "\n",
        "        for b in range(self.B):\n",
        "            self.save_tf_record(b)\n",
        "\n",
        "        self.writer.close()\n",
        "        self.validwriter.close()\n",
        "        self.sim_counter+=1\n",
        "        return \n",
        "\n",
        "    def save_tf_record(self, b):\n",
        "        pos =  tf.io.serialize_tensor(self.micro_state[b,:,:,0:2])\n",
        "        vel =  tf.io.serialize_tensor(self.micro_state[b,:,:,2:4])\n",
        "\n",
        "        tf_record = get_record(b,pos,vel)\n",
        "        if b> self.B*self.valid_fraction:\n",
        "            self.writer.write(tf_record.SerializeToString())\n",
        "        else:\n",
        "            self.validwriter.write(tf_record.SerializeToString())\n",
        "\n",
        "        \n",
        "        return "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-iHCH3WaZuO",
        "cellView": "form"
      },
      "source": [
        "#@title ### Params\n",
        "\n",
        "n_points=50 #10\n",
        "\n",
        "param_values = np.linspace(0,25,n_points)\n",
        "L= 200\n",
        "N= 100 \n",
        "repeat = 100\n",
        "discard = 0\n",
        "timesteps = 200\n",
        "save_interval=1\n",
        "dt=0.1\n",
        "\n",
        "\n",
        "sim = zonal_model(N,timesteps=timesteps+discard,discard=discard,L=L,repeat=repeat, dt=dt,save_interval=save_interval,disable_progress=False)\n",
        "\n",
        "latt=0  # adapt\n",
        "lrep= 1 # adapt\n",
        "lali= 5 # adapt\n",
        "eta=0.9 # adapt\n",
        "va=2*pi # adapt\n",
        "vs=5 # fix \n",
        "sigma=0.1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "kiyL0DASnrzU",
        "cellView": "form",
        "outputId": "e9f07251-3cb3-4165-ad38-e9245d704a66"
      },
      "source": [
        "#@title ### Create Training Data\n",
        "\n",
        "def evaluate_zonal_model(X):\n",
        "    sim.run_sim(eta, latt, X, lrep, vs, va, sigma)\n",
        "    return\n",
        "\n",
        "evaluate_zonal_model(0)\n",
        "\n",
        "\"\"\"\n",
        "for i in tqdm(range(param_values.shape[0])):\n",
        "    evaluate_zonal_model(param_values[i])\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [00:27<00:00,  7.40it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfor i in tqdm(range(param_values.shape[0])):\\n    evaluate_zonal_model(param_values[i])\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2LGVb74rFW9"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BL8x-cdXhgo",
        "cellView": "form"
      },
      "source": [
        "#@title ### connectivity_utils\n",
        "\n",
        "def _compute_connectivity(positions, radius, add_self_edges):\n",
        "  \"\"\"Get the indices of connected edges with radius connectivity.\n",
        "  Args:\n",
        "    positions: Positions of nodes in the graph. Shape:\n",
        "      [num_nodes_in_graph, num_dims].\n",
        "    radius: Radius of connectivity.\n",
        "    add_self_edges: Whether to include self edges or not.\n",
        "  Returns:\n",
        "    senders indices [num_edges_in_graph]\n",
        "    receiver indices [num_edges_in_graph]\n",
        "  \"\"\"\n",
        "  tree = neighbors.KDTree(positions)\n",
        "  receivers_list = tree.query_radius(positions, r=radius)\n",
        "  num_nodes = len(positions)\n",
        "  senders = np.repeat(range(num_nodes), [len(a) for a in receivers_list])\n",
        "  receivers = np.concatenate(receivers_list, axis=0)\n",
        "\n",
        "  if not add_self_edges:\n",
        "    # Remove self edges.\n",
        "    mask = senders != receivers\n",
        "    senders = senders[mask]\n",
        "    receivers = receivers[mask]\n",
        "\n",
        "  return senders, receivers\n",
        "\n",
        "\n",
        "def _compute_connectivity_for_batch(\n",
        "    positions, n_node, radius, add_self_edges):\n",
        "  \"\"\"`compute_connectivity` for a batch of graphs.\n",
        "  Args:\n",
        "    positions: Positions of nodes in the batch of graphs. Shape:\n",
        "      [num_nodes_in_batch, num_dims].\n",
        "    n_node: Number of nodes for each graph in the batch. Shape:\n",
        "      [num_graphs in batch].\n",
        "    radius: Radius of connectivity.\n",
        "    add_self_edges: Whether to include self edges or not.\n",
        "  Returns:\n",
        "    senders indices [num_edges_in_batch]\n",
        "    receiver indices [num_edges_in_batch]\n",
        "    number of edges per graph [num_graphs_in_batch]\n",
        "  \"\"\"\n",
        "\n",
        "  # TODO(alvarosg): Consider if we want to support batches here or not.\n",
        "  # Separate the positions corresponding to particles in different graphs.\n",
        "  positions_per_graph_list = np.split(positions, np.cumsum(n_node[:-1]), axis=0)\n",
        "  receivers_list = []\n",
        "  senders_list = []\n",
        "  n_edge_list = []\n",
        "  num_nodes_in_previous_graphs = 0\n",
        "\n",
        "  # Compute connectivity for each graph in the batch.\n",
        "  for positions_graph_i in positions_per_graph_list:\n",
        "    senders_graph_i, receivers_graph_i = _compute_connectivity(\n",
        "        positions_graph_i, radius, add_self_edges)\n",
        "\n",
        "    num_edges_graph_i = len(senders_graph_i)\n",
        "    n_edge_list.append(num_edges_graph_i)\n",
        "\n",
        "    # Because the inputs will be concatenated, we need to add offsets to the\n",
        "    # sender and receiver indices according to the number of nodes in previous\n",
        "    # graphs in the same batch.\n",
        "    receivers_list.append(receivers_graph_i + num_nodes_in_previous_graphs)\n",
        "    senders_list.append(senders_graph_i + num_nodes_in_previous_graphs)\n",
        "\n",
        "    num_nodes_graph_i = len(positions_graph_i)\n",
        "    num_nodes_in_previous_graphs += num_nodes_graph_i\n",
        "\n",
        "  # Concatenate all of the results.\n",
        "  senders = np.concatenate(senders_list, axis=0).astype(np.int32)\n",
        "  receivers = np.concatenate(receivers_list, axis=0).astype(np.int32)\n",
        "  n_edge = np.stack(n_edge_list).astype(np.int32)\n",
        "\n",
        "  return senders, receivers, n_edge\n",
        "\n",
        "\n",
        "def compute_connectivity_for_batch_pyfunc(\n",
        "    positions, n_node, radius, add_self_edges=True):\n",
        "  \"\"\"`_compute_connectivity_for_batch` wrapped in a pyfunc.\"\"\"\n",
        "  partial_fn = functools.partial(\n",
        "      _compute_connectivity_for_batch, add_self_edges=add_self_edges)\n",
        "  #print(\"Connectivity_func\", positions.shape, n_node.shape, radius)\n",
        "  senders, receivers, n_edge = tf.py_function(\n",
        "      partial_fn,\n",
        "      [positions, n_node, radius],\n",
        "      [tf.int32, tf.int32, tf.int32])\n",
        "  senders.set_shape([None])\n",
        "  receivers.set_shape([None])\n",
        "  n_edge.set_shape(n_node.get_shape())\n",
        "  return senders, receivers, n_edge"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEx3i6DgXslW",
        "cellView": "form"
      },
      "source": [
        "#@title ### graph_network\n",
        "\n",
        "Reducer = Callable[[tf.Tensor, tf.Tensor, tf.Tensor], tf.Tensor]\n",
        "\n",
        "def build_mlp(\n",
        "    hidden_size: int, num_hidden_layers: int, output_size: int) -> snt.Module:\n",
        "  \"\"\"Builds an MLP.\"\"\"\n",
        "  return snt.nets.MLP(\n",
        "      output_sizes=[hidden_size] * num_hidden_layers + [output_size])\n",
        "\n",
        "\n",
        "class EncodeProcessDecode(snt.Module):\n",
        "  \"\"\"Encode-Process-Decode function approximator for learnable simulator.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      latent_size: int,\n",
        "      mlp_hidden_size: int,\n",
        "      mlp_num_hidden_layers: int,\n",
        "      num_message_passing_steps: int,\n",
        "      output_size: int,\n",
        "      reducer: Reducer = tf.math.unsorted_segment_sum,\n",
        "      name: str = \"EncodeProcessDecode\"):\n",
        "    \"\"\"Inits the model.\n",
        "    Args:\n",
        "      latent_size: Size of the node and edge latent representations.\n",
        "      mlp_hidden_size: Hidden layer size for all MLPs.\n",
        "      mlp_num_hidden_layers: Number of hidden layers in all MLPs.\n",
        "      num_message_passing_steps: Number of message passing steps.\n",
        "      output_size: Output size of the decode node representations as required\n",
        "        by the downstream update function.\n",
        "      reducer: Reduction to be used when aggregating the edges in the nodes in\n",
        "        the interaction network. This should be a callable whose signature\n",
        "        matches tf.math.unsorted_segment_sum.\n",
        "      name: Name of the model.\n",
        "    \"\"\"\n",
        "\n",
        "    super().__init__(name=name)\n",
        "\n",
        "    self._latent_size = latent_size\n",
        "    self._mlp_hidden_size = mlp_hidden_size\n",
        "    self._mlp_num_hidden_layers = mlp_num_hidden_layers\n",
        "    self._num_message_passing_steps = num_message_passing_steps\n",
        "    self._output_size = output_size\n",
        "    self._reducer = reducer\n",
        "\n",
        "    #with self._enter_variable_scope():\n",
        "    self._networks_builder()\n",
        "\n",
        "  def __call__(self, input_graph: gn.graphs.GraphsTuple) -> tf.Tensor:\n",
        "    \"\"\"Forward pass of the learnable dynamics model.\"\"\"\n",
        "\n",
        "    # Encode the input_graph.\n",
        "    latent_graph_0 = self._encode(input_graph)\n",
        "\n",
        "    # Do `m` message passing steps in the latent graphs.\n",
        "    latent_graph_m = self._process(latent_graph_0)\n",
        "\n",
        "    # Decode from the last latent graph.\n",
        "    return self._decode(latent_graph_m)\n",
        "\n",
        "  def _networks_builder(self):\n",
        "    \"\"\"Builds the networks.\"\"\"\n",
        "    def build_mlp_with_layer_norm():\n",
        "      mlp = build_mlp(\n",
        "          hidden_size=self._mlp_hidden_size,\n",
        "          num_hidden_layers=self._mlp_num_hidden_layers,\n",
        "          output_size=self._latent_size)\n",
        "      return snt.Sequential([mlp, snt.LayerNorm(axis=slice(1, None),create_scale=False,create_offset=False)])\n",
        "\n",
        "    # The encoder graph network independently encodes edge and node features.\n",
        "    encoder_kwargs = dict(\n",
        "        edge_model_fn=build_mlp_with_layer_norm,\n",
        "        node_model_fn=build_mlp_with_layer_norm)\n",
        "    self._encoder_network = gn.modules.GraphIndependent(**encoder_kwargs)\n",
        "    # Create `num_message_passing_steps` graph networks with unshared parameters\n",
        "    # that update the node and edge latent features.\n",
        "    # Note that we can use `modules.InteractionNetwork` because\n",
        "    # it also outputs the messages as updated edge latent features.\n",
        "    self._processor_networks = []\n",
        "    for _ in range(self._num_message_passing_steps):\n",
        "      self._processor_networks.append(\n",
        "          gn.modules.InteractionNetwork(\n",
        "              edge_model_fn=build_mlp_with_layer_norm,\n",
        "              node_model_fn=build_mlp_with_layer_norm,\n",
        "              reducer=self._reducer))\n",
        "    # The decoder MLP decodes node latent features into the output size.\n",
        "    self._decoder_network = build_mlp(\n",
        "        hidden_size=self._mlp_hidden_size,\n",
        "        num_hidden_layers=self._mlp_num_hidden_layers,\n",
        "        output_size=self._output_size)\n",
        "  def _encode(\n",
        "      self, input_graph: gn.graphs.GraphsTuple) -> gn.graphs.GraphsTuple:\n",
        "    \"\"\"Encodes the input graph features into a latent graph.\"\"\"\n",
        "\n",
        "    # Copy the globals to all of the nodes, if applicable.\n",
        "    if input_graph.globals is not None:\n",
        "      broadcasted_globals = gn.blocks.broadcast_globals_to_nodes(input_graph)\n",
        "      input_graph = input_graph.replace(\n",
        "          nodes=tf.concat([input_graph.nodes, broadcasted_globals], axis=-1),\n",
        "          globals=None)\n",
        "\n",
        "    # Encode the node and edge features.\n",
        "    latent_graph_0 = self._encoder_network(input_graph)\n",
        "\n",
        "    return latent_graph_0\n",
        "\n",
        "  def _process(\n",
        "      self, latent_graph_0: gn.graphs.GraphsTuple) -> gn.graphs.GraphsTuple:\n",
        "    \"\"\"Processes the latent graph with several steps of message passing.\"\"\"\n",
        "\n",
        "    # Do `m` message passing steps in the latent graphs.\n",
        "    # (In the shared parameters case, just reuse the same `processor_network`)\n",
        "    latent_graph_prev_k = latent_graph_0\n",
        "    latent_graph_k = latent_graph_0\n",
        "    for processor_network_k in self._processor_networks:\n",
        "      latent_graph_k = self._process_step(\n",
        "          processor_network_k, latent_graph_prev_k)\n",
        "      latent_graph_prev_k = latent_graph_k\n",
        "\n",
        "    latent_graph_m = latent_graph_k\n",
        "    return latent_graph_m\n",
        "\n",
        "  def _process_step(\n",
        "      self, processor_network_k: snt.Module,\n",
        "      latent_graph_prev_k: gn.graphs.GraphsTuple) -> gn.graphs.GraphsTuple:\n",
        "    \"\"\"Single step of message passing with node/edge residual connections.\"\"\"\n",
        "\n",
        "    # One step of message passing.\n",
        "    latent_graph_k = processor_network_k(latent_graph_prev_k)\n",
        "\n",
        "    # Add residuals.\n",
        "    latent_graph_k = latent_graph_k.replace(\n",
        "        nodes=latent_graph_k.nodes+latent_graph_prev_k.nodes,\n",
        "        edges=latent_graph_k.edges+latent_graph_prev_k.edges)\n",
        "    return latent_graph_k\n",
        "\n",
        "  def _decode(self, latent_graph: gn.graphs.GraphsTuple) -> tf.Tensor:\n",
        "    \"\"\"Decodes from the latent graph.\"\"\"\n",
        "    return self._decoder_network(latent_graph.nodes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnGLHy8zTbYo",
        "cellView": "form"
      },
      "source": [
        "#@title ### learned_simulator\n",
        "\n",
        "STD_EPSILON = 1e-8\n",
        "\n",
        "\n",
        "class LearnedSimulator(snt.Module):\n",
        "  \"\"\"Learned simulator from https://arxiv.org/pdf/2002.09405.pdf.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      num_dimensions,\n",
        "      connectivity_radius,\n",
        "      graph_network_kwargs,\n",
        "      boundaries,\n",
        "      normalization_stats,\n",
        "      num_particle_types,\n",
        "      particle_type_embedding_size,\n",
        "      name=\"LearnedSimulator\"):\n",
        "    \"\"\"Inits the model.\n",
        "    Args:\n",
        "      num_dimensions: Dimensionality of the problem.\n",
        "      connectivity_radius: Scalar with the radius of connectivity.\n",
        "      graph_network_kwargs: Keyword arguments to pass to the learned part\n",
        "        of the graph network `model.EncodeProcessDecode`.\n",
        "      boundaries: List of 2-tuples, containing the lower and upper boundaries of\n",
        "        the cuboid containing the particles along each dimensions, matching\n",
        "        the dimensionality of the problem.\n",
        "      normalization_stats: Dictionary with statistics with keys \"acceleration\"\n",
        "        and \"velocity\", containing a named tuple for each with mean and std\n",
        "        fields, matching the dimensionality of the problem.\n",
        "      num_particle_types: Number of different particle types.\n",
        "      particle_type_embedding_size: Embedding size for the particle type.\n",
        "      name: Name of the Sonnet module.\n",
        "    \"\"\"\n",
        "    super().__init__(name=name)\n",
        "\n",
        "    self._connectivity_radius = connectivity_radius\n",
        "    self._num_particle_types = num_particle_types\n",
        "    self._boundaries = boundaries\n",
        "    self._normalization_stats = normalization_stats\n",
        "    #with self._enter_variable_scope():\n",
        "    self._graph_network = EncodeProcessDecode(output_size=num_dimensions, **graph_network_kwargs)\n",
        "\n",
        "    if self._num_particle_types > 1:\n",
        "        self._particle_type_embedding = tf.compat.v1.get_variable(\n",
        "            \"particle_embedding\",\n",
        "            [self._num_particle_types, particle_type_embedding_size],\n",
        "            trainable=True, use_resource=True)\n",
        "\n",
        "  def __call__(self, position_sequence, velocity_sequence, n_particles_per_example=N,\n",
        "             global_context=None, particle_types=None):\n",
        "    \"\"\"Produces a model step, outputting the next position for each particle.\n",
        "    Args:\n",
        "      position_sequence: Sequence of positions for each node in the batch,\n",
        "        with shape [num_particles_in_batch, sequence_length, num_dimensions]\n",
        "      n_particles_per_example: Number of particles for each graph in the batch\n",
        "        with shape [batch_size]\n",
        "      global_context: Tensor of shape [batch_size, context_size], with global\n",
        "        context.\n",
        "      particle_types: Integer tensor of shape [num_particles_in_batch] with\n",
        "        the integer types of the particles, from 0 to `num_particle_types - 1`.\n",
        "        If None, we assume all particles are the same type.\n",
        "    Returns:\n",
        "      Next position with shape [num_particles_in_batch, num_dimensions] for one\n",
        "      step into the future from the input sequence.\n",
        "    \"\"\"\n",
        "    input_graphs_tuple = self._encoder_preprocessor(\n",
        "        position_sequence,\n",
        "        velocity_sequence,\n",
        "        n_particles_per_example,\n",
        "        global_context=None,\n",
        "        particle_types=None)\n",
        "\n",
        "    normalized_acceleration = self._graph_network(input_graphs_tuple)\n",
        "\n",
        "    next_position, next_velocity = self._decoder_postprocessor(\n",
        "        normalized_acceleration, position_sequence)\n",
        "\n",
        "    return next_position, next_velocity\n",
        "\n",
        "  def _encoder_preprocessor(\n",
        "      self, position_sequence, velocity_sequence, n_node, global_context, particle_types):\n",
        "    # Extract important features from the position_sequence.\n",
        "    most_recent_position = position_sequence[-1]\n",
        "\n",
        "    #print(\"encoder\", position_sequence.shape, velocity_sequence.shape, n_node)\n",
        "\n",
        "    # Get connectivity of the graph.\n",
        "    (senders, receivers, n_edge\n",
        "     ) = compute_connectivity_for_batch_pyfunc(\n",
        "         most_recent_position, n_node, self._connectivity_radius)\n",
        "\n",
        "    # Collect node features.\n",
        "    node_features = []\n",
        "\n",
        "    #Calculate velocity mean and std\n",
        "    vm = tf.reduce_mean(velocity_sequence)\n",
        "    vstd = tf.math.reduce_std(velocity_sequence)\n",
        "\n",
        "    # Normalized velocity sequence, merging spatial an time axis.\n",
        "    normalized_velocity_sequence = (\n",
        "        velocity_sequence - vm) / vstd\n",
        "    \n",
        "    normalized_velocity_sequence = tf.reshape(normalized_velocity_sequence, shape=[N,-1])\n",
        "    node_features.append(normalized_velocity_sequence)\n",
        "\n",
        "    # Normalized clipped distances to lower and upper boundaries.\n",
        "    # boundaries are an array of shape [num_dimensions, 2], where the second\n",
        "    # axis, provides the lower/upper boundaries.\n",
        "    boundaries = tf.constant(self._boundaries, dtype=tf.float32)\n",
        "    distance_to_lower_boundary = (\n",
        "        most_recent_position - tf.expand_dims(boundaries[:, 0], 0))\n",
        "    distance_to_upper_boundary = (\n",
        "        tf.expand_dims(boundaries[:, 1], 0) - most_recent_position)\n",
        "    distance_to_boundaries = tf.concat(\n",
        "        [distance_to_lower_boundary, distance_to_upper_boundary], axis=1)\n",
        "    normalized_clipped_distance_to_boundaries = tf.clip_by_value(\n",
        "        distance_to_boundaries / self._connectivity_radius, -1., 1.)\n",
        "    node_features.append(normalized_clipped_distance_to_boundaries)\n",
        "\n",
        "    # Particle type.\n",
        "    if self._num_particle_types > 1:\n",
        "      particle_type_embeddings = tf.nn.embedding_lookup(\n",
        "          self._particle_type_embedding, particle_types)\n",
        "      node_features.append(particle_type_embeddings)\n",
        "\n",
        "    # Collect edge features.\n",
        "    edge_features = []\n",
        "\n",
        "    # Relative displacement and distances normalized to radius\n",
        "    normalized_relative_displacements = (\n",
        "        tf.gather(most_recent_position, senders) -\n",
        "        tf.gather(most_recent_position, receivers)) / self._connectivity_radius\n",
        "    edge_features.append(normalized_relative_displacements)\n",
        "\n",
        "    normalized_relative_distances = tf.norm(\n",
        "        normalized_relative_displacements, axis=-1, keepdims=True)\n",
        "    edge_features.append(normalized_relative_distances)\n",
        "\n",
        "    # Normalize the global context.\n",
        "    if global_context is not None:\n",
        "      context_stats = self._normalization_stats[\"context\"]\n",
        "      # Context in some datasets are all zero, so add an epsilon for numerical\n",
        "      # stability.\n",
        "      global_context = (global_context - context_stats.mean) / tf.math.maximum(\n",
        "          context_stats.std, STD_EPSILON)\n",
        "      \n",
        "    return gn.graphs.GraphsTuple(\n",
        "        nodes=tf.concat(node_features, axis=-1),\n",
        "        edges=tf.concat(edge_features, axis=-1),\n",
        "        globals=global_context,  # self._graph_net will appending this to nodes.\n",
        "        n_node=n_node,\n",
        "        n_edge=n_edge,\n",
        "        senders=senders,\n",
        "        receivers=receivers,\n",
        "        )\n",
        "\n",
        "  def _decoder_postprocessor(self, normalized_acceleration, position_sequence):\n",
        "\n",
        "    # The model produces the output in normalized space so we apply inverse\n",
        "    # normalization.\n",
        "    accmean = tf.reduce_mean(normalized_acceleration)\n",
        "    accstd = tf.math.reduce_std(normalized_acceleration)\n",
        "    acceleration = (\n",
        "        normalized_acceleration * accstd\n",
        "        ) + accmean\n",
        "\n",
        "    # Use an Euler integrator to go from acceleration to position, assuming\n",
        "    # a dt=1 corresponding to the size of the finite difference.\n",
        "    most_recent_position = position_sequence[-1]\n",
        "    most_recent_velocity = most_recent_position - position_sequence[-2]\n",
        "\n",
        "    new_velocity = most_recent_velocity + acceleration  # * dt = 1\n",
        "    new_position = most_recent_position + new_velocity  # * dt = 1\n",
        "    return new_position, new_velocity\n",
        "\n",
        "  def get_predicted_and_target_normalized_accelerations(\n",
        "      self, next_position, position_sequence_noise, position_sequence, velocity_sequence,\n",
        "      n_particles_per_example, global_context=None, particle_types=None):  # pylint: disable=g-doc-args\n",
        "    \"\"\"Produces normalized and predicted acceleration targets.\n",
        "    Args:\n",
        "      next_position: Tensor of shape [num_particles_in_batch, num_dimensions]\n",
        "        with the positions the model should output given the inputs.\n",
        "      position_sequence_noise: Tensor of the same shape as `position_sequence`\n",
        "        with the noise to apply to each particle.\n",
        "      position_sequence, n_node, global_context, particle_types: Inputs to the\n",
        "        model as defined by `_build`.\n",
        "    Returns:\n",
        "      Tensors of shape [num_particles_in_batch, num_dimensions] with the\n",
        "        predicted and target normalized accelerations.\n",
        "    \"\"\"\n",
        "\n",
        "    # Add noise to the input position sequence.\n",
        "    noisy_position_sequence = position_sequence + position_sequence_noise\n",
        "\n",
        "    # Perform the forward pass with the noisy position sequence.\n",
        "    input_graphs_tuple = self._encoder_preprocessor(\n",
        "        noisy_position_sequence, velocity_sequence, n_particles_per_example, global_context,\n",
        "        particle_types)\n",
        "    predicted_normalized_acceleration = self._graph_network(input_graphs_tuple)\n",
        "\n",
        "    # Calculate the target acceleration, using an `adjusted_next_position `that\n",
        "    # is shifted by the noise in the last input position.\n",
        "\n",
        "    next_position_adjusted = next_position + position_sequence_noise[-1]\n",
        "    target_normalized_acceleration = self._inverse_decoder_postprocessor(\n",
        "        next_position_adjusted, noisy_position_sequence)\n",
        "    # As a result the inverted Euler update in the `_inverse_decoder` produces:\n",
        "    # * A target acceleration that does not explicitly correct for the noise in\n",
        "    #   the input positions, as the `next_position_adjusted` is different\n",
        "    #   from the true `next_position`.\n",
        "    # * A target acceleration that exactly corrects noise in the input velocity\n",
        "    #   since the target next velocity calculated by the inverse Euler update\n",
        "    #   as `next_position_adjusted - noisy_position_sequence[:,-1]`\n",
        "    #   matches the ground truth next velocity (noise cancels out).\n",
        "\n",
        "    return predicted_normalized_acceleration, target_normalized_acceleration\n",
        "\n",
        "  def _inverse_decoder_postprocessor(self, next_position, position_sequence):\n",
        "    \"\"\"Inverse of `_decoder_postprocessor`.\"\"\"\n",
        "\n",
        "    previous_position = position_sequence[-1]\n",
        "    previous_velocity = previous_position - position_sequence[-2]\n",
        "    next_velocity = next_position - previous_position\n",
        "    acceleration = next_velocity - previous_velocity\n",
        "\n",
        "    accmean = tf.reduce_mean(acceleration)\n",
        "    accstd = tf.math.reduce_std(acceleration)\n",
        "    normalized_acceleration = (\n",
        "        acceleration - accmean) / accstd\n",
        "    return normalized_acceleration\n",
        "\n",
        "\n",
        "def time_diff(input_sequence):\n",
        "  return input_sequence[:, 1:] - input_sequence[:, :-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ntN9CfJTROC",
        "cellView": "form"
      },
      "source": [
        "#@title ### noise_utils\n",
        "\n",
        "\n",
        "def get_random_walk_noise_for_position_sequence(\n",
        "    position_sequence, noise_std_last_step):\n",
        "  \"\"\"Returns random-walk noise in the velocity applied to the position.\"\"\"\n",
        "\n",
        "  velocity_sequence = time_diff(position_sequence)\n",
        "\n",
        "  # We want the noise scale in the velocity at the last step to be fixed.\n",
        "  # Because we are going to compose noise at each step using a random_walk:\n",
        "  # std_last_step**2 = num_velocities * std_each_step**2\n",
        "  # so to keep `std_last_step` fixed, we apply at each step:\n",
        "  # std_each_step `std_last_step / np.sqrt(num_input_velocities)`\n",
        "  # TODO(alvarosg): Make sure this is consistent with the value and\n",
        "  # description provided in the paper.\n",
        "  num_velocities = velocity_sequence.shape.as_list()[1]\n",
        "  velocity_sequence_noise = tf.random.normal(\n",
        "      tf.shape(velocity_sequence),\n",
        "      stddev=noise_std_last_step / num_velocities ** 0.5,\n",
        "      dtype=position_sequence.dtype)\n",
        "\n",
        "  # Apply the random walk.\n",
        "  velocity_sequence_noise = tf.cumsum(velocity_sequence_noise, axis=1)\n",
        "\n",
        "  # Integrate the noise in the velocity to the positions, assuming\n",
        "  # an Euler intergrator and a dt = 1, and adding no noise to the very first\n",
        "  # position (since that will only be used to calculate the first position\n",
        "  # change).\n",
        "  position_sequence_noise = tf.concat([\n",
        "      tf.zeros_like(velocity_sequence_noise[:, 0:1]),\n",
        "      tf.cumsum(velocity_sequence_noise, axis=1)], axis=1)\n",
        "  return position_sequence_noise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKnr3tQGX7pv",
        "cellView": "form"
      },
      "source": [
        "#@title ### reading_utils\n",
        "\n",
        "# Create a description of the features.\n",
        "_FEATURE_DESCRIPTION = {\n",
        "    'pos': tf.io.FixedLenSequenceFeature([timesteps//save_interval, N, 2], tf.string),\n",
        "    'vel': tf.io.FixedLenSequenceFeature([timesteps//save_interval, N, 2], tf.string)\n",
        "}\n",
        "\n",
        "_FEATURE_DESCRIPTION_WITH_GLOBAL_CONTEXT = _FEATURE_DESCRIPTION.copy()\n",
        "_FEATURE_DESCRIPTION_WITH_GLOBAL_CONTEXT['step_context'] = tf.io.VarLenFeature(\n",
        "    tf.string)\n",
        "\n",
        "_FEATURE_DTYPES = {\n",
        "    'pos': {\n",
        "        'in': np.float32,\n",
        "        'out': tf.float32\n",
        "    },\n",
        "    'vel': {\n",
        "        'in': np.float32,\n",
        "        'out': tf.float32\n",
        "    }\n",
        "}\n",
        "\n",
        "_CONTEXT_FEATURES = {\n",
        "    'group_id': tf.io.FixedLenFeature([1],tf.int64)\n",
        "}\n",
        "\n",
        "\n",
        "def convert_to_tensor(x, encoded_dtype):\n",
        "  if len(x) == 1:\n",
        "    out = np.frombuffer(x[0].numpy(), dtype=encoded_dtype)\n",
        "  else:\n",
        "    out = []\n",
        "    for el in x:\n",
        "      out.append(np.frombuffer(el.numpy(), dtype=encoded_dtype))\n",
        "  out = tf.convert_to_tensor(np.array(out))\n",
        "  return out\n",
        "\n",
        "\n",
        "\n",
        "def split_trajectory(context, features, window_length=7):\n",
        "  \"\"\"Splits trajectory into sliding windows.\"\"\"\n",
        "  # Our strategy is to make sure all the leading dimensions are the same size,\n",
        "  # then we can use from_tensor_slices.\n",
        "  \n",
        "  trajectory_length = features['pos'].get_shape().as_list()[0]\n",
        "\n",
        "  # We then stack window_length position changes so the final\n",
        "  # trajectory length will be - window_length +1 (the 1 to make sure we get\n",
        "  # the last split).\n",
        "  input_trajectory_length = trajectory_length - window_length + 1\n",
        "\n",
        "  model_input_features = {}\n",
        "  # Prepare the context features per step.\n",
        "  model_input_features['group_id'] = tf.tile(\n",
        "      tf.expand_dims(context['group_id'], axis=0),\n",
        "      [input_trajectory_length, 1])\n",
        "\n",
        "  if 'step_context' in features:\n",
        "    global_stack = []\n",
        "    for idx in range(input_trajectory_length):\n",
        "      global_stack.append(features['step_context'][idx:idx + window_length])\n",
        "    model_input_features['step_context'] = tf.stack(global_stack)\n",
        "\n",
        "  pos_stack = []\n",
        "  for idx in range(input_trajectory_length):\n",
        "    pos_stack.append(features['pos'][idx:idx + window_length])\n",
        "  # Get the corresponding positions\n",
        "  model_input_features['pos'] = tf.stack(pos_stack)\n",
        "\n",
        "  vel_stack = []\n",
        "  for idx in range(input_trajectory_length):\n",
        "    vel_stack.append(features['vel'][idx:idx + window_length])\n",
        "  # Get the corresponding velocities\n",
        "  model_input_features['vel'] = tf.stack(vel_stack)\n",
        "\n",
        "  return tf.data.Dataset.from_tensor_slices(model_input_features)\n",
        "\n",
        "def print_graphs_tuple(graphs_tuple):\n",
        "  print(\"Shapes of `GraphsTuple`'s fields:\")\n",
        "  print(graphs_tuple.map(lambda x: x if x is None else x.shape, fields=gn.graphs.ALL_FIELDS))\n",
        "  # print(\"\\nData contained in `GraphsTuple`'s fields:\")\n",
        "  # print(\"globals:\\n{}\".format(graphs_tuple.globals))\n",
        "  # print(\"nodes:\\n{}\".format(graphs_tuple.nodes))\n",
        "  # print(\"edges:\\n{}\".format(graphs_tuple.edges))\n",
        "  # print(\"senders:\\n{}\".format(graphs_tuple.senders))\n",
        "  # print(\"receivers:\\n{}\".format(graphs_tuple.receivers))\n",
        "  # print(\"n_node:\\n{}\".format(graphs_tuple.n_node))\n",
        "  # print(\"n_edge:\\n{}\".format(graphs_tuple.n_edge))\n",
        "\n",
        "#print_graphs_tuple(graphs_tuple)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_J2CFR8rljZ"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYeIwYzirjCM",
        "cellView": "form"
      },
      "source": [
        "#@title ### Colin Code\n",
        "\n",
        "#Create a tf.data.Dataset from the TFRecord.\n",
        "ds = tf.data.TFRecordDataset(['train_datasets/microstates-' + str(i) + '.tfrecords' for i in range(1)])\n",
        "\n",
        "WINDOW_SIZE=7\n",
        "\n",
        "def _parse_record(x):\n",
        "  feature_description = {'group_id': tf.io.FixedLenFeature([], tf.int64),\n",
        "                        'pos': tf.io.FixedLenFeature([], tf.string),\n",
        "                        'vel': tf.io.FixedLenFeature([], tf.string)}\n",
        "  # Parse the input tf.train.Example proto using the dictionary above.\n",
        "  return tf.io.parse_single_example(x, feature_description)\n",
        "\n",
        "def _parse_tensor(x):\n",
        "    output = {'group_id': x['group_id'],\n",
        "                'pos': tf.io.parse_tensor(x['pos'],out_type=tf.float32),\n",
        "                'vel': tf.io.parse_tensor(x['vel'],out_type=tf.float32)}\n",
        "    return output\n",
        "\n",
        "def create_data(ds, WINDOW_SIZE=7):\n",
        "    # map the record to features\n",
        "    ds = ds.map(_parse_record)\n",
        "\n",
        "    # map the features to tensors\n",
        "    ds = ds.map(_parse_tensor)\n",
        "\n",
        "\n",
        "    def make_window_dataset(x):\n",
        "        # make a dataset from the time series tensor\n",
        "        windows = tf.data.Dataset.from_tensor_slices((x['pos'],x['vel']))\n",
        "        #print(windows)\n",
        "        # convert to windows\n",
        "        windows = windows.window(WINDOW_SIZE+1, shift=1, stride=1)\n",
        "        #print(windows)\n",
        "        # take a batch of window size and combine pos, vel to a single dataset\n",
        "        windows = windows.flat_map(lambda pos_ds,vel_ds: tf.data.Dataset.zip((pos_ds.batch(WINDOW_SIZE, drop_remainder=True),vel_ds.batch(WINDOW_SIZE, drop_remainder=True))))\n",
        "        #print(windows)\n",
        "        return windows\n",
        "\n",
        "    # flatten the windowed dataset\n",
        "    ds = ds.flat_map(make_window_dataset)\n",
        "    #print(ds)\n",
        "    ds.shuffle(2000)\n",
        "\n",
        "    def split_targets(x, y, w=7):\n",
        "        inputs = (x[0:w-1], y[0:w-1])\n",
        "        targets = (x[-1], y[-1])\n",
        "        return (inputs, targets)\n",
        "\n",
        "    split_with_window = functools.partial(\n",
        "      split_targets,\n",
        "      w=WINDOW_SIZE)\n",
        "    ds = ds.map(split_with_window)\n",
        "\n",
        "    return ds\n",
        "\n",
        "ds = create_data(ds, WINDOW_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzgGNFysMx35",
        "cellView": "form"
      },
      "source": [
        "#@title ### Preprocessing function\n",
        "\n",
        "interaction_radius = 15.0\n",
        "L = 200\n",
        "def preprocess_data(databatch):\n",
        "    \n",
        "    # node features xpos, ypos, xvel, yvel\n",
        "    # edge features distance, rel angle to receiver\n",
        "    X = databatch['pos']\n",
        "    V = databatch['vel']\n",
        "    \n",
        "    Xx = tf.expand_dims(X[...,0],-1)\n",
        "    dx = -Xx + tf.linalg.matrix_transpose(Xx)\n",
        "    dx = tf.where(dx>0.5*L, dx-L, dx)\n",
        "    dx = tf.where(dx<-0.5*L, dx+L, dx)\n",
        "\n",
        "    Xy = tf.expand_dims(X[...,1],-1)\n",
        "    dy = -Xy + tf.linalg.matrix_transpose(Xy)\n",
        "    dy = tf.where(dy>0.5*L, dy-L, dy)\n",
        "    dy = tf.where(dy<-0.5*L, dy+L, dy)\n",
        "\n",
        "\n",
        "\n",
        "    A = tf.expand_dims(tf.math.atan2(V[...,1],V[...,0]),-1)\n",
        "    angle_to_neigh = tf.math.atan2(dy, dx)\n",
        "\n",
        "    rel_angle_to_neigh = angle_to_neigh - A\n",
        "\n",
        "    dist = tf.math.sqrt(tf.square(dx)+tf.square(dy))\n",
        "\n",
        "    adj_matrix = tf.where(dist<interaction_radius, tf.ones_like(dist,dtype=tf.int32), tf.zeros_like(dist,dtype=tf.int32))\n",
        "    adj_matrix = tf.linalg.set_diag(adj_matrix, tf.zeros(tf.shape(adj_matrix)[:2],dtype=tf.int32))\n",
        "    sender_recv_list = tf.where(adj_matrix)\n",
        "    n_edge = tf.reduce_sum(adj_matrix, axis=[1,2])\n",
        "    n_node = tf.ones_like(n_edge)*tf.shape(adj_matrix)[-1]\n",
        "\n",
        "    senders = sender_recv_list[:,1] + sender_recv_list[:,0]*tf.shape(adj_matrix,out_type=tf.int64)[-1]\n",
        "    receivers = sender_recv_list[:,2] + sender_recv_list[:,0]*tf.shape(adj_matrix,out_type=tf.int64)[-1]\n",
        "\n",
        "\n",
        "    edge_distance = tf.expand_dims(tf.gather_nd(dist,sender_recv_list),-1)\n",
        "    edge_x_distance =  tf.expand_dims(tf.gather_nd(tf.math.cos(rel_angle_to_neigh),sender_recv_list),-1)  # relative to sender heading\n",
        "    edge_y_distance =  tf.expand_dims( tf.gather_nd(tf.math.sin(rel_angle_to_neigh),sender_recv_list),-1)  # relative to sender heading\n",
        "\n",
        "\n",
        "    edges = tf.concat([edge_distance,edge_x_distance,edge_y_distance],axis=-1)\n",
        "\n",
        "    node_positions = tf.reshape(X,(-1,2))\n",
        "    node_velocities = tf.reshape(V,(-1,2))\n",
        "\n",
        "    nodes = tf.concat([node_positions,node_velocities],axis=-1)\n",
        "\n",
        "    gn = graphs.GraphsTuple(nodes=nodes,edges=edges,globals=None,receivers=receivers,senders=senders,n_node=n_node,n_edge=n_edge)\n",
        "    gn = utils_tf.set_zero_global_features(gn,1)\n",
        "    \n",
        "    return gn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbMPiqZENpAe",
        "cellView": "form"
      },
      "source": [
        "#@title ### More Functions\n",
        "\n",
        "INPUT_SEQUENCE_LENGTH = WINDOW_SIZE-1  # So we can calculate the last 5 velocities.\n",
        "NUM_PARTICLE_TYPES = 1\n",
        "KINEMATIC_PARTICLE_ID = 3\n",
        "\n",
        "batch_size=5\n",
        "\n",
        "metadata = {\"sequence_length\": timesteps//save_interval,\n",
        "            \"dim\": 2,\n",
        "            \"acc_mean\": 1,\n",
        "            \"acc_std\": 1,\n",
        "            \"vel_mean\": 1,\n",
        "            \"vel_std\": 1,\n",
        "            \"default_connectivity_radius\": lali,\n",
        "            \"bounds\": [(0,L), (0,L)]\n",
        "            }\n",
        "\n",
        "def _read_metadata(data_path):\n",
        "    with open(os.path.join(data_path, 'metadata.json'), 'rt') as fp:\n",
        "        return json.loads(fp.read())\n",
        "\n",
        "def prepare_inputs(tensor_dict):\n",
        "  \"\"\"Prepares a single stack of inputs by calculating inputs and targets.\n",
        "\n",
        "  Computes n_particles_per_example, which is a tensor that contains information\n",
        "  about how to partition the axis - i.e. which nodes belong to which graph.\n",
        "\n",
        "  Adds a batch axis to `n_particles_per_example` and `step_context` so they can\n",
        "  later be batched using `batch_concat`. This batch will be the same as if the\n",
        "  elements had been batched via stacking.\n",
        "\n",
        "  Note that all other tensors have a variable size particle axis,\n",
        "  and in this case they will simply be concatenated along that\n",
        "  axis.\n",
        "\n",
        "\n",
        "\n",
        "  Args:\n",
        "    tensor_dict: A dict of tensors containing positions, and step context (\n",
        "    if available).\n",
        "\n",
        "  Returns:\n",
        "    A tuple of input features and target positions.\n",
        "\n",
        "  \"\"\"\n",
        "  # Position is encoded as [sequence_length, num_particles, dim] but the model\n",
        "  # expects [num_particles, sequence_length, dim].\n",
        "  pos = tensor_dict['pos']\n",
        "  pos = pos[tf.newaxis,...]\n",
        "  pos = tf.transpose(pos, perm=[0, 2, 1, 3])\n",
        "\n",
        "  # The target position is the final step of the stack of positions.\n",
        "  target_position = pos[:, :, -1]\n",
        "\n",
        "  # Remove the target from the input.\n",
        "  tensor_dict['pos'] = pos[:, :, :-1]\n",
        "\n",
        "  # Velocity is encoded as [sequence_length, num_particles, dim] but the model\n",
        "  # expects [num_particles, sequence_length, dim].\n",
        "  vel = tensor_dict['vel']\n",
        "  vel = vel[tf.newaxis, ...]\n",
        "  vel = tf.transpose(vel, perm=[0, 2, 1, 3])\n",
        "\n",
        "  # The target position is the final step of the stack of positions.\n",
        "  target_position = vel[:, :, -1]\n",
        "\n",
        "  # Remove the target from the input.\n",
        "  tensor_dict['vel'] = vel[:, :, :-1]\n",
        "\n",
        "  # Compute the number of particles per example.\n",
        "  num_particles = tf.shape(pos)[0]\n",
        "  # Add an extra dimension for stacking via concat.\n",
        "  tensor_dict['n_particles_per_example'] = num_particles[tf.newaxis]\n",
        "\n",
        "  if 'step_context' in tensor_dict:\n",
        "    # Take the input global context. We have a stack of global contexts,\n",
        "    # and we take the penultimate since the final is the target.\n",
        "    tensor_dict['step_context'] = tensor_dict['step_context'][-2]\n",
        "    # Add an extra dimension for stacking via concat.\n",
        "    tensor_dict['step_context'] = tensor_dict['step_context'][tf.newaxis]\n",
        "  return tensor_dict, target_position\n",
        "\n",
        "def prepare_rollout_inputs(context, features):\n",
        "  \"\"\"Prepares an inputs trajectory for rollout.\"\"\"\n",
        "  out_dict = {**context}\n",
        "  # Position is encoded as [sequence_length, num_particles, dim] but the model\n",
        "  # expects [num_particles, sequence_length, dim].\n",
        "  pos = tf.transpose(features['pos'], [1, 0, 2])\n",
        "  # The target position is the final step of the stack of positions.\n",
        "  target_position = pos[:, -1]\n",
        "  # Remove the target from the input.\n",
        "  out_dict['pos'] = pos[:, :-1]\n",
        "\n",
        "  # Compute the number of nodes\n",
        "  out_dict['n_particles_per_example'] = [tf.shape(pos)[0]]\n",
        "  if 'step_context' in features:\n",
        "    out_dict['step_context'] = features['step_context']\n",
        "  out_dict['is_trajectory'] = tf.constant([True], tf.bool)\n",
        "  return out_dict, target_position"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o51fJ2WOoCc5"
      },
      "source": [
        "# Create and Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUe5GAlNCD2X",
        "cellView": "form"
      },
      "source": [
        "#@title ### Create Model\n",
        "#@markdown loss_fn may be broken - features[0] adds noise to all timesteps\n",
        "\n",
        "noise_std=6.7e-4\n",
        "latent_size=128\n",
        "hidden_size=128\n",
        "hidden_layers=2\n",
        "message_passing_steps=10\n",
        "\n",
        "\"\"\"Gets one step model for training simulation.\"\"\"\n",
        "#metadata = _read_metadata(data_path)\n",
        "model_kwargs = dict(\n",
        "      latent_size=latent_size,\n",
        "      mlp_hidden_size=hidden_size,\n",
        "      mlp_num_hidden_layers=hidden_layers,\n",
        "      num_message_passing_steps=message_passing_steps)\n",
        "def _combine_std(std_x, std_y):\n",
        "  return np.sqrt(std_x**2 + std_y**2)\n",
        "\n",
        "Stats = collections.namedtuple('Stats', ['mean', 'std'])\n",
        "vel_noise_std=noise_std\n",
        "acc_noise_std=noise_std\n",
        "\"\"\"Instantiates the simulator.\"\"\"\n",
        "# Cast statistics to numpy so they are arrays when entering the model.\n",
        "cast = lambda v: np.array(v, dtype=np.float32)\n",
        "acceleration_stats = Stats(cast(metadata['acc_mean']), _combine_std(cast(metadata['acc_std']), acc_noise_std))\n",
        "velocity_stats = Stats(cast(metadata['vel_mean']),_combine_std(cast(metadata['vel_std']), vel_noise_std))\n",
        "normalization_stats = {'acceleration': acceleration_stats, 'velocity': velocity_stats}\n",
        "\n",
        "\n",
        "\n",
        "if 'context_mean' in metadata:\n",
        "    context_stats = Stats(cast(metadata['context_mean']), cast(metadata['context_std']))\n",
        "    normalization_stats['context'] = context_stats\n",
        "\n",
        "\n",
        "simulator = LearnedSimulator(\n",
        "      num_dimensions=metadata['dim'],\n",
        "      connectivity_radius=metadata['default_connectivity_radius'],\n",
        "      graph_network_kwargs=model_kwargs,\n",
        "      boundaries=metadata['bounds'],\n",
        "      num_particle_types=NUM_PARTICLE_TYPES,\n",
        "      normalization_stats=normalization_stats,\n",
        "      particle_type_embedding_size=16)\n",
        "\n",
        "\n",
        "KINEMATIC_PARTICLE_ID = 3\n",
        "def get_kinematic_mask(particle_types):\n",
        "  \"\"\"Returns a boolean mask, set to true for kinematic (obstacle) particles.\"\"\"\n",
        "  return tf.equal(particle_types, KINEMATIC_PARTICLE_ID)\n",
        "\n",
        "#@tf.function\n",
        "def loss_fn(features, labels):\n",
        "    target_next_position = labels[0]\n",
        "    # Sample the noise to add to the inputs to the model during training.\n",
        "    sampled_noise = get_random_walk_noise_for_position_sequence(\n",
        "        features[0], noise_std_last_step=noise_std)\n",
        "    non_kinematic_mask = tf.logical_not(get_kinematic_mask(tf.zeros(N)))\n",
        "    noise_mask = tf.cast(non_kinematic_mask, sampled_noise.dtype)\n",
        "    sampled_noise *= noise_mask[tf.newaxis,:,tf.newaxis]\n",
        "\n",
        "    # Get the predictions and target accelerations.\n",
        "    pred_target = simulator.get_predicted_and_target_normalized_accelerations(\n",
        "        next_position=target_next_position,\n",
        "        position_sequence=features[0],\n",
        "        position_sequence_noise=sampled_noise,\n",
        "        velocity_sequence=features[1],\n",
        "        n_particles_per_example=tf.constant([N]),\n",
        "        particle_types=None,\n",
        "        global_context=None)\n",
        "\n",
        "    pred_acceleration, target_acceleration = pred_target\n",
        "\n",
        "    # Calculate the loss and mask out loss on kinematic particles/\n",
        "    loss = (pred_acceleration - target_acceleration)**2\n",
        "    num_non_kinematic = tf.reduce_sum(tf.cast(non_kinematic_mask, tf.float32))\n",
        "    loss = tf.where(tf.expand_dims(non_kinematic_mask,-1), loss, tf.zeros_like(loss))\n",
        "    loss = tf.reduce_sum(loss) / tf.reduce_sum(num_non_kinematic)\n",
        "    \n",
        "    return loss\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8VFYHpFS-6X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "b6f39ed7-bb40-4b97-f053-1fd47bcbe107"
      },
      "source": [
        "#@title ### Train\n",
        "\n",
        "min_lr = 1e-6\n",
        "lr = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-4 - min_lr,\n",
        "                                decay_steps=int(5e6),\n",
        "                                decay_rate=0.1) #+ min_lr\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "# @tf.function - ideally we'd like to decorate this with tf.function for faster code - but the connectivity utils is written using numpy so will need to be converted to tensorflow code\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss_value = loss_fn(x,y)\n",
        "    grads = tape.gradient(loss_value, simulator.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, simulator.trainable_variables))\n",
        "    return loss_value\n",
        "\n",
        "max_data_points = -1 #Set to -1 to include all data points\n",
        "ds = ds.take(max_data_points)\n",
        "\n",
        "epochs = 2\n",
        "loss_values = np.zeros((epochs, len(list(ds))))\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in tqdm(enumerate(iter(ds)), total=len(list(ds))):\n",
        "        loss_value = train_step(x_batch_train, y_batch_train)\n",
        "        loss_values[epoch, step] = loss_value\n",
        "        # Log every 200 batches.\n",
        "        if step % 200 == 0:\n",
        "            print(\n",
        "                \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                % (step, float(loss_value))\n",
        "            )\n",
        "            print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start of epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/17266 [00:01<6:00:08,  1.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 0: 3.1273\n",
            "Seen so far: 5 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 201/17266 [01:06<2:29:23,  1.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 200: 2.0836\n",
            "Seen so far: 1005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 401/17266 [02:11<2:12:38,  2.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 400: 2.2037\n",
            "Seen so far: 2005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 601/17266 [03:15<1:55:34,  2.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 600: 2.2531\n",
            "Seen so far: 3005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▍         | 801/17266 [04:21<1:45:02,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 800: 1.9519\n",
            "Seen so far: 4005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 1001/17266 [05:25<1:37:07,  2.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 1000: 2.1377\n",
            "Seen so far: 5005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 1201/17266 [06:29<1:25:42,  3.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 1200: 2.8198\n",
            "Seen so far: 6005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|▊         | 1401/17266 [07:32<1:28:50,  2.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 1400: 2.0476\n",
            "Seen so far: 7005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  9%|▉         | 1601/17266 [08:35<1:19:48,  3.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 1600: 1.9061\n",
            "Seen so far: 8005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1801/17266 [09:39<1:23:04,  3.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 1800: 2.4440\n",
            "Seen so far: 9005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 12%|█▏        | 2001/17266 [10:43<1:18:21,  3.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 2000: 1.8444\n",
            "Seen so far: 10005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 2201/17266 [11:47<1:18:32,  3.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 2200: 1.9859\n",
            "Seen so far: 11005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 14%|█▍        | 2401/17266 [12:51<1:15:44,  3.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 2400: 2.0277\n",
            "Seen so far: 12005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 15%|█▌        | 2601/17266 [13:55<1:12:54,  3.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 2600: 2.0333\n",
            "Seen so far: 13005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|█▌        | 2801/17266 [14:59<1:10:36,  3.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 2800: 1.9848\n",
            "Seen so far: 14005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 3001/17266 [16:02<1:07:46,  3.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 3000: 1.9763\n",
            "Seen so far: 15005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 19%|█▊        | 3201/17266 [17:06<1:08:10,  3.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 3200: 2.0264\n",
            "Seen so far: 16005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|█▉        | 3401/17266 [18:09<1:06:38,  3.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 3400: 1.9888\n",
            "Seen so far: 17005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 21%|██        | 3601/17266 [19:12<1:06:28,  3.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 3600: 2.0006\n",
            "Seen so far: 18005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 22%|██▏       | 3801/17266 [20:15<1:03:53,  3.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 3800: 1.9842\n",
            "Seen so far: 19005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 23%|██▎       | 4001/17266 [21:20<1:04:55,  3.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 4000: 1.9928\n",
            "Seen so far: 20005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 24%|██▍       | 4201/17266 [22:23<1:02:47,  3.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 4200: 1.9833\n",
            "Seen so far: 21005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 4401/17266 [23:26<1:02:13,  3.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 4400: 1.9796\n",
            "Seen so far: 22005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|██▋       | 4601/17266 [24:31<1:01:49,  3.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 4600: 2.0319\n",
            "Seen so far: 23005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 28%|██▊       | 4801/17266 [25:36<1:00:38,  3.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 4800: 2.0504\n",
            "Seen so far: 24005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 29%|██▉       | 5001/17266 [26:40<57:10,  3.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 5000: 1.9930\n",
            "Seen so far: 25005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 5201/17266 [27:44<56:41,  3.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 5200: 2.0280\n",
            "Seen so far: 26005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 31%|███▏      | 5401/17266 [28:48<56:23,  3.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 5400: 1.9923\n",
            "Seen so far: 27005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 32%|███▏      | 5601/17266 [29:51<54:20,  3.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 5600: 2.0005\n",
            "Seen so far: 28005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▎      | 5801/17266 [30:55<52:22,  3.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 5800: 2.0010\n",
            "Seen so far: 29005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 35%|███▍      | 6001/17266 [32:02<53:06,  3.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 6000: 2.0016\n",
            "Seen so far: 30005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 36%|███▌      | 6201/17266 [33:06<51:30,  3.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 6200: 2.0014\n",
            "Seen so far: 31005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 37%|███▋      | 6401/17266 [34:11<49:43,  3.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 6400: 2.0001\n",
            "Seen so far: 32005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 38%|███▊      | 6601/17266 [35:15<1:36:44,  1.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 6600: 1.9925\n",
            "Seen so far: 33005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 39%|███▉      | 6801/17266 [36:24<1:33:59,  1.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 6800: 1.9942\n",
            "Seen so far: 34005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|████      | 7001/17266 [37:29<1:20:32,  2.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 7000: 2.1795\n",
            "Seen so far: 35005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 42%|████▏     | 7201/17266 [38:35<1:15:15,  2.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 7200: 2.0087\n",
            "Seen so far: 36005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 43%|████▎     | 7401/17266 [39:41<58:24,  2.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 7400: 1.9956\n",
            "Seen so far: 37005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 44%|████▍     | 7601/17266 [40:44<52:30,  3.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 7600: 2.0027\n",
            "Seen so far: 38005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 45%|████▌     | 7801/17266 [41:48<50:04,  3.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 7800: 2.0005\n",
            "Seen so far: 39005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 46%|████▋     | 8001/17266 [42:55<50:53,  3.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 8000: 2.0049\n",
            "Seen so far: 40005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 47%|████▋     | 8201/17266 [44:00<46:41,  3.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 8200: 1.9988\n",
            "Seen so far: 41005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 49%|████▊     | 8401/17266 [45:05<46:12,  3.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 8400: 2.0000\n",
            "Seen so far: 42005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|████▉     | 8601/17266 [46:12<45:55,  3.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 8600: 1.9995\n",
            "Seen so far: 43005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 51%|█████     | 8801/17266 [47:18<43:17,  3.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 8800: 1.9976\n",
            "Seen so far: 44005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 52%|█████▏    | 9001/17266 [48:23<42:48,  3.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 9000: 1.9985\n",
            "Seen so far: 45005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 53%|█████▎    | 9201/17266 [49:28<41:35,  3.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 9200: 2.0040\n",
            "Seen so far: 46005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|█████▍    | 9401/17266 [50:35<41:02,  3.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 9400: 2.0007\n",
            "Seen so far: 47005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 56%|█████▌    | 9601/17266 [51:40<37:26,  3.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 9600: 1.9957\n",
            "Seen so far: 48005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 57%|█████▋    | 9801/17266 [52:44<36:32,  3.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 9800: 2.0126\n",
            "Seen so far: 49005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 58%|█████▊    | 10001/17266 [53:48<35:30,  3.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 10000: 2.0081\n",
            "Seen so far: 50005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 59%|█████▉    | 10201/17266 [54:53<34:55,  3.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 10200: 1.9962\n",
            "Seen so far: 51005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 10401/17266 [55:58<32:46,  3.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 10400: 2.0022\n",
            "Seen so far: 52005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 61%|██████▏   | 10601/17266 [57:02<32:47,  3.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 10600: 2.0029\n",
            "Seen so far: 53005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 63%|██████▎   | 10801/17266 [58:08<33:29,  3.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 10800: 1.9997\n",
            "Seen so far: 54005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 64%|██████▎   | 11001/17266 [59:16<31:43,  3.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 11000: 2.0057\n",
            "Seen so far: 55005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▍   | 11201/17266 [1:00:22<29:23,  3.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 11200: 2.0153\n",
            "Seen so far: 56005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 66%|██████▌   | 11401/17266 [1:01:28<27:37,  3.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 11400: 2.0146\n",
            "Seen so far: 57005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 11601/17266 [1:02:31<26:40,  3.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 11600: 1.9873\n",
            "Seen so far: 58005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 68%|██████▊   | 11801/17266 [1:03:34<25:23,  3.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 11800: 1.9976\n",
            "Seen so far: 59005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|██████▉   | 12001/17266 [1:04:38<23:48,  3.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 12000: 1.9931\n",
            "Seen so far: 60005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 71%|███████   | 12201/17266 [1:05:41<23:39,  3.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 12200: 1.9983\n",
            "Seen so far: 61005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 72%|███████▏  | 12401/17266 [1:06:47<22:57,  3.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 12400: 1.9987\n",
            "Seen so far: 62005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 73%|███████▎  | 12601/17266 [1:07:55<23:13,  3.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 12600: 1.9989\n",
            "Seen so far: 63005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|███████▍  | 12801/17266 [1:09:00<21:23,  3.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 12800: 2.0013\n",
            "Seen so far: 64005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|███████▌  | 13001/17266 [1:10:05<33:35,  2.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 13000: 1.9907\n",
            "Seen so far: 65005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 76%|███████▋  | 13201/17266 [1:11:12<37:41,  1.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 13200: 2.0045\n",
            "Seen so far: 66005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 78%|███████▊  | 13401/17266 [1:12:18<29:43,  2.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 13400: 1.9995\n",
            "Seen so far: 67005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 79%|███████▉  | 13601/17266 [1:13:23<27:24,  2.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 13600: 1.9976\n",
            "Seen so far: 68005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|███████▉  | 13801/17266 [1:14:30<23:03,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 13800: 2.0034\n",
            "Seen so far: 69005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 81%|████████  | 14001/17266 [1:15:35<18:21,  2.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 14000: 1.9994\n",
            "Seen so far: 70005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 82%|████████▏ | 14201/17266 [1:16:39<17:26,  2.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 14200: 2.0016\n",
            "Seen so far: 71005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 83%|████████▎ | 14401/17266 [1:17:44<15:21,  3.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 14400: 1.9973\n",
            "Seen so far: 72005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 85%|████████▍ | 14601/17266 [1:18:48<13:44,  3.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 14600: 1.9944\n",
            "Seen so far: 73005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 86%|████████▌ | 14801/17266 [1:19:52<12:52,  3.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 14800: 2.0001\n",
            "Seen so far: 74005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 87%|████████▋ | 15001/17266 [1:20:57<11:44,  3.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 15000: 1.9982\n",
            "Seen so far: 75005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 88%|████████▊ | 15201/17266 [1:22:03<11:19,  3.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 15200: 1.9977\n",
            "Seen so far: 76005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 89%|████████▉ | 15401/17266 [1:23:09<09:20,  3.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 15400: 2.0047\n",
            "Seen so far: 77005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 15601/17266 [1:24:11<08:22,  3.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 15600: 2.0042\n",
            "Seen so far: 78005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▏| 15801/17266 [1:25:16<07:02,  3.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 15800: 2.0009\n",
            "Seen so far: 79005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 93%|█████████▎| 16001/17266 [1:26:20<06:02,  3.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 16000: 1.9917\n",
            "Seen so far: 80005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 94%|█████████▍| 16201/17266 [1:27:25<05:14,  3.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 16200: 2.0057\n",
            "Seen so far: 81005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 95%|█████████▍| 16401/17266 [1:28:29<04:12,  3.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 16400: 2.0032\n",
            "Seen so far: 82005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|█████████▌| 16601/17266 [1:29:35<03:20,  3.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 16600: 2.0016\n",
            "Seen so far: 83005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 97%|█████████▋| 16801/17266 [1:30:41<02:16,  3.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 16800: 1.9951\n",
            "Seen so far: 84005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|█████████▊| 17001/17266 [1:31:48<01:17,  3.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 17000: 1.9914\n",
            "Seen so far: 85005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 17201/17266 [1:32:52<00:19,  3.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 17200: 1.9885\n",
            "Seen so far: 86005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17266/17266 [1:33:11<00:00,  3.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start of epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/17266 [00:00<3:04:22,  1.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 0: 1.9992\n",
            "Seen so far: 5 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 201/17266 [01:12<2:48:17,  1.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 200: 2.0053\n",
            "Seen so far: 1005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 401/17266 [02:23<2:25:49,  1.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 400: 1.9912\n",
            "Seen so far: 2005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 601/17266 [03:33<2:02:20,  2.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 600: 1.9885\n",
            "Seen so far: 3005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▍         | 801/17266 [04:37<1:39:45,  2.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 800: 2.0024\n",
            "Seen so far: 4005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 1001/17266 [05:42<1:39:31,  2.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 1000: 1.9961\n",
            "Seen so far: 5005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 1201/17266 [06:47<1:27:10,  3.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 1200: 1.9900\n",
            "Seen so far: 6005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|▊         | 1401/17266 [07:50<1:26:39,  3.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 1400: 1.9989\n",
            "Seen so far: 7005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  9%|▉         | 1601/17266 [08:53<1:21:35,  3.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 1600: 2.0027\n",
            "Seen so far: 8005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1801/17266 [09:56<1:21:41,  3.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 1800: 1.9944\n",
            "Seen so far: 9005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 12%|█▏        | 2001/17266 [11:01<1:20:44,  3.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 2000: 1.9967\n",
            "Seen so far: 10005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 2201/17266 [12:07<1:19:41,  3.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 2200: 2.0053\n",
            "Seen so far: 11005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 14%|█▍        | 2401/17266 [13:10<1:12:56,  3.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 2400: 2.0015\n",
            "Seen so far: 12005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 15%|█▌        | 2601/17266 [14:14<1:13:12,  3.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 2600: 2.0004\n",
            "Seen so far: 13005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|█▌        | 2801/17266 [15:20<1:14:17,  3.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 2800: 2.0048\n",
            "Seen so far: 14005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 3001/17266 [16:26<1:09:38,  3.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 3000: 2.0022\n",
            "Seen so far: 15005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 19%|█▊        | 3201/17266 [17:31<1:11:19,  3.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 3200: 2.0002\n",
            "Seen so far: 16005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|█▉        | 3401/17266 [18:35<1:06:38,  3.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 3400: 1.9951\n",
            "Seen so far: 17005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 21%|██        | 3601/17266 [19:40<1:12:17,  3.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 3600: 1.9993\n",
            "Seen so far: 18005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 22%|██▏       | 3801/17266 [20:46<1:08:27,  3.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 3800: 1.9921\n",
            "Seen so far: 19005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 23%|██▎       | 4001/17266 [21:50<1:03:18,  3.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 4000: 2.0007\n",
            "Seen so far: 20005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 24%|██▍       | 4201/17266 [22:54<1:03:46,  3.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 4200: 2.0035\n",
            "Seen so far: 21005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 4401/17266 [23:57<1:01:05,  3.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 4400: 2.0002\n",
            "Seen so far: 22005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|██▋       | 4601/17266 [25:02<1:02:07,  3.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 4600: 2.0022\n",
            "Seen so far: 23005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 28%|██▊       | 4801/17266 [26:05<57:57,  3.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 4800: 2.0026\n",
            "Seen so far: 24005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 29%|██▉       | 5001/17266 [27:10<57:53,  3.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 5000: 2.0087\n",
            "Seen so far: 25005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 5201/17266 [28:17<58:11,  3.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 5200: 1.9984\n",
            "Seen so far: 26005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 31%|███▏      | 5401/17266 [29:22<59:24,  3.33it/s]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 5400: 1.9956\n",
            "Seen so far: 27005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 32%|███▏      | 5601/17266 [30:31<57:19,  3.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 5600: 2.0018\n",
            "Seen so far: 28005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▎      | 5801/17266 [31:38<56:54,  3.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 5800: 1.9996\n",
            "Seen so far: 29005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 35%|███▍      | 6001/17266 [32:46<56:17,  3.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 6000: 1.9987\n",
            "Seen so far: 30005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 36%|███▌      | 6201/17266 [33:53<51:21,  3.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 6200: 1.9986\n",
            "Seen so far: 31005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 37%|███▋      | 6401/17266 [34:59<52:33,  3.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 6400: 2.0001\n",
            "Seen so far: 32005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 38%|███▊      | 6601/17266 [36:07<1:34:41,  1.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 6600: 1.9903\n",
            "Seen so far: 33005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 39%|███▉      | 6801/17266 [37:15<1:28:09,  1.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 6800: 2.0052\n",
            "Seen so far: 34005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|████      | 7001/17266 [38:21<1:14:55,  2.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 7000: 1.9972\n",
            "Seen so far: 35005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 42%|████▏     | 7201/17266 [39:27<1:09:27,  2.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 7200: 2.0000\n",
            "Seen so far: 36005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 43%|████▎     | 7401/17266 [40:33<1:01:21,  2.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 7400: 1.9958\n",
            "Seen so far: 37005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 44%|████▍     | 7601/17266 [41:38<55:42,  2.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 7600: 1.9989\n",
            "Seen so far: 38005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 45%|████▌     | 7801/17266 [42:44<50:59,  3.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 7800: 1.9928\n",
            "Seen so far: 39005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 46%|████▋     | 8001/17266 [43:49<51:22,  3.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 8000: 1.9868\n",
            "Seen so far: 40005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 47%|████▋     | 8201/17266 [44:55<49:37,  3.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 8200: 2.0096\n",
            "Seen so far: 41005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 49%|████▊     | 8401/17266 [46:00<47:08,  3.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 8400: 1.9959\n",
            "Seen so far: 42005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|████▉     | 8601/17266 [47:07<46:46,  3.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 8600: 2.0108\n",
            "Seen so far: 43005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 51%|█████     | 8801/17266 [48:14<45:57,  3.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 8800: 1.9921\n",
            "Seen so far: 44005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 52%|█████▏    | 9001/17266 [49:21<43:24,  3.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 9000: 2.0076\n",
            "Seen so far: 45005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 53%|█████▎    | 9201/17266 [50:27<42:37,  3.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 9200: 2.0045\n",
            "Seen so far: 46005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|█████▍    | 9401/17266 [51:34<40:31,  3.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 9400: 2.0099\n",
            "Seen so far: 47005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 56%|█████▌    | 9601/17266 [52:41<39:27,  3.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 9600: 2.0062\n",
            "Seen so far: 48005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 57%|█████▋    | 9801/17266 [53:46<37:37,  3.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 9800: 1.9972\n",
            "Seen so far: 49005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 58%|█████▊    | 10001/17266 [54:50<36:15,  3.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 10000: 2.0018\n",
            "Seen so far: 50005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 59%|█████▉    | 10201/17266 [55:56<35:15,  3.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 10200: 1.9827\n",
            "Seen so far: 51005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 10401/17266 [57:03<35:01,  3.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 10400: 1.9986\n",
            "Seen so far: 52005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 61%|██████▏   | 10601/17266 [58:09<34:22,  3.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 10600: 2.0070\n",
            "Seen so far: 53005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 63%|██████▎   | 10801/17266 [59:14<31:42,  3.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 10800: 2.0043\n",
            "Seen so far: 54005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 64%|██████▎   | 11001/17266 [1:00:21<31:01,  3.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 11000: 1.9876\n",
            "Seen so far: 55005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▍   | 11201/17266 [1:01:27<29:27,  3.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 11200: 2.0135\n",
            "Seen so far: 56005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 66%|██████▌   | 11401/17266 [1:02:33<27:52,  3.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 11400: 1.9872\n",
            "Seen so far: 57005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 11601/17266 [1:03:37<27:00,  3.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 11600: 1.9952\n",
            "Seen so far: 58005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 68%|██████▊   | 11801/17266 [1:04:42<26:37,  3.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 11800: 1.9985\n",
            "Seen so far: 59005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|██████▉   | 12001/17266 [1:05:47<25:53,  3.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 12000: 1.9912\n",
            "Seen so far: 60005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 71%|███████   | 12201/17266 [1:06:52<23:32,  3.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 12200: 1.9980\n",
            "Seen so far: 61005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 72%|███████▏  | 12401/17266 [1:07:56<23:33,  3.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 12400: 1.9979\n",
            "Seen so far: 62005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 73%|███████▎  | 12601/17266 [1:09:03<22:47,  3.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 12600: 1.9983\n",
            "Seen so far: 63005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|███████▍  | 12801/17266 [1:10:07<21:37,  3.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 12800: 2.0024\n",
            "Seen so far: 64005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|███████▌  | 13001/17266 [1:11:14<32:43,  2.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 13000: 2.0234\n",
            "Seen so far: 65005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 76%|███████▋  | 13201/17266 [1:12:21<35:32,  1.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 13200: 1.9908\n",
            "Seen so far: 66005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 78%|███████▊  | 13401/17266 [1:13:26<28:48,  2.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 13400: 2.0062\n",
            "Seen so far: 67005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 79%|███████▉  | 13601/17266 [1:14:33<25:47,  2.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 13600: 1.9839\n",
            "Seen so far: 68005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|███████▉  | 13801/17266 [1:15:40<21:09,  2.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 13800: 2.0073\n",
            "Seen so far: 69005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 81%|████████  | 14001/17266 [1:16:44<19:28,  2.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 14000: 2.0185\n",
            "Seen so far: 70005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 82%|████████▏ | 14201/17266 [1:17:50<17:10,  2.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 14200: 1.9841\n",
            "Seen so far: 71005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 83%|████████▎ | 14401/17266 [1:18:57<16:21,  2.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 14400: 2.0136\n",
            "Seen so far: 72005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 85%|████████▍ | 14601/17266 [1:20:03<14:36,  3.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 14600: 2.1733\n",
            "Seen so far: 73005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 86%|████████▌ | 14801/17266 [1:21:09<14:09,  2.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 14800: 2.0006\n",
            "Seen so far: 74005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 87%|████████▋ | 15001/17266 [1:22:15<12:18,  3.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 15000: 1.9968\n",
            "Seen so far: 75005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 88%|████████▊ | 15201/17266 [1:23:22<11:24,  3.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 15200: 1.9890\n",
            "Seen so far: 76005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 89%|████████▉ | 15401/17266 [1:24:30<09:30,  3.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 15400: 1.9925\n",
            "Seen so far: 77005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 15601/17266 [1:25:34<08:24,  3.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 15600: 2.0018\n",
            "Seen so far: 78005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▏| 15801/17266 [1:26:42<07:55,  3.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 15800: 2.0190\n",
            "Seen so far: 79005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 93%|█████████▎| 16001/17266 [1:27:51<06:57,  3.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 16000: 2.0021\n",
            "Seen so far: 80005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 94%|█████████▍| 16201/17266 [1:29:00<05:36,  3.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 16200: 2.0144\n",
            "Seen so far: 81005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 95%|█████████▍| 16401/17266 [1:30:08<04:17,  3.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 16400: 1.9999\n",
            "Seen so far: 82005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|█████████▌| 16601/17266 [1:31:15<03:29,  3.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 16600: 2.0118\n",
            "Seen so far: 83005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 97%|█████████▋| 16801/17266 [1:32:22<02:19,  3.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 16800: 2.0055\n",
            "Seen so far: 84005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|█████████▊| 17001/17266 [1:33:29<01:20,  3.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 17000: 1.9943\n",
            "Seen so far: 85005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 17201/17266 [1:34:36<00:19,  3.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 17200: 2.0125\n",
            "Seen so far: 86005 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17266/17266 [1:34:56<00:00,  3.03it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8G5v9ju4aNR",
        "outputId": "49c13056-f522-444e-ab93-3c09be8fb5de"
      },
      "source": [
        "tf.saved_model.save(simulator, './Zonal_Model_Sim')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ./Zonal_Model_Sim/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRU0P8443Aeq",
        "outputId": "98c7d10d-155d-45da-bf14-9de944e20cbe"
      },
      "source": [
        "compression = 10\n",
        "\n",
        "loss = np.ndarray.flatten(loss_values)\n",
        "loss = np.nanmean(np.pad(loss.astype(float), (0, compression - loss.size%compression), mode='constant', constant_values=np.NaN).reshape(-1, compression), axis=1)\n",
        "plt.plot(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f510ac1af10>]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_liH6pP5oHB1"
      },
      "source": [
        "# Rollouts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tch35ftasJfW"
      },
      "source": [
        "simulator = tf.saved_model.load('./Zonal_Model_Sim')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfpV_h2cAp28",
        "cellView": "form"
      },
      "source": [
        "#@title ### Rollout Function\n",
        "\n",
        "def rollout(simulator, features, num_steps):\n",
        "  \"\"\"Rolls out a trajectory by applying the model in sequence.\"\"\"\n",
        "  \n",
        "  initial_positions = features['pos'][0:INPUT_SEQUENCE_LENGTH]\n",
        "  initial_velocities = features['vel'][0:INPUT_SEQUENCE_LENGTH]\n",
        "  ground_truth_positions = features['pos'][INPUT_SEQUENCE_LENGTH:]\n",
        "  ground_truth_velocities = features['vel'][INPUT_SEQUENCE_LENGTH:]\n",
        "  global_context = None\n",
        "  \n",
        "  def step_fn(step, current_positions, current_velocities, pos_predictions, vel_predictions):\n",
        "    if global_context is None:\n",
        "      global_context_step = None\n",
        "    else:\n",
        "      global_context_step = global_context[\n",
        "          step + INPUT_SEQUENCE_LENGTH - 1][tf.newaxis]\n",
        "\n",
        "    next_position, next_velocity = simulator(\n",
        "        current_positions,\n",
        "        current_velocities,\n",
        "        n_particles_per_example=tf.constant([N]),\n",
        "        particle_types=None,\n",
        "        global_context=global_context_step)\n",
        "\n",
        "    # Update kinematic particles from prescribed trajectory.\n",
        "    kinematic_mask = get_kinematic_mask(tf.zeros(N))\n",
        "    next_position_ground_truth = ground_truth_positions[step,:,:]\n",
        "    next_velocity_ground_truth = ground_truth_velocities[step,:,:]\n",
        "    next_position = tf.where(tf.expand_dims(kinematic_mask,-1), next_position_ground_truth,\n",
        "                             next_position)\n",
        "    next_velocity = tf.where(tf.expand_dims(kinematic_mask,-1), next_velocity_ground_truth,\n",
        "                             next_velocity)\n",
        "    updated_pos_predictions = pos_predictions.write(step, next_position)\n",
        "    updated_vel_predictions = vel_predictions.write(step, next_velocity)\n",
        "\n",
        "    # Shift `current_positions`, removing the oldest position in the sequence\n",
        "    # and appending the next position at the end.\n",
        "    next_positions = tf.concat([current_positions[1:,:,:],\n",
        "                                next_position[tf.newaxis,:,:]], axis=0)\n",
        "    next_velocities = tf.concat([current_velocities[1:,:,:],\n",
        "                                next_velocity[tf.newaxis,:,:]], axis=0)\n",
        "    return (step + 1, next_positions, next_velocities, updated_pos_predictions, updated_vel_predictions)\n",
        "  \n",
        "  pos_predictions = tf.TensorArray(size=num_steps, dtype=tf.float32)\n",
        "  vel_predictions = tf.TensorArray(size=num_steps, dtype=tf.float32)\n",
        "\n",
        "  _, _, _, pos_predictions, vel_predictions = tf.while_loop(\n",
        "      cond=lambda step, state, vel, pos_prediction, vel_prediction: tf.less(step, num_steps),\n",
        "      body=step_fn,\n",
        "      loop_vars=(0, initial_positions, initial_velocities, pos_predictions, vel_predictions),\n",
        "      parallel_iterations=1)\n",
        "\n",
        "  output_dict = {\n",
        "      'initial_positions': initial_positions,\n",
        "      'initial_velocities': initial_velocities,\n",
        "      'pos_predicted_rollout': pos_predictions.stack(),\n",
        "      'vel_predicted_rollout': vel_predictions.stack(),\n",
        "      'ground_truth_position_rollout': ground_truth_positions,\n",
        "      'ground_truth_velocity_rollout': ground_truth_velocities\n",
        "  }\n",
        "\n",
        "  if global_context is not None:\n",
        "    output_dict['global_context'] = global_context\n",
        "  return output_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ANUWjzPAy6-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6829b4e4-47c3-43bc-ad83-929c5fce30b5"
      },
      "source": [
        "#@title ### Rollouts\n",
        "\n",
        "num_steps = metadata['sequence_length'] - INPUT_SEQUENCE_LENGTH\n",
        "\n",
        "ds = tf.data.TFRecordDataset(['train_datasets/microstates-' + str(i) + '.tfrecords' for i in range(1)])\n",
        "\n",
        "# map the record to features\n",
        "ds = ds.map(_parse_record)\n",
        "\n",
        "# map the features to tensors\n",
        "ds = ds.map(_parse_tensor)\n",
        "\n",
        "#print(ds.element_spec)\n",
        "\n",
        "def make_dataset(x):\n",
        "    # make a dataset from the time series tensor\n",
        "    pos = tf.data.Dataset.from_tensors((x['pos']))\n",
        "    vel = tf.data.Dataset.from_tensors((x['vel']))\n",
        "\n",
        "    data = tf.data.Dataset.zip((pos, vel))\n",
        "    #data = data.flat_map(lambda pos_ds,vel_ds: tf.data.Dataset.zip((pos_ds,vel_ds)))\n",
        "\n",
        "    return data\n",
        "\n",
        "# flatten the windowed dataset\n",
        "#ds = ds.map(make_dataset)\n",
        "\n",
        "for databatch in ds:\n",
        "    output = rollout(simulator,databatch,num_steps)\n",
        "    break\n",
        "\n",
        "output['metadata'] = metadata\n",
        "filename = '/zonal_model.pkl'\n",
        "with open(filename, 'wb') as file:\n",
        "    pickle.dump(output, file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-f11b1c8a8aee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdatabatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimulator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdatabatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-40616e0834c9>\u001b[0m in \u001b[0;36mrollout\u001b[0;34m(simulator, features, num_steps)\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m       \u001b[0mloop_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_velocities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvel_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m       parallel_iterations=1)\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m   output_dict = {\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m                   \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'in a future version'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m                   if date is None else ('after %s' % date), instructions)\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     doc = _add_deprecated_arg_value_notice_to_docstring(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop_v2\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001b[0m\n\u001b[1;32m   2539\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2540\u001b[0m       \u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2541\u001b[0;31m       return_same_structure=True)\n\u001b[0m\u001b[1;32m   2542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   2775\u001b[0m                                               list(loop_vars))\n\u001b[1;32m   2776\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m         \u001b[0mloop_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basetuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m           \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-40616e0834c9>\u001b[0m in \u001b[0;36mstep_fn\u001b[0;34m(step, current_positions, current_velocities, pos_predictions, vel_predictions)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mn_particles_per_example\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mparticle_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         global_context=global_context_step)\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Update kinematic particles from prescribed trajectory.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '_UserObject' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "027yPE8irqvU",
        "cellView": "form"
      },
      "source": [
        "#@title ### Plots of Predicted Pos/Vel vs Ground Truth\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "fig, axs = plt.subplots(2,2, figsize=(8, 6), facecolor='w', edgecolor='k')  \n",
        "labels = [\"x values\", \"y values\", \"u values\", \"v values\"]\n",
        "axs = axs.ravel()\n",
        "for pred_i in range(4):\n",
        "    pred_list = tf.concat([output['pos_predicted_rollout'], output['vel_predicted_rollout']], axis=-1)\n",
        "    true_list = tf.concat([output['ground_truth_position_rollout'], output['ground_truth_velocity_rollout']], axis=-1)\n",
        "\n",
        "    pred_vals = np.array([pp[:,pred_i] for pp in pred_list]).flatten()\n",
        "    true_vals = np.array([tt[:,pred_i] for tt in true_list]).flatten()\n",
        "\n",
        "    bin_means, bin_edges, binnumber = stats.binned_statistic(true_vals, pred_vals,bins=100)\n",
        "    bin_width = (bin_edges[1] - bin_edges[0])\n",
        "    bin_centers = bin_edges[1:] - bin_width/2\n",
        "\n",
        "    bin_stds, bin_edges, binnumber = stats.binned_statistic(true_vals, pred_vals,statistic='std',bins=100)\n",
        "\n",
        "\n",
        "    axs[pred_i].plot(bin_centers,bin_means,c='C0')\n",
        "\n",
        "    axs[pred_i].fill_between(bin_centers,bin_means-bin_stds,bin_means+bin_stds,color='C0',alpha=0.5)\n",
        "\n",
        "    xx = np.linspace(0,200,10)\n",
        "    vv = np.linspace(-1,1,10)\n",
        "    if pred_i<=1:\n",
        "        axs[pred_i].plot(xx,xx,c='k',ls='--')\n",
        "    else:\n",
        "        axs[pred_i].plot(vv,vv,c='k',ls='--')\n",
        "    axs[pred_i].set_ylabel('GNN prediction of parameter')\n",
        "    axs[pred_i].set_xlabel('True parameter')\n",
        "    axs[pred_i].set_title(labels[pred_i])\n",
        "\n",
        "fig.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXpla4UdkJtD"
      },
      "source": [
        "#@title ### Order Parameter Plot\n",
        "\n",
        "pred_av_velocity = np.mean(output['vel_predicted_rollout'],axis=1)\n",
        "true_av_velocity = np.mean(output['ground_truth_velocity_rollout'],axis=1)\n",
        "pred_std = np.std(output['vel_predicted_rollout'],axis=1)\n",
        "true_std = np.std(output['ground_truth_velocity_rollout'],axis=1)\n",
        "\n",
        "pred_order_parameter = np.linalg.norm(pred_av_velocity,axis=1)/vs\n",
        "true_order_parameter = np.linalg.norm(true_av_velocity,axis=1)/vs\n",
        "\n",
        "line = np.linspace(0,max(pred_order_parameter),10)\n",
        "#plt.scatter(true_order_parameter, pred_order_parameter)\n",
        "#plt.plot(line, line)\n",
        "\n",
        "order = np.argsort(true_order_parameter)\n",
        "true_order_parameter = true_order_parameter[order]\n",
        "pred_order_parameter = pred_order_parameter[order]\n",
        "\n",
        "bin_means, bin_edges, binnumber = stats.binned_statistic(true_order_parameter, pred_order_parameter,bins=100)\n",
        "bin_width = (bin_edges[1] - bin_edges[0])\n",
        "bin_centers = bin_edges[1:] - bin_width/2\n",
        "\n",
        "bin_stds, bin_edges, binnumber = stats.binned_statistic(true_order_parameter, pred_order_parameter,statistic='std',bins=100)\n",
        "\n",
        "plt.plot(bin_centers,bin_means,c='C0')\n",
        "plt.xlabel(\"Ground Truth Order Parameter\")\n",
        "plt.ylabel(\"Predicted Order Parameter\")\n",
        "plt.fill_between(bin_centers,bin_means-bin_stds,bin_means+bin_stds,color='C0',alpha=0.5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XPJcbaJoOxM",
        "cellView": "form"
      },
      "source": [
        "#@title ### Render Rollouts\n",
        "\n",
        "TYPE_TO_COLOR = {\n",
        "    3: \"black\",  # Boundary particles.\n",
        "    0: \"green\",  # Rigid solids.\n",
        "    7: \"magenta\",  # Goop.\n",
        "    6: \"gold\",  # Sand.\n",
        "    5: \"blue\",  # Water.\n",
        "}\n",
        "\n",
        "step_stride = 3\n",
        "block_on_show = True\n",
        "\n",
        "with open(filename, \"rb\") as file:\n",
        "  rollout_data = pickle.load(file)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "plot_info = []\n",
        "Q={}\n",
        "for ax_i, (label, (rollout_field_1, rollout_field_2)) in enumerate(\n",
        "    [(\"Ground truth\", (\"ground_truth_position_rollout\", \"ground_truth_velocity_rollout\")),\n",
        "      (\"Prediction\", (\"pos_predicted_rollout\", \"vel_predicted_rollout\"))]):\n",
        "  # Append the initial positions to get the full trajectory.\n",
        "  pos_trajectory = np.concatenate([\n",
        "      rollout_data[\"initial_positions\"],\n",
        "      rollout_data[rollout_field_1]], axis=0)\n",
        "  vel_trajectory = np.concatenate([\n",
        "      rollout_data[\"initial_velocities\"],\n",
        "      rollout_data[rollout_field_2]], axis=0)\n",
        "  ax = axes[ax_i]\n",
        "  ax.set_title(label)\n",
        "  bounds = rollout_data[\"metadata\"][\"bounds\"]\n",
        "  ax.set_xlim(bounds[0][0], bounds[0][1])\n",
        "  ax.set_ylim(bounds[1][0], bounds[1][1])\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "  ax.set_aspect(1.)\n",
        "  points = {\n",
        "      particle_type: ax.plot([], [], \"o\", ms=2, color=color)[0]\n",
        "      for particle_type, color in TYPE_TO_COLOR.items()}\n",
        "  X, Y, U, V = pos_trajectory[0,:,0], pos_trajectory[0,:,1], vel_trajectory[0,:,0], vel_trajectory[0,:,1]\n",
        "  Q[f'{ax_i}'] = ax.quiver(X, Y, U, V, pivot='mid', color='b', units='inches')\n",
        "  plot_info.append((ax, pos_trajectory, vel_trajectory, points))\n",
        "\n",
        "\n",
        "num_steps = pos_trajectory.shape[0]\n",
        "\n",
        "\n",
        "def update_quiver(num):\n",
        "  for i, (ax, pos_trajectory, vel_trajectory, _) in enumerate(plot_info):\n",
        "    U, V = vel_trajectory[num,:,0], vel_trajectory[num,:,1]\n",
        "    offsets = pos_trajectory[num]\n",
        "\n",
        "    Q[f'{i}'].set_offsets(offsets)\n",
        "    Q[f'{i}'].set_UVC(U,V)\n",
        "  return axes\n",
        "\n",
        "anim = animation.FuncAnimation(fig, update_quiver,\n",
        "                               interval=50, frames=np.arange(0, num_steps, step_stride),blit=False)\n",
        "#plt.show(block=block_on_show)\n",
        "rc('animation', html='jshtml')\n",
        "anim\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}