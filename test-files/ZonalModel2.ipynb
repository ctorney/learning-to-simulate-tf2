{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ZonalModel2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPIeRfSUNkRCOryc2Z4CaRh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ctorney/learning-to-simulate-tf2/blob/main/test-files/ZonalModel2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3TcOyZ_njXA"
      },
      "source": [
        "!pip install \"graph_nets>=1.1\" \"dm-sonnet>=2.0.0b0\" \"tensorflow_probability\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XemapxUWfYJZ"
      },
      "source": [
        "#@title ### Imports\n",
        "\n",
        "import numpy as np\n",
        "from math import *\n",
        "import sys\n",
        "import random\n",
        "import time\n",
        "\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "import functools\n",
        "\n",
        "from sklearn import neighbors\n",
        "\n",
        "from typing import Callable\n",
        "\n",
        "import graph_nets as gn\n",
        "import sonnet as snt\n",
        "import collections\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djgc1AVtrAdy"
      },
      "source": [
        "# Generate Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "o_lpv__-npkr"
      },
      "source": [
        "#@title ### Zonal Model\n",
        "\n",
        "def get_record(group_id,pos,vel):\n",
        "    feature = { 'group_id': tf.train.Feature(int64_list=tf.train.Int64List(value=[group_id])),\n",
        "                'pos': tf.train.Feature(bytes_list=tf.train.BytesList(value=[pos.numpy()])),\n",
        "                'vel': tf.train.Feature(bytes_list=tf.train.BytesList(value=[vel.numpy()]))\n",
        "                }\n",
        "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "\n",
        "\n",
        "class zonal_model:\n",
        "    def __init__(self, N, timesteps, discard, repeat, L, dt, save_interval,train_directory='train_datasets', valid_directory='valid_datasets', disable_progress=False):\n",
        "        self.N = N\n",
        "        self.timesteps = timesteps\n",
        "        self.discard = discard\n",
        "        self.B = repeat  # repeat for B batches\n",
        "        self.L = L\n",
        "        self.dt = dt\n",
        "        self.save_interval = save_interval\n",
        "        \n",
        "        self.micro_state = np.zeros((self.B, (self.timesteps - self.discard)//self.save_interval, N, 4),dtype=np.float32)\n",
        "\n",
        "        self.sim_counter=0\n",
        "\n",
        "        if not os.path.exists(train_directory):\n",
        "            os.makedirs(train_directory)\n",
        "\n",
        "        if not os.path.exists(valid_directory):\n",
        "            os.makedirs(valid_directory)\n",
        "\n",
        "        self.train_directory = train_directory\n",
        "        self.valid_directory = valid_directory\n",
        "\n",
        "        # turn progress bar on or off\n",
        "        self.disable_progress = disable_progress\n",
        "\n",
        "        self.valid_fraction = 0.1\n",
        "        \n",
        "    def initialise_state(self):\n",
        "\n",
        "        self.positions = tf.random.uniform((self.B,self.N,2),0.5*self.L, 0.5*self.L+20) #0,self.L)\n",
        "        #self.positions = tf.random.uniform((self.B,self.N,2),0, self.L) \n",
        "        self.angles = tf.random.uniform((self.B,self.N,1), 0, 2*pi) #\n",
        "        \n",
        "\n",
        "\n",
        "    def run_sim(self, *params):\n",
        "\n",
        "        eta, Ra, Ro, Rr, vs, va, sigma = params\n",
        "        \n",
        "        record_file = self.train_directory + '/microstates-' + str(self.sim_counter) + '.tfrecords'\n",
        "        self.writer = tf.io.TFRecordWriter(record_file) \n",
        "        \n",
        "        valid_file = self.valid_directory + '/microstates-' + str(self.sim_counter) + '.tfrecords'\n",
        "        self.validwriter = tf.io.TFRecordWriter(valid_file) \n",
        "        \n",
        "        # tensorflow function to run an update step\n",
        "        @tf.function\n",
        "        def update_tf(X, A):\n",
        "            cos_A = tf.math.cos(A)\n",
        "            sin_A = tf.math.sin(A)\n",
        "\n",
        "\n",
        "            Xx = tf.expand_dims(X[...,0],-1)\n",
        "            dx = -Xx + tf.linalg.matrix_transpose(Xx)\n",
        "            dx = tf.where(dx>0.5*self.L, dx-self.L, dx)\n",
        "            dx = tf.where(dx<-0.5*self.L, dx+self.L, dx)\n",
        "\n",
        "            Xy = tf.expand_dims(X[...,1],-1)\n",
        "            dy = -Xy + tf.linalg.matrix_transpose(Xy)\n",
        "            dy = tf.where(dy>0.5*self.L, dy-self.L, dy)\n",
        "            dy = tf.where(dy<-0.5*self.L, dy+self.L, dy)\n",
        "\n",
        "\n",
        "            angle_to_neigh = tf.math.atan2(dy, dx)\n",
        "            cos_N = tf.math.cos(angle_to_neigh)\n",
        "            sin_N = tf.math.sin(angle_to_neigh)\n",
        "            rel_angle_to_neigh = angle_to_neigh - A\n",
        "            rel_angle_to_neigh = tf.math.atan2(tf.math.sin(rel_angle_to_neigh), tf.math.cos(rel_angle_to_neigh))\n",
        "            \n",
        "            dist = tf.math.sqrt(tf.square(dx)+tf.square(dy))\n",
        "    \n",
        "            # repulsion \n",
        "            rep_x = tf.where(dist<=Rr, -dx, tf.zeros_like(dx))\n",
        "            rep_x = tf.where(rel_angle_to_neigh<0.5*va, rep_x, tf.zeros_like(rep_x))\n",
        "            rep_x = tf.where(rel_angle_to_neigh>-0.5*va, rep_x, tf.zeros_like(rep_x))\n",
        "            rep_x = tf.math.divide_no_nan(rep_x,tf.math.square(dist))\n",
        "            rep_x = tf.reduce_sum(rep_x,axis=2)\n",
        "\n",
        "            rep_y = tf.where(dist<=Rr, -dy, tf.zeros_like(dy))\n",
        "            rep_y = tf.where(rel_angle_to_neigh<0.5*va, rep_y, tf.zeros_like(rep_y))\n",
        "            rep_y = tf.where(rel_angle_to_neigh>-0.5*va, rep_y, tf.zeros_like(rep_y))\n",
        "            rep_y = tf.math.divide_no_nan(rep_y,tf.math.square(dist))\n",
        "            rep_y = tf.reduce_sum(rep_y,axis=2)\n",
        "\n",
        "            # alignment \n",
        "            align_x = tf.where(dist<=Ro, cos_A, tf.zeros_like(cos_A))\n",
        "            align_x = tf.where(rel_angle_to_neigh<0.5*va, align_x, tf.zeros_like(align_x))\n",
        "            align_x = tf.where(rel_angle_to_neigh>-0.5*va, align_x, tf.zeros_like(align_x))\n",
        "            align_x = tf.reduce_sum(align_x,axis=1)\n",
        "            \n",
        "            align_y = tf.where(dist<=Ro, sin_A, tf.zeros_like(sin_A))\n",
        "            align_y = tf.where(rel_angle_to_neigh<0.5*va, align_y, tf.zeros_like(align_y))\n",
        "            align_y = tf.where(rel_angle_to_neigh>-0.5*va, align_y, tf.zeros_like(align_y))\n",
        "            align_y = tf.reduce_sum(align_y,axis=1)\n",
        "\n",
        "            al_norm = tf.math.sqrt(align_x**2+align_y**2)\n",
        "            align_x = tf.math.divide_no_nan(align_x,al_norm)\n",
        "            align_y = tf.math.divide_no_nan(align_y,al_norm)\n",
        "\n",
        "            # attractive interactions\n",
        "            attr_x = tf.where(dist<=Ra, dx, tf.zeros_like(dx))\n",
        "            attr_x = tf.where(rel_angle_to_neigh<0.5*va, attr_x, tf.zeros_like(attr_x))\n",
        "            attr_x = tf.where(rel_angle_to_neigh>-0.5*va, attr_x, tf.zeros_like(attr_x))\n",
        "            attr_x = tf.reduce_sum(attr_x,axis=2)\n",
        "\n",
        "            attr_y = tf.where(dist<=Ra, dy, tf.zeros_like(dy))\n",
        "            attr_y = tf.where(rel_angle_to_neigh<0.5*va, attr_y, tf.zeros_like(attr_y))\n",
        "            attr_y = tf.where(rel_angle_to_neigh>-0.5*va, attr_y, tf.zeros_like(attr_y))\n",
        "            attr_y = tf.reduce_sum(attr_y,axis=2)\n",
        "\n",
        "            at_norm = tf.math.sqrt(attr_x**2+attr_y**2)\n",
        "            attr_x = tf.math.divide_no_nan(attr_x,at_norm)\n",
        "            attr_y = tf.math.divide_no_nan(attr_y,at_norm)\n",
        "\n",
        "            # combine angles and convert to desired angle change\n",
        "            social_x = rep_x + align_x + attr_x\n",
        "            social_y = rep_y + align_y + attr_y\n",
        "\n",
        "            d_angle = tf.math.atan2(social_y,social_x)\n",
        "            d_angle = tf.expand_dims(d_angle,-1)\n",
        "\n",
        "            \n",
        "            d_angle = tf.math.atan2((1-eta)*tf.math.sin(d_angle) + eta*sin_A, (1-eta)*tf.math.cos(d_angle) + eta*cos_A)\n",
        "\n",
        "            d_angle = d_angle - A\n",
        "            d_angle = tf.where(d_angle>pi, d_angle-2*pi, d_angle)\n",
        "            d_angle = tf.where(d_angle<-pi, d_angle+2*pi, d_angle)\n",
        "\n",
        "\n",
        "            # add perception noise\n",
        "            noise = tf.random.normal(shape=(self.B,self.N,1),mean=0,stddev=sigma*(self.dt**0.5))\n",
        "            d_angle = d_angle + noise\n",
        "            \n",
        "            # restrict to maximum turning angle\n",
        "            #d_angle = tf.where(tf.math.abs(d_angle)>eta*self.dt, tf.math.sign(d_angle)*eta*self.dt, d_angle)\n",
        "            \n",
        "            # rotate headings\n",
        "            A = A + d_angle\n",
        "            \n",
        "            # update positions\n",
        "            velocity = self.dt*vs*tf.concat([tf.cos(A),tf.sin(A)],axis=-1)\n",
        "            X += velocity\n",
        "\n",
        "            # add periodic boundary conditions\n",
        "            A = tf.where(A<-pi,  A+2*pi, A)\n",
        "            A = tf.where(A>pi, A-2*pi, A)\n",
        "\n",
        "            X = tf.where(X>self.L, X-self.L, X)\n",
        "            X = tf.where(X<0, X+self.L, X)\n",
        "\n",
        "            X = tf.where(X>self.L, X-self.L, X)\n",
        "            X = tf.where(X<0, X+self.L, X)\n",
        "\n",
        "            return X, A\n",
        "            \n",
        "        self.initialise_state()\n",
        "\n",
        "        counter=0\n",
        "        for i in tqdm(range(self.timesteps),disable=self.disable_progress):\n",
        "            self.positions, self.angles = update_tf(self.positions,  self.angles)\n",
        "            if i>=self.discard:\n",
        "                if i%self.save_interval==0:\n",
        "                    # store in an array in case we want to visualise\n",
        "                    self.micro_state[:,counter,:,0:2] = self.positions.numpy()\n",
        "                    self.micro_state[:,counter,:,2:3] = np.cos(self.angles.numpy())\n",
        "                    self.micro_state[:,counter,:,3:4] = np.sin(self.angles.numpy())\n",
        "                        \n",
        "                    \n",
        "\n",
        "                    counter = counter + 1\n",
        "\n",
        "        for b in range(self.B):\n",
        "            self.save_tf_record(b)\n",
        "\n",
        "        self.writer.close()\n",
        "        self.validwriter.close()\n",
        "        self.sim_counter+=1\n",
        "        return \n",
        "\n",
        "    def save_tf_record(self, b):\n",
        "        pos =  tf.io.serialize_tensor(self.micro_state[b,:,:,0:2])\n",
        "        vel =  tf.io.serialize_tensor(self.micro_state[b,:,:,2:4])\n",
        "\n",
        "        tf_record = get_record(b,pos,vel)\n",
        "        if b> self.B*self.valid_fraction:\n",
        "            self.writer.write(tf_record.SerializeToString())\n",
        "        else:\n",
        "            self.validwriter.write(tf_record.SerializeToString())\n",
        "\n",
        "        \n",
        "        return "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "W-iHCH3WaZuO"
      },
      "source": [
        "#@title ### Params\n",
        "\n",
        "n_points=50 #10\n",
        "\n",
        "param_values = np.linspace(0,25,n_points)\n",
        "L= 200\n",
        "N= 100 \n",
        "repeat = 100\n",
        "discard = 500\n",
        "timesteps = 500\n",
        "save_interval=10\n",
        "dt=0.1 \n",
        "\n",
        "\n",
        "sim = zonal_model(N,timesteps=timesteps+discard,discard=discard,L=L,repeat=repeat, dt=dt,save_interval=save_interval,disable_progress=False)\n",
        "\n",
        "latt=0  # adapt\n",
        "lrep= 1 # adapt\n",
        "lali= 5 # adapt\n",
        "eta=0.9 # adapt\n",
        "va=2*pi # adapt\n",
        "vs=5 # fix \n",
        "sigma=0.1 "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "cellView": "form",
        "id": "kiyL0DASnrzU",
        "outputId": "4a2b6ff2-e734-48a1-8258-b3275789726b"
      },
      "source": [
        "#@title ### Create Training Data\n",
        "\n",
        "def evaluate_zonal_model(X):\n",
        "    sim.run_sim(eta, latt, X, lrep, vs, va, sigma)\n",
        "    return\n",
        "\n",
        "evaluate_zonal_model(0)\n",
        "\n",
        "\"\"\"\n",
        "for i in tqdm(range(param_values.shape[0])):\n",
        "    evaluate_zonal_model(param_values[i])\n",
        "\"\"\""
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [02:11<00:00,  7.59it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfor i in tqdm(range(param_values.shape[0])):\\n    evaluate_zonal_model(param_values[i])\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2LGVb74rFW9"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BL8x-cdXhgo"
      },
      "source": [
        "#@title ### connectivity_utils\n",
        "\n",
        "def _compute_connectivity(positions, radius, add_self_edges):\n",
        "  \"\"\"Get the indices of connected edges with radius connectivity.\n",
        "  Args:\n",
        "    positions: Positions of nodes in the graph. Shape:\n",
        "      [num_nodes_in_graph, num_dims].\n",
        "    radius: Radius of connectivity.\n",
        "    add_self_edges: Whether to include self edges or not.\n",
        "  Returns:\n",
        "    senders indices [num_edges_in_graph]\n",
        "    receiver indices [num_edges_in_graph]\n",
        "  \"\"\"\n",
        "  tree = neighbors.KDTree(positions)\n",
        "  receivers_list = tree.query_radius(positions, r=radius)\n",
        "  num_nodes = len(positions)\n",
        "  senders = np.repeat(range(num_nodes), [len(a) for a in receivers_list])\n",
        "  receivers = np.concatenate(receivers_list, axis=0)\n",
        "\n",
        "  if not add_self_edges:\n",
        "    # Remove self edges.\n",
        "    mask = senders != receivers\n",
        "    senders = senders[mask]\n",
        "    receivers = receivers[mask]\n",
        "\n",
        "  return senders, receivers\n",
        "\n",
        "\n",
        "def _compute_connectivity_for_batch(\n",
        "    positions, n_node, radius, add_self_edges):\n",
        "  \"\"\"`compute_connectivity` for a batch of graphs.\n",
        "  Args:\n",
        "    positions: Positions of nodes in the batch of graphs. Shape:\n",
        "      [num_nodes_in_batch, num_dims].\n",
        "    n_node: Number of nodes for each graph in the batch. Shape:\n",
        "      [num_graphs in batch].\n",
        "    radius: Radius of connectivity.\n",
        "    add_self_edges: Whether to include self edges or not.\n",
        "  Returns:\n",
        "    senders indices [num_edges_in_batch]\n",
        "    receiver indices [num_edges_in_batch]\n",
        "    number of edges per graph [num_graphs_in_batch]\n",
        "  \"\"\"\n",
        "\n",
        "  # TODO(alvarosg): Consider if we want to support batches here or not.\n",
        "  # Separate the positions corresponding to particles in different graphs.\n",
        "  positions_per_graph_list = np.split(positions, np.cumsum(n_node[:-1]), axis=0)\n",
        "  receivers_list = []\n",
        "  senders_list = []\n",
        "  n_edge_list = []\n",
        "  num_nodes_in_previous_graphs = 0\n",
        "\n",
        "  # Compute connectivity for each graph in the batch.\n",
        "  for positions_graph_i in positions_per_graph_list:\n",
        "    senders_graph_i, receivers_graph_i = _compute_connectivity(\n",
        "        positions_graph_i, radius, add_self_edges)\n",
        "\n",
        "    num_edges_graph_i = len(senders_graph_i)\n",
        "    n_edge_list.append(num_edges_graph_i)\n",
        "\n",
        "    # Because the inputs will be concatenated, we need to add offsets to the\n",
        "    # sender and receiver indices according to the number of nodes in previous\n",
        "    # graphs in the same batch.\n",
        "    receivers_list.append(receivers_graph_i + num_nodes_in_previous_graphs)\n",
        "    senders_list.append(senders_graph_i + num_nodes_in_previous_graphs)\n",
        "\n",
        "    num_nodes_graph_i = len(positions_graph_i)\n",
        "    num_nodes_in_previous_graphs += num_nodes_graph_i\n",
        "\n",
        "  # Concatenate all of the results.\n",
        "  senders = np.concatenate(senders_list, axis=0).astype(np.int32)\n",
        "  receivers = np.concatenate(receivers_list, axis=0).astype(np.int32)\n",
        "  n_edge = np.stack(n_edge_list).astype(np.int32)\n",
        "\n",
        "  return senders, receivers, n_edge\n",
        "\n",
        "\n",
        "def compute_connectivity_for_batch_pyfunc(\n",
        "    positions, n_node, radius, add_self_edges=True):\n",
        "  \"\"\"`_compute_connectivity_for_batch` wrapped in a pyfunc.\"\"\"\n",
        "  partial_fn = functools.partial(\n",
        "      _compute_connectivity_for_batch, add_self_edges=add_self_edges)\n",
        "  senders, receivers, n_edge = tf.py_function(\n",
        "      partial_fn,\n",
        "      [positions, n_node, radius],\n",
        "      [tf.int32, tf.int32, tf.int32])\n",
        "  senders.set_shape([None])\n",
        "  receivers.set_shape([None])\n",
        "  n_edge.set_shape(n_node.get_shape())\n",
        "  return senders, receivers, n_edge"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEx3i6DgXslW",
        "cellView": "form"
      },
      "source": [
        "#@title ### graph_network\n",
        "\n",
        "Reducer = Callable[[tf.Tensor, tf.Tensor, tf.Tensor], tf.Tensor]\n",
        "\n",
        "\n",
        "def build_mlp(\n",
        "    hidden_size: int, num_hidden_layers: int, output_size: int) -> snt.Module:\n",
        "  \"\"\"Builds an MLP.\"\"\"\n",
        "  return snt.nets.MLP(\n",
        "      output_sizes=[hidden_size] * num_hidden_layers + [output_size])\n",
        "\n",
        "\n",
        "class EncodeProcessDecode(snt.Module):\n",
        "  \"\"\"Encode-Process-Decode function approximator for learnable simulator.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      latent_size: int,\n",
        "      mlp_hidden_size: int,\n",
        "      mlp_num_hidden_layers: int,\n",
        "      num_message_passing_steps: int,\n",
        "      output_size: int,\n",
        "      reducer: Reducer = tf.math.unsorted_segment_sum,\n",
        "      name: str = \"EncodeProcessDecode\"):\n",
        "    \"\"\"Inits the model.\n",
        "    Args:\n",
        "      latent_size: Size of the node and edge latent representations.\n",
        "      mlp_hidden_size: Hidden layer size for all MLPs.\n",
        "      mlp_num_hidden_layers: Number of hidden layers in all MLPs.\n",
        "      num_message_passing_steps: Number of message passing steps.\n",
        "      output_size: Output size of the decode node representations as required\n",
        "        by the downstream update function.\n",
        "      reducer: Reduction to be used when aggregating the edges in the nodes in\n",
        "        the interaction network. This should be a callable whose signature\n",
        "        matches tf.math.unsorted_segment_sum.\n",
        "      name: Name of the model.\n",
        "    \"\"\"\n",
        "\n",
        "    super().__init__(name=name)\n",
        "\n",
        "    self._latent_size = latent_size\n",
        "    self._mlp_hidden_size = mlp_hidden_size\n",
        "    self._mlp_num_hidden_layers = mlp_num_hidden_layers\n",
        "    self._num_message_passing_steps = num_message_passing_steps\n",
        "    self._output_size = output_size\n",
        "    self._reducer = reducer\n",
        "\n",
        "    #with self._enter_variable_scope():\n",
        "    self._networks_builder()\n",
        "\n",
        "  def __call__(self, input_graph: gn.graphs.GraphsTuple) -> tf.Tensor:\n",
        "    \"\"\"Forward pass of the learnable dynamics model.\"\"\"\n",
        "\n",
        "    # Encode the input_graph.\n",
        "    latent_graph_0 = self._encode(input_graph)\n",
        "\n",
        "    # Do `m` message passing steps in the latent graphs.\n",
        "    latent_graph_m = self._process(latent_graph_0)\n",
        "\n",
        "    # Decode from the last latent graph.\n",
        "    return self._decode(latent_graph_m)\n",
        "\n",
        "  def _networks_builder(self):\n",
        "    \"\"\"Builds the networks.\"\"\"\n",
        "    def build_mlp_with_layer_norm():\n",
        "      mlp = build_mlp(\n",
        "          hidden_size=self._mlp_hidden_size,\n",
        "          num_hidden_layers=self._mlp_num_hidden_layers,\n",
        "          output_size=self._latent_size)\n",
        "      return snt.Sequential([mlp, snt.LayerNorm(axis=slice(1, None),create_scale=False,create_offset=False)])\n",
        "\n",
        "    # The encoder graph network independently encodes edge and node features.\n",
        "    encoder_kwargs = dict(\n",
        "        edge_model_fn=build_mlp_with_layer_norm,\n",
        "        node_model_fn=build_mlp_with_layer_norm)\n",
        "    self._encoder_network = gn.modules.GraphIndependent(**encoder_kwargs)\n",
        "    # Create `num_message_passing_steps` graph networks with unshared parameters\n",
        "    # that update the node and edge latent features.\n",
        "    # Note that we can use `modules.InteractionNetwork` because\n",
        "    # it also outputs the messages as updated edge latent features.\n",
        "    self._processor_networks = []\n",
        "    for _ in range(self._num_message_passing_steps):\n",
        "      self._processor_networks.append(\n",
        "          gn.modules.InteractionNetwork(\n",
        "              edge_model_fn=build_mlp_with_layer_norm,\n",
        "              node_model_fn=build_mlp_with_layer_norm,\n",
        "              reducer=self._reducer))\n",
        "    # The decoder MLP decodes node latent features into the output size.\n",
        "    self._decoder_network = build_mlp(\n",
        "        hidden_size=self._mlp_hidden_size,\n",
        "        num_hidden_layers=self._mlp_num_hidden_layers,\n",
        "        output_size=self._output_size)\n",
        "  def _encode(\n",
        "      self, input_graph: gn.graphs.GraphsTuple) -> gn.graphs.GraphsTuple:\n",
        "    \"\"\"Encodes the input graph features into a latent graph.\"\"\"\n",
        "\n",
        "    # Copy the globals to all of the nodes, if applicable.\n",
        "    if input_graph.globals is not None:\n",
        "      broadcasted_globals = gn.blocks.broadcast_globals_to_nodes(input_graph)\n",
        "      input_graph = input_graph.replace(\n",
        "          nodes=tf.concat([input_graph.nodes, broadcasted_globals], axis=-1),\n",
        "          globals=None)\n",
        "\n",
        "    # Encode the node and edge features.\n",
        "    latent_graph_0 = self._encoder_network(input_graph)\n",
        "    return latent_graph_0\n",
        "\n",
        "  def _process(\n",
        "      self, latent_graph_0: gn.graphs.GraphsTuple) -> gn.graphs.GraphsTuple:\n",
        "    \"\"\"Processes the latent graph with several steps of message passing.\"\"\"\n",
        "\n",
        "    # Do `m` message passing steps in the latent graphs.\n",
        "    # (In the shared parameters case, just reuse the same `processor_network`)\n",
        "    latent_graph_prev_k = latent_graph_0\n",
        "    latent_graph_k = latent_graph_0\n",
        "    for processor_network_k in self._processor_networks:\n",
        "      latent_graph_k = self._process_step(\n",
        "          processor_network_k, latent_graph_prev_k)\n",
        "      latent_graph_prev_k = latent_graph_k\n",
        "\n",
        "    latent_graph_m = latent_graph_k\n",
        "    return latent_graph_m\n",
        "\n",
        "  def _process_step(\n",
        "      self, processor_network_k: snt.Module,\n",
        "      latent_graph_prev_k: gn.graphs.GraphsTuple) -> gn.graphs.GraphsTuple:\n",
        "    \"\"\"Single step of message passing with node/edge residual connections.\"\"\"\n",
        "\n",
        "    # One step of message passing.\n",
        "    latent_graph_k = processor_network_k(latent_graph_prev_k)\n",
        "\n",
        "    # Add residuals.\n",
        "    latent_graph_k = latent_graph_k.replace(\n",
        "        nodes=latent_graph_k.nodes+latent_graph_prev_k.nodes,\n",
        "        edges=latent_graph_k.edges+latent_graph_prev_k.edges)\n",
        "    return latent_graph_k\n",
        "\n",
        "  def _decode(self, latent_graph: gn.graphs.GraphsTuple) -> tf.Tensor:\n",
        "    \"\"\"Decodes from the latent graph.\"\"\"\n",
        "    return self._decoder_network(latent_graph.nodes)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnGLHy8zTbYo",
        "cellView": "form"
      },
      "source": [
        "#@title ### learned_simulator\n",
        "\n",
        "STD_EPSILON = 1e-8\n",
        "\n",
        "\n",
        "class LearnedSimulator(snt.Module):\n",
        "  \"\"\"Learned simulator from https://arxiv.org/pdf/2002.09405.pdf.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      num_dimensions,\n",
        "      connectivity_radius,\n",
        "      graph_network_kwargs,\n",
        "      boundaries,\n",
        "      normalization_stats,\n",
        "      num_particle_types,\n",
        "      particle_type_embedding_size,\n",
        "      name=\"LearnedSimulator\"):\n",
        "    \"\"\"Inits the model.\n",
        "    Args:\n",
        "      num_dimensions: Dimensionality of the problem.\n",
        "      connectivity_radius: Scalar with the radius of connectivity.\n",
        "      graph_network_kwargs: Keyword arguments to pass to the learned part\n",
        "        of the graph network `model.EncodeProcessDecode`.\n",
        "      boundaries: List of 2-tuples, containing the lower and upper boundaries of\n",
        "        the cuboid containing the particles along each dimensions, matching\n",
        "        the dimensionality of the problem.\n",
        "      normalization_stats: Dictionary with statistics with keys \"acceleration\"\n",
        "        and \"velocity\", containing a named tuple for each with mean and std\n",
        "        fields, matching the dimensionality of the problem.\n",
        "      num_particle_types: Number of different particle types.\n",
        "      particle_type_embedding_size: Embedding size for the particle type.\n",
        "      name: Name of the Sonnet module.\n",
        "    \"\"\"\n",
        "    super().__init__(name=name)\n",
        "\n",
        "    self._connectivity_radius = connectivity_radius\n",
        "    self._num_particle_types = num_particle_types\n",
        "    self._boundaries = boundaries\n",
        "    self._normalization_stats = normalization_stats\n",
        "    #with self._enter_variable_scope():\n",
        "    self._graph_network = EncodeProcessDecode(output_size=num_dimensions, **graph_network_kwargs)\n",
        "\n",
        "    if self._num_particle_types > 1:\n",
        "        self._particle_type_embedding = tf.compat.v1.get_variable(\n",
        "            \"particle_embedding\",\n",
        "            [self._num_particle_types, particle_type_embedding_size],\n",
        "            trainable=True, use_resource=True)\n",
        "\n",
        "  def __call__(self, position_sequence, n_particles_per_example,\n",
        "             global_context=None, particle_types=None):\n",
        "    \"\"\"Produces a model step, outputting the next position for each particle.\n",
        "    Args:\n",
        "      position_sequence: Sequence of positions for each node in the batch,\n",
        "        with shape [num_particles_in_batch, sequence_length, num_dimensions]\n",
        "      n_particles_per_example: Number of particles for each graph in the batch\n",
        "        with shape [batch_size]\n",
        "      global_context: Tensor of shape [batch_size, context_size], with global\n",
        "        context.\n",
        "      particle_types: Integer tensor of shape [num_particles_in_batch] with\n",
        "        the integer types of the particles, from 0 to `num_particle_types - 1`.\n",
        "        If None, we assume all particles are the same type.\n",
        "    Returns:\n",
        "      Next position with shape [num_particles_in_batch, num_dimensions] for one\n",
        "      step into the future from the input sequence.\n",
        "    \"\"\"\n",
        "    input_graphs_tuple = self._encoder_preprocessor(\n",
        "        position_sequence, n_particles_per_example, global_context,\n",
        "        particle_types)\n",
        "\n",
        "    normalized_acceleration = self._graph_network(input_graphs_tuple)\n",
        "\n",
        "    next_position = self._decoder_postprocessor(\n",
        "        normalized_acceleration, position_sequence)\n",
        "\n",
        "    return next_position\n",
        "\n",
        "  def _encoder_preprocessor(\n",
        "      self, position_sequence, velocity_sequence, n_node, global_context, particle_types):\n",
        "    # Extract important features from the position_sequence.\n",
        "    most_recent_position = position_sequence[-1]\n",
        "\n",
        "    # Get connectivity of the graph.\n",
        "    (senders, receivers, n_edge\n",
        "     ) = compute_connectivity_for_batch_pyfunc(\n",
        "         most_recent_position, n_node, self._connectivity_radius)\n",
        "\n",
        "    # Collect node features.\n",
        "    node_features = []\n",
        "\n",
        "    # Normalized velocity sequence, merging spatial an time axis.\n",
        "    velocity_stats = self._normalization_stats[\"velocity\"]\n",
        "    normalized_velocity_sequence = (\n",
        "        velocity_sequence - velocity_stats.mean) / velocity_stats.std\n",
        "    normalized_velocity_sequence = tf.reshape(normalized_velocity_sequence, shape=[N,-1])\n",
        "    node_features.append(normalized_velocity_sequence)\n",
        "\n",
        "    # Normalized clipped distances to lower and upper boundaries.\n",
        "    # boundaries are an array of shape [num_dimensions, 2], where the second\n",
        "    # axis, provides the lower/upper boundaries.\n",
        "    boundaries = tf.constant(self._boundaries, dtype=tf.float32)\n",
        "    distance_to_lower_boundary = (\n",
        "        most_recent_position - tf.expand_dims(boundaries[:, 0], 0))\n",
        "    distance_to_upper_boundary = (\n",
        "        tf.expand_dims(boundaries[:, 1], 0) - most_recent_position)\n",
        "    distance_to_boundaries = tf.concat(\n",
        "        [distance_to_lower_boundary, distance_to_upper_boundary], axis=1)\n",
        "    normalized_clipped_distance_to_boundaries = tf.clip_by_value(\n",
        "        distance_to_boundaries / self._connectivity_radius, -1., 1.)\n",
        "    node_features.append(normalized_clipped_distance_to_boundaries)\n",
        "\n",
        "    # Particle type.\n",
        "    if self._num_particle_types > 1:\n",
        "      particle_type_embeddings = tf.nn.embedding_lookup(\n",
        "          self._particle_type_embedding, particle_types)\n",
        "      node_features.append(particle_type_embeddings)\n",
        "\n",
        "    # Collect edge features.\n",
        "    edge_features = []\n",
        "\n",
        "    # Relative displacement and distances normalized to radius\n",
        "    normalized_relative_displacements = (\n",
        "        tf.gather(most_recent_position, senders) -\n",
        "        tf.gather(most_recent_position, receivers)) / self._connectivity_radius\n",
        "    edge_features.append(normalized_relative_displacements)\n",
        "\n",
        "    normalized_relative_distances = tf.norm(\n",
        "        normalized_relative_displacements, axis=-1, keepdims=True)\n",
        "    edge_features.append(normalized_relative_distances)\n",
        "\n",
        "    # Normalize the global context.\n",
        "    if global_context is not None:\n",
        "      context_stats = self._normalization_stats[\"context\"]\n",
        "      # Context in some datasets are all zero, so add an epsilon for numerical\n",
        "      # stability.\n",
        "      global_context = (global_context - context_stats.mean) / tf.math.maximum(\n",
        "          context_stats.std, STD_EPSILON)\n",
        "      \n",
        "    return gn.graphs.GraphsTuple(\n",
        "        nodes=tf.concat(node_features, axis=-1),\n",
        "        edges=tf.concat(edge_features, axis=-1),\n",
        "        globals=global_context,  # self._graph_net will appending this to nodes.\n",
        "        n_node=n_node,\n",
        "        n_edge=n_edge,\n",
        "        senders=senders,\n",
        "        receivers=receivers,\n",
        "        )\n",
        "\n",
        "  def _decoder_postprocessor(self, normalized_acceleration, position_sequence):\n",
        "\n",
        "    # The model produces the output in normalized space so we apply inverse\n",
        "    # normalization.\n",
        "    acceleration_stats = self._normalization_stats[\"acceleration\"]\n",
        "    acceleration = (\n",
        "        normalized_acceleration * acceleration_stats.std\n",
        "        ) + acceleration_stats.mean\n",
        "\n",
        "    # Use an Euler integrator to go from acceleration to position, assuming\n",
        "    # a dt=1 corresponding to the size of the finite difference.\n",
        "    most_recent_position = position_sequence[:, -1]\n",
        "    most_recent_velocity = most_recent_position - position_sequence[:, -2]\n",
        "\n",
        "    new_velocity = most_recent_velocity + acceleration  # * dt = 1\n",
        "    new_position = most_recent_position + new_velocity  # * dt = 1\n",
        "    return new_position\n",
        "\n",
        "  def get_predicted_and_target_normalized_accelerations(\n",
        "      self, next_position, position_sequence_noise, position_sequence, velocity_sequence,\n",
        "      n_particles_per_example, global_context=None, particle_types=None):  # pylint: disable=g-doc-args\n",
        "    \"\"\"Produces normalized and predicted acceleration targets.\n",
        "    Args:\n",
        "      next_position: Tensor of shape [num_particles_in_batch, num_dimensions]\n",
        "        with the positions the model should output given the inputs.\n",
        "      position_sequence_noise: Tensor of the same shape as `position_sequence`\n",
        "        with the noise to apply to each particle.\n",
        "      position_sequence, n_node, global_context, particle_types: Inputs to the\n",
        "        model as defined by `_build`.\n",
        "    Returns:\n",
        "      Tensors of shape [num_particles_in_batch, num_dimensions] with the\n",
        "        predicted and target normalized accelerations.\n",
        "    \"\"\"\n",
        "\n",
        "    # Add noise to the input position sequence.\n",
        "    noisy_position_sequence = position_sequence + position_sequence_noise\n",
        "\n",
        "    # Perform the forward pass with the noisy position sequence.\n",
        "    input_graphs_tuple = self._encoder_preprocessor(\n",
        "        noisy_position_sequence, velocity_sequence, n_particles_per_example, global_context,\n",
        "        particle_types)\n",
        "    predicted_normalized_acceleration = self._graph_network(input_graphs_tuple)\n",
        "\n",
        "    # Calculate the target acceleration, using an `adjusted_next_position `that\n",
        "    # is shifted by the noise in the last input position.\n",
        "\n",
        "    next_position_adjusted = next_position + position_sequence_noise[-1]\n",
        "    target_normalized_acceleration = self._inverse_decoder_postprocessor(\n",
        "        next_position_adjusted, noisy_position_sequence)\n",
        "    # As a result the inverted Euler update in the `_inverse_decoder` produces:\n",
        "    # * A target acceleration that does not explicitly correct for the noise in\n",
        "    #   the input positions, as the `next_position_adjusted` is different\n",
        "    #   from the true `next_position`.\n",
        "    # * A target acceleration that exactly corrects noise in the input velocity\n",
        "    #   since the target next velocity calculated by the inverse Euler update\n",
        "    #   as `next_position_adjusted - noisy_position_sequence[:,-1]`\n",
        "    #   matches the ground truth next velocity (noise cancels out).\n",
        "\n",
        "    return predicted_normalized_acceleration, target_normalized_acceleration\n",
        "\n",
        "  def _inverse_decoder_postprocessor(self, next_position, position_sequence):\n",
        "    \"\"\"Inverse of `_decoder_postprocessor`.\"\"\"\n",
        "\n",
        "    previous_position = position_sequence[-1]\n",
        "    previous_velocity = previous_position - position_sequence[-2]\n",
        "    next_velocity = next_position - previous_position\n",
        "    acceleration = next_velocity - previous_velocity\n",
        "\n",
        "    acceleration_stats = self._normalization_stats[\"acceleration\"]\n",
        "    normalized_acceleration = (\n",
        "        acceleration - acceleration_stats.mean) / acceleration_stats.std\n",
        "    return normalized_acceleration\n",
        "\n",
        "\n",
        "def time_diff(input_sequence):\n",
        "  return input_sequence[:, 1:] - input_sequence[:, :-1]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ntN9CfJTROC",
        "cellView": "form"
      },
      "source": [
        "#@title ### noise_utils\n",
        "\n",
        "\n",
        "def get_random_walk_noise_for_position_sequence(\n",
        "    position_sequence, noise_std_last_step):\n",
        "  \"\"\"Returns random-walk noise in the velocity applied to the position.\"\"\"\n",
        "\n",
        "  velocity_sequence = time_diff(position_sequence)\n",
        "\n",
        "  # We want the noise scale in the velocity at the last step to be fixed.\n",
        "  # Because we are going to compose noise at each step using a random_walk:\n",
        "  # std_last_step**2 = num_velocities * std_each_step**2\n",
        "  # so to keep `std_last_step` fixed, we apply at each step:\n",
        "  # std_each_step `std_last_step / np.sqrt(num_input_velocities)`\n",
        "  # TODO(alvarosg): Make sure this is consistent with the value and\n",
        "  # description provided in the paper.\n",
        "  num_velocities = velocity_sequence.shape.as_list()[1]\n",
        "  velocity_sequence_noise = tf.random.normal(\n",
        "      tf.shape(velocity_sequence),\n",
        "      stddev=noise_std_last_step / num_velocities ** 0.5,\n",
        "      dtype=position_sequence.dtype)\n",
        "\n",
        "  # Apply the random walk.\n",
        "  velocity_sequence_noise = tf.cumsum(velocity_sequence_noise, axis=1)\n",
        "\n",
        "  # Integrate the noise in the velocity to the positions, assuming\n",
        "  # an Euler intergrator and a dt = 1, and adding no noise to the very first\n",
        "  # position (since that will only be used to calculate the first position\n",
        "  # change).\n",
        "  position_sequence_noise = tf.concat([\n",
        "      tf.zeros_like(velocity_sequence_noise[:, 0:1]),\n",
        "      tf.cumsum(velocity_sequence_noise, axis=1)], axis=1)\n",
        "  return position_sequence_noise"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKnr3tQGX7pv",
        "cellView": "form"
      },
      "source": [
        "#@title ### reading_utils\n",
        "\n",
        "# Create a description of the features.\n",
        "_FEATURE_DESCRIPTION = {\n",
        "    'pos': tf.io.FixedLenSequenceFeature([timesteps//save_interval, N, 2], tf.string),\n",
        "    'vel': tf.io.FixedLenSequenceFeature([timesteps//save_interval, N, 2], tf.string)\n",
        "}\n",
        "\n",
        "_FEATURE_DESCRIPTION_WITH_GLOBAL_CONTEXT = _FEATURE_DESCRIPTION.copy()\n",
        "_FEATURE_DESCRIPTION_WITH_GLOBAL_CONTEXT['step_context'] = tf.io.VarLenFeature(\n",
        "    tf.string)\n",
        "\n",
        "_FEATURE_DTYPES = {\n",
        "    'pos': {\n",
        "        'in': np.float32,\n",
        "        'out': tf.float32\n",
        "    },\n",
        "    'vel': {\n",
        "        'in': np.float32,\n",
        "        'out': tf.float32\n",
        "    }\n",
        "}\n",
        "\n",
        "_CONTEXT_FEATURES = {\n",
        "    'group_id': tf.io.FixedLenFeature([1],tf.int64)\n",
        "}\n",
        "\n",
        "\n",
        "def convert_to_tensor(x, encoded_dtype):\n",
        "  if len(x) == 1:\n",
        "    out = np.frombuffer(x[0].numpy(), dtype=encoded_dtype)\n",
        "  else:\n",
        "    out = []\n",
        "    for el in x:\n",
        "      out.append(np.frombuffer(el.numpy(), dtype=encoded_dtype))\n",
        "  out = tf.convert_to_tensor(np.array(out))\n",
        "  return out\n",
        "\n",
        "\n",
        "\n",
        "def split_trajectory(context, features, window_length=7):\n",
        "  \"\"\"Splits trajectory into sliding windows.\"\"\"\n",
        "  # Our strategy is to make sure all the leading dimensions are the same size,\n",
        "  # then we can use from_tensor_slices.\n",
        "  \n",
        "  trajectory_length = features['pos'].get_shape().as_list()[0]\n",
        "\n",
        "  # We then stack window_length position changes so the final\n",
        "  # trajectory length will be - window_length +1 (the 1 to make sure we get\n",
        "  # the last split).\n",
        "  input_trajectory_length = trajectory_length - window_length + 1\n",
        "\n",
        "  model_input_features = {}\n",
        "  # Prepare the context features per step.\n",
        "  model_input_features['group_id'] = tf.tile(\n",
        "      tf.expand_dims(context['group_id'], axis=0),\n",
        "      [input_trajectory_length, 1])\n",
        "\n",
        "  if 'step_context' in features:\n",
        "    global_stack = []\n",
        "    for idx in range(input_trajectory_length):\n",
        "      global_stack.append(features['step_context'][idx:idx + window_length])\n",
        "    model_input_features['step_context'] = tf.stack(global_stack)\n",
        "\n",
        "  pos_stack = []\n",
        "  for idx in range(input_trajectory_length):\n",
        "    pos_stack.append(features['pos'][idx:idx + window_length])\n",
        "  # Get the corresponding positions\n",
        "  model_input_features['pos'] = tf.stack(pos_stack)\n",
        "\n",
        "  vel_stack = []\n",
        "  for idx in range(input_trajectory_length):\n",
        "    vel_stack.append(features['vel'][idx:idx + window_length])\n",
        "  # Get the corresponding velocities\n",
        "  model_input_features['vel'] = tf.stack(vel_stack)\n",
        "\n",
        "  return tf.data.Dataset.from_tensor_slices(model_input_features)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_J2CFR8rljZ"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "AYeIwYzirjCM"
      },
      "source": [
        "#@title ### Colin Code\n",
        "\n",
        "WINDOW_SIZE=5\n",
        "\n",
        "#Create a tf.data.Dataset from the TFRecord.\n",
        "ds = tf.data.TFRecordDataset(['train_datasets/microstates-' + str(i) + '.tfrecords' for i in range(1)])\n",
        "\n",
        "feature_description = {'group_id': tf.io.FixedLenFeature([], tf.int64),\n",
        "                        'pos': tf.io.FixedLenFeature([], tf.string),\n",
        "                        'vel': tf.io.FixedLenFeature([], tf.string)}\n",
        "\n",
        "def _parse_record(x):\n",
        "  # Parse the input tf.train.Example proto using the dictionary above.\n",
        "  return tf.io.parse_single_example(x, feature_description)\n",
        "\n",
        "def _parse_tensor(x):\n",
        "    output = {'group_id': x['group_id'],\n",
        "                'pos': tf.io.parse_tensor(x['pos'],out_type=tf.float32),\n",
        "                'vel': tf.io.parse_tensor(x['vel'],out_type=tf.float32)}\n",
        "    return output\n",
        "\n",
        "\n",
        "# map the record to features\n",
        "ds = ds.map(_parse_record)\n",
        "\n",
        "# map the features to tensors\n",
        "ds = ds.map(_parse_tensor)\n",
        "\n",
        "\n",
        "def make_window_dataset(x):\n",
        "    # make a dataset from the time series tensor\n",
        "    windows = tf.data.Dataset.from_tensor_slices((x['pos'],x['vel']))\n",
        "    # convert to windows\n",
        "    windows = windows.window(WINDOW_SIZE+1, shift=1, stride=1)\n",
        "    # take a batch of window size and combine pos, vel to a single dataset\n",
        "    windows = windows.flat_map(lambda pos_ds,vel_ds: tf.data.Dataset.zip((pos_ds.batch(WINDOW_SIZE, drop_remainder=True),vel_ds.batch(WINDOW_SIZE, drop_remainder=True))))\n",
        "  \n",
        "    return windows\n",
        "\n",
        "# flatten the windowed dataset\n",
        "ds = ds.flat_map(make_window_dataset)\n",
        "\n",
        "ds.shuffle(2000)\n",
        "\n",
        "def split_targets(x,y, w=7):\n",
        "    inputs = (x[0:w-1], y[0:w-1])\n",
        "    targets = (x[-1], y[-1])\n",
        "    return (inputs, targets)\n",
        "\n",
        "split_with_window = functools.partial(\n",
        "   split_targets,\n",
        "   w=WINDOW_SIZE)\n",
        "ds = ds.map(split_with_window)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzgGNFysMx35",
        "cellView": "form"
      },
      "source": [
        "#@title ### Preprocessing function\n",
        "\n",
        "interaction_radius = 15.0\n",
        "L = 200\n",
        "def preprocess_data(databatch):\n",
        "    \n",
        "    # node features xpos, ypos, xvel, yvel\n",
        "    # edge features distance, rel angle to receiver\n",
        "    X = databatch['pos']\n",
        "    V = databatch['vel']\n",
        "    \n",
        "    Xx = tf.expand_dims(X[...,0],-1)\n",
        "    dx = -Xx + tf.linalg.matrix_transpose(Xx)\n",
        "    dx = tf.where(dx>0.5*L, dx-L, dx)\n",
        "    dx = tf.where(dx<-0.5*L, dx+L, dx)\n",
        "\n",
        "    Xy = tf.expand_dims(X[...,1],-1)\n",
        "    dy = -Xy + tf.linalg.matrix_transpose(Xy)\n",
        "    dy = tf.where(dy>0.5*L, dy-L, dy)\n",
        "    dy = tf.where(dy<-0.5*L, dy+L, dy)\n",
        "\n",
        "\n",
        "\n",
        "    A = tf.expand_dims(tf.math.atan2(V[...,1],V[...,0]),-1)\n",
        "    angle_to_neigh = tf.math.atan2(dy, dx)\n",
        "\n",
        "    rel_angle_to_neigh = angle_to_neigh - A\n",
        "\n",
        "    dist = tf.math.sqrt(tf.square(dx)+tf.square(dy))\n",
        "\n",
        "    adj_matrix = tf.where(dist<interaction_radius, tf.ones_like(dist,dtype=tf.int32), tf.zeros_like(dist,dtype=tf.int32))\n",
        "    adj_matrix = tf.linalg.set_diag(adj_matrix, tf.zeros(tf.shape(adj_matrix)[:2],dtype=tf.int32))\n",
        "    sender_recv_list = tf.where(adj_matrix)\n",
        "    n_edge = tf.reduce_sum(adj_matrix, axis=[1,2])\n",
        "    n_node = tf.ones_like(n_edge)*tf.shape(adj_matrix)[-1]\n",
        "\n",
        "    senders = sender_recv_list[:,1] + sender_recv_list[:,0]*tf.shape(adj_matrix,out_type=tf.int64)[-1]\n",
        "    receivers = sender_recv_list[:,2] + sender_recv_list[:,0]*tf.shape(adj_matrix,out_type=tf.int64)[-1]\n",
        "\n",
        "\n",
        "    edge_distance = tf.expand_dims(tf.gather_nd(dist,sender_recv_list),-1)\n",
        "    edge_x_distance =  tf.expand_dims(tf.gather_nd(tf.math.cos(rel_angle_to_neigh),sender_recv_list),-1)  # relative to sender heading\n",
        "    edge_y_distance =  tf.expand_dims( tf.gather_nd(tf.math.sin(rel_angle_to_neigh),sender_recv_list),-1)  # relative to sender heading\n",
        "\n",
        "\n",
        "    edges = tf.concat([edge_distance,edge_x_distance,edge_y_distance],axis=-1)\n",
        "\n",
        "    node_positions = tf.reshape(X,(-1,2))\n",
        "    node_velocities = tf.reshape(V,(-1,2))\n",
        "\n",
        "    nodes = tf.concat([node_positions,node_velocities],axis=-1)\n",
        "\n",
        "    gn = graphs.GraphsTuple(nodes=nodes,edges=edges,globals=None,receivers=receivers,senders=senders,n_node=n_node,n_edge=n_edge)\n",
        "    gn = utils_tf.set_zero_global_features(gn,1)\n",
        "    \n",
        "    return gn"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbMPiqZENpAe",
        "cellView": "form"
      },
      "source": [
        "#@title ### More Functions\n",
        "\n",
        "INPUT_SEQUENCE_LENGTH = 6  # So we can calculate the last 5 velocities.\n",
        "NUM_PARTICLE_TYPES = 1\n",
        "KINEMATIC_PARTICLE_ID = 3\n",
        "\n",
        "batch_size=5\n",
        "\n",
        "metadata = {\"sequence_length\": timesteps//save_interval,\n",
        "            \"dim\": 2,\n",
        "            \"acc_mean\": 1,\n",
        "            \"acc_std\": 1,\n",
        "            \"vel_mean\": 1,\n",
        "            \"vel_std\": 1,\n",
        "            \"default_connectivity_radius\": 1,\n",
        "            \"bounds\": [(0,L), (0,L)]\n",
        "            }\n",
        "\n",
        "def _read_metadata(data_path):\n",
        "    with open(os.path.join(data_path, 'metadata.json'), 'rt') as fp:\n",
        "        return json.loads(fp.read())\n",
        "\n",
        "def prepare_inputs(tensor_dict):\n",
        "  \"\"\"Prepares a single stack of inputs by calculating inputs and targets.\n",
        "\n",
        "  Computes n_particles_per_example, which is a tensor that contains information\n",
        "  about how to partition the axis - i.e. which nodes belong to which graph.\n",
        "\n",
        "  Adds a batch axis to `n_particles_per_example` and `step_context` so they can\n",
        "  later be batched using `batch_concat`. This batch will be the same as if the\n",
        "  elements had been batched via stacking.\n",
        "\n",
        "  Note that all other tensors have a variable size particle axis,\n",
        "  and in this case they will simply be concatenated along that\n",
        "  axis.\n",
        "\n",
        "\n",
        "\n",
        "  Args:\n",
        "    tensor_dict: A dict of tensors containing positions, and step context (\n",
        "    if available).\n",
        "\n",
        "  Returns:\n",
        "    A tuple of input features and target positions.\n",
        "\n",
        "  \"\"\"\n",
        "  # Position is encoded as [sequence_length, num_particles, dim] but the model\n",
        "  # expects [num_particles, sequence_length, dim].\n",
        "  pos = tensor_dict['pos']\n",
        "  pos = pos[tf.newaxis,...]\n",
        "  pos = tf.transpose(pos, perm=[0, 2, 1, 3])\n",
        "\n",
        "  # The target position is the final step of the stack of positions.\n",
        "  target_position = pos[:, :, -1]\n",
        "\n",
        "  # Remove the target from the input.\n",
        "  tensor_dict['pos'] = pos[:, :, :-1]\n",
        "\n",
        "  # Velocity is encoded as [sequence_length, num_particles, dim] but the model\n",
        "  # expects [num_particles, sequence_length, dim].\n",
        "  vel = tensor_dict['vel']\n",
        "  vel = vel[tf.newaxis, ...]\n",
        "  vel = tf.transpose(vel, perm=[0, 2, 1, 3])\n",
        "\n",
        "  # The target position is the final step of the stack of positions.\n",
        "  target_position = vel[:, :, -1]\n",
        "\n",
        "  # Remove the target from the input.\n",
        "  tensor_dict['vel'] = vel[:, :, :-1]\n",
        "\n",
        "  # Compute the number of particles per example.\n",
        "  num_particles = tf.shape(pos)[0]\n",
        "  # Add an extra dimension for stacking via concat.\n",
        "  tensor_dict['n_particles_per_example'] = num_particles[tf.newaxis]\n",
        "\n",
        "  if 'step_context' in tensor_dict:\n",
        "    # Take the input global context. We have a stack of global contexts,\n",
        "    # and we take the penultimate since the final is the target.\n",
        "    tensor_dict['step_context'] = tensor_dict['step_context'][-2]\n",
        "    # Add an extra dimension for stacking via concat.\n",
        "    tensor_dict['step_context'] = tensor_dict['step_context'][tf.newaxis]\n",
        "  return tensor_dict, target_position\n",
        "\n",
        "def batch_concat(dataset, batch_size):\n",
        "  \"\"\"We implement batching as concatenating on the leading axis.\"\"\"\n",
        "\n",
        "  # We create a dataset of datasets of length batch_size.\n",
        "  windowed_ds = dataset.window(batch_size)\n",
        "  print(windowed_ds)\n",
        "  for window in dataset:\n",
        "      print(\"yes\")\n",
        "      [print(item.numpy()) for item in window]\n",
        "  # The plan is then to reduce every nested dataset by concatenating. We can\n",
        "  # do this using tf.data.Dataset.reduce. This requires an initial state, and\n",
        "  # then incrementally reduces by running through the dataset\n",
        "\n",
        "  # Get initial state. In this case this will be empty tensors of the\n",
        "  # correct shape.\n",
        "  initial_state = tree.map_structure(\n",
        "      lambda spec: tf.zeros(  # pylint: disable=g-long-lambda\n",
        "          shape=[1] + spec.shape.as_list()[1:], dtype=spec.dtype),\n",
        "      dataset.element_spec)\n",
        "  #print(initial_state.shape)\n",
        "  # We run through the nest and concatenate each entry with the previous state.\n",
        "  def reduce_window(initial_state, ds):\n",
        "    return ds.reduce(initial_state, lambda x, y: tf.stack([x, y], axis=0))\n",
        "\n",
        "  return windowed_ds.map(\n",
        "      lambda *x: tree.map_structure(reduce_window, initial_state, x))\n",
        "\n",
        "\n",
        "def prepare_rollout_inputs(context, features):\n",
        "  \"\"\"Prepares an inputs trajectory for rollout.\"\"\"\n",
        "  out_dict = {**context}\n",
        "  # Position is encoded as [sequence_length, num_particles, dim] but the model\n",
        "  # expects [num_particles, sequence_length, dim].\n",
        "  pos = tf.transpose(features['pos'], [1, 0, 2])\n",
        "  # The target position is the final step of the stack of positions.\n",
        "  target_position = pos[:, -1]\n",
        "  # Remove the target from the input.\n",
        "  out_dict['pos'] = pos[:, :-1]\n",
        "\n",
        "  # Velocity is encoded as [sequence_length, num_particles, dim] but the model\n",
        "  # expects [num_particles, sequence_length, dim].\n",
        "  vel = tf.transpose(features['vel'], [1, 0, 2])\n",
        "  # The target position is the final step of the stack of positions.\n",
        "  target_position = vel[:, -1]\n",
        "  # Remove the target from the input.\n",
        "  out_dict['vel'] = vel[:, :-1]\n",
        "\n",
        "  # Compute the number of nodes\n",
        "  out_dict['n_particles_per_example'] = [tf.shape(pos)[0]]\n",
        "  if 'step_context' in features:\n",
        "    out_dict['step_context'] = features['step_context']\n",
        "  out_dict['is_trajectory'] = tf.constant([True], tf.bool)\n",
        "  return out_dict, target_position"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUe5GAlNCD2X"
      },
      "source": [
        "#@title ### Create Model\n",
        "#@markdown loss_fn may be broken - features[0] adds noise to all timesteps\n",
        "\n",
        "noise_std=6.7e-4\n",
        "latent_size=128\n",
        "hidden_size=128\n",
        "hidden_layers=2\n",
        "message_passing_steps=10\n",
        "\n",
        "\"\"\"Gets one step model for training simulation.\"\"\"\n",
        "#metadata = _read_metadata(data_path)\n",
        "model_kwargs = dict(\n",
        "      latent_size=latent_size,\n",
        "      mlp_hidden_size=hidden_size,\n",
        "      mlp_num_hidden_layers=hidden_layers,\n",
        "      num_message_passing_steps=message_passing_steps)\n",
        "def _combine_std(std_x, std_y):\n",
        "  return np.sqrt(std_x**2 + std_y**2)\n",
        "\n",
        "Stats = collections.namedtuple('Stats', ['mean', 'std'])\n",
        "vel_noise_std=noise_std\n",
        "acc_noise_std=noise_std\n",
        "\"\"\"Instantiates the simulator.\"\"\"\n",
        "# Cast statistics to numpy so they are arrays when entering the model.\n",
        "cast = lambda v: np.array(v, dtype=np.float32)\n",
        "acceleration_stats = Stats(cast(metadata['acc_mean']), _combine_std(cast(metadata['acc_std']), acc_noise_std))\n",
        "velocity_stats = Stats(cast(metadata['vel_mean']),_combine_std(cast(metadata['vel_std']), vel_noise_std))\n",
        "normalization_stats = {'acceleration': acceleration_stats, 'velocity': velocity_stats}\n",
        "\n",
        "\n",
        "\n",
        "if 'context_mean' in metadata:\n",
        "    context_stats = Stats(cast(metadata['context_mean']), cast(metadata['context_std']))\n",
        "    normalization_stats['context'] = context_stats\n",
        "\n",
        "\n",
        "simulator = LearnedSimulator(\n",
        "      num_dimensions=metadata['dim'],\n",
        "      connectivity_radius=metadata['default_connectivity_radius'],\n",
        "      graph_network_kwargs=model_kwargs,\n",
        "      boundaries=metadata['bounds'],\n",
        "      num_particle_types=NUM_PARTICLE_TYPES,\n",
        "      normalization_stats=normalization_stats,\n",
        "      particle_type_embedding_size=16)\n",
        "\n",
        "\n",
        "KINEMATIC_PARTICLE_ID = 3\n",
        "def get_kinematic_mask(particle_types):\n",
        "  \"\"\"Returns a boolean mask, set to true for kinematic (obstacle) particles.\"\"\"\n",
        "  return tf.equal(particle_types, KINEMATIC_PARTICLE_ID)\n",
        "\n",
        "#@tf.function\n",
        "def loss_fn(features, labels):\n",
        "    target_next_position = labels[0]\n",
        "    # Sample the noise to add to the inputs to the model during training.\n",
        "    sampled_noise = get_random_walk_noise_for_position_sequence(\n",
        "        features[0], noise_std_last_step=noise_std)\n",
        "    non_kinematic_mask = tf.logical_not(get_kinematic_mask(tf.zeros(N)))\n",
        "    noise_mask = tf.cast(non_kinematic_mask, sampled_noise.dtype)\n",
        "    sampled_noise *= noise_mask[tf.newaxis,:,tf.newaxis]\n",
        "\n",
        "    # Get the predictions and target accelerations.\n",
        "    pred_target = simulator.get_predicted_and_target_normalized_accelerations(\n",
        "        next_position=target_next_position,\n",
        "        position_sequence=features[0],\n",
        "        position_sequence_noise=sampled_noise,\n",
        "        velocity_sequence=features[1],\n",
        "        n_particles_per_example=tf.constant([N]),\n",
        "        particle_types=None,\n",
        "        global_context=None)\n",
        "\n",
        "    pred_acceleration, target_acceleration = pred_target\n",
        "\n",
        "    # Calculate the loss and mask out loss on kinematic particles/\n",
        "    loss = (pred_acceleration - target_acceleration)**2\n",
        "    num_non_kinematic = tf.reduce_sum(tf.cast(non_kinematic_mask, tf.float32))\n",
        "    loss = tf.where(tf.expand_dims(non_kinematic_mask,-1), loss, tf.zeros_like(loss))\n",
        "    loss = tf.reduce_sum(loss) / tf.reduce_sum(num_non_kinematic)\n",
        "    \n",
        "    return loss\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8VFYHpFS-6X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "166e7618-940d-4cdf-917b-63a1408f8ef0"
      },
      "source": [
        "#@title ### Train\n",
        "\n",
        "min_lr = 1e-6\n",
        "lr = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-4 - min_lr,\n",
        "                                decay_steps=int(5e6),\n",
        "                                decay_rate=0.1) #+ min_lr\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "# @tf.function - ideally we'd like to decorate this with tf.function for faster code - but the connectivity utils is written using numpy so will need to be converted to tensorflow code\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss_value = loss_fn(x,y)\n",
        "    grads = tape.gradient(loss_value, simulator.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, simulator.trainable_variables))\n",
        "    return loss_value\n",
        "\n",
        "epochs = 2\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in tqdm(enumerate(iter(ds)), total=len(list(ds))):\n",
        "        loss_value = train_step(x_batch_train, y_batch_train)\n",
        "\n",
        "        # Log every 200 batches.\n",
        "        if step % 200 == 0:\n",
        "            print(\n",
        "                \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                % (step, float(loss_value))\n",
        "            )\n",
        "            print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start of epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/4094 [00:00<29:19,  2.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 0: 790.3579\n",
            "Seen so far: 5 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 31/4094 [00:08<17:45,  3.81it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-5003414de17a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Iterate over the batches of the dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Log every 200 batches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-5003414de17a>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-d0ee433f15b4>\u001b[0m in \u001b[0;36mloss_fn\u001b[0;34m(features, labels)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mn_particles_per_example\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mparticle_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         global_context=None)\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mpred_acceleration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_acceleration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sonnet/src/utils.py\u001b[0m in \u001b[0;36m_decorate_unbound_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0m_decorate_unbound_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mbound_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=attribute-error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_decorate_unbound_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sonnet/src/base.py\u001b[0m in \u001b[0;36mwrap_with_name_scope\u001b[0;34m(method, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;31m# snt.Module enters the module name scope for all methods. To disable this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;31m# for a particular method annotate it with `@snt.no_name_scope`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-447cc71c69b1>\u001b[0m in \u001b[0;36mget_predicted_and_target_normalized_accelerations\u001b[0;34m(self, next_position, position_sequence_noise, position_sequence, velocity_sequence, n_particles_per_example, global_context, particle_types)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mnoisy_position_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvelocity_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_particles_per_example\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         particle_types)\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0mpredicted_normalized_acceleration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_graphs_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;31m# Calculate the target acceleration, using an `adjusted_next_position `that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sonnet/src/utils.py\u001b[0m in \u001b[0;36m_decorate_unbound_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0m_decorate_unbound_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mbound_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=attribute-error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_decorate_unbound_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sonnet/src/base.py\u001b[0m in \u001b[0;36mwrap_with_name_scope\u001b[0;34m(method, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;31m# snt.Module enters the module name scope for all methods. To disable this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;31m# for a particular method annotate it with `@snt.no_name_scope`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-8e5f4fff33bf>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_graph)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# Do `m` message passing steps in the latent graphs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mlatent_graph_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_graph_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# Decode from the last latent graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sonnet/src/utils.py\u001b[0m in \u001b[0;36m_decorate_unbound_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0m_decorate_unbound_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mbound_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=attribute-error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_decorate_unbound_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sonnet/src/base.py\u001b[0m in \u001b[0;36mwrap_with_name_scope\u001b[0;34m(method, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;31m# snt.Module enters the module name scope for all methods. To disable this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;31m# for a particular method annotate it with `@snt.no_name_scope`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-8e5f4fff33bf>\u001b[0m in \u001b[0;36m_process\u001b[0;34m(self, latent_graph_0)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mprocessor_network_k\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_processor_networks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m       latent_graph_k = self._process_step(\n\u001b[0;32m--> 118\u001b[0;31m           processor_network_k, latent_graph_prev_k)\n\u001b[0m\u001b[1;32m    119\u001b[0m       \u001b[0mlatent_graph_prev_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlatent_graph_k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sonnet/src/utils.py\u001b[0m in \u001b[0;36m_decorate_unbound_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0m_decorate_unbound_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mbound_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=attribute-error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_decorate_unbound_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sonnet/src/base.py\u001b[0m in \u001b[0;36mwrap_with_name_scope\u001b[0;34m(method, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;31m# snt.Module enters the module name scope for all methods. To disable this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;31m# for a particular method annotate it with `@snt.no_name_scope`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-8e5f4fff33bf>\u001b[0m in \u001b[0;36m_process_step\u001b[0;34m(self, processor_network_k, latent_graph_prev_k)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;31m# One step of message passing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mlatent_graph_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor_network_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_graph_prev_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;31m# Add residuals.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sonnet/src/utils.py\u001b[0m in \u001b[0;36m_decorate_unbound_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0m_decorate_unbound_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mbound_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=attribute-error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_decorate_unbound_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sonnet/src/base.py\u001b[0m in \u001b[0;36mwrap_with_name_scope\u001b[0;34m(method, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;31m# snt.Module enters the module name scope for all methods. To disable this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;31m# for a particular method annotate it with `@snt.no_name_scope`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sonnet/src/utils.py\u001b[0m in \u001b[0;36m_decorate_unbound_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0m_decorate_unbound_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mbound_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=attribute-error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_decorate_unbound_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sonnet/src/base.py\u001b[0m in \u001b[0;36mwrap_with_name_scope\u001b[0;34m(method, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;31m# snt.Module enters the module name scope for all methods. To disable this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;31m# for a particular method annotate it with `@snt.no_name_scope`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/graph_nets/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sonnet/src/utils.py\u001b[0m in \u001b[0;36m_decorate_unbound_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0m_decorate_unbound_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mbound_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=attribute-error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_decorate_unbound_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sonnet/src/base.py\u001b[0m in \u001b[0;36mwrap_with_name_scope\u001b[0;34m(method, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;31m# snt.Module enters the module name scope for all methods. To disable this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;31m# for a particular method annotate it with `@snt.no_name_scope`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/graph_nets/modules.py\u001b[0m in \u001b[0;36m_build\u001b[0;34m(self, graph)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msenders\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edge_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sonnet/src/utils.py\u001b[0m in \u001b[0;36m_decorate_unbound_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0m_decorate_unbound_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mbound_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=attribute-error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_decorate_unbound_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sonnet/src/base.py\u001b[0m in \u001b[0;36mwrap_with_name_scope\u001b[0;34m(method, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;31m# snt.Module enters the module name scope for all methods. To disable this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;31m# for a particular method annotate it with `@snt.no_name_scope`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sonnet/src/utils.py\u001b[0m in \u001b[0;36m_decorate_unbound_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0m_decorate_unbound_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mbound_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=attribute-error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_decorate_unbound_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sonnet/src/base.py\u001b[0m in \u001b[0;36mwrap_with_name_scope\u001b[0;34m(method, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;31m# snt.Module enters the module name scope for all methods. To disable this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;31m# for a particular method annotate it with `@snt.no_name_scope`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/graph_nets/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sonnet/src/utils.py\u001b[0m in \u001b[0;36m_decorate_unbound_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0m_decorate_unbound_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mbound_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=attribute-error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_decorate_unbound_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sonnet/src/base.py\u001b[0m in \u001b[0;36mwrap_with_name_scope\u001b[0;34m(method, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;31m# snt.Module enters the module name scope for all methods. To disable this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;31m# for a particular method annotate it with `@snt.no_name_scope`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/graph_nets/blocks.py\u001b[0m in \u001b[0;36m_build\u001b[0;34m(self, graph)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[0mcollected_edges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medges_to_collect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m     \u001b[0mupdated_edges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edge_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollected_edges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdated_edges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sonnet/src/utils.py\u001b[0m in \u001b[0;36m_decorate_unbound_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0m_decorate_unbound_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mbound_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=attribute-error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_decorate_unbound_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sonnet/src/base.py\u001b[0m in \u001b[0;36mwrap_with_name_scope\u001b[0;34m(method, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;31m# snt.Module enters the module name scope for all methods. To disable this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;31m# for a particular method annotate it with `@snt.no_name_scope`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sonnet/src/sequential.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# Pass additional arguments to the first layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sonnet/src/utils.py\u001b[0m in \u001b[0;36m_decorate_unbound_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0m_decorate_unbound_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mbound_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=attribute-error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_decorate_unbound_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sonnet/src/base.py\u001b[0m in \u001b[0;36mwrap_with_name_scope\u001b[0;34m(method, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;31m# snt.Module enters the module name scope for all methods. To disable this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;31m# for a particular method annotate it with `@snt.no_name_scope`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sonnet/src/nets/mlp.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, is_training)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activate_final\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# Only perform dropout if we are activating the output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sonnet/src/utils.py\u001b[0m in \u001b[0;36m_decorate_unbound_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0m_decorate_unbound_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mbound_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=attribute-error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_decorate_unbound_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sonnet/src/base.py\u001b[0m in \u001b[0;36mwrap_with_name_scope\u001b[0;34m(method, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;31m# snt.Module enters the module name scope for all methods. To disable this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;31m# for a particular method annotate it with `@snt.no_name_scope`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sonnet/src/linear.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, output_type, name)\u001b[0m\n\u001b[1;32m   3653\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3654\u001b[0m         return gen_math_ops.mat_mul(\n\u001b[0;32m-> 3655\u001b[0;31m             a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   3656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5691\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   5692\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5693\u001b[0;31m         transpose_b)\n\u001b[0m\u001b[1;32m   5694\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5695\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZwtnG5pcnRk",
        "outputId": "96503402-081d-4730-99dc-27532dbe90eb"
      },
      "source": [
        "len(list(ds))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4094"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bR7a7iXuQCJ"
      },
      "source": [
        "import tensorflow as tf \n",
        "raw_dataset = tf.data.TFRecordDataset(\"/content/train_datasets/microstates-0.tfrecords\")\n",
        "\n",
        "for raw_record in raw_dataset.take(1):\n",
        "    example = tf.train.Example()\n",
        "    example.ParseFromString(raw_record.numpy())\n",
        "    print(example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxS-AEFhSVLo"
      },
      "source": [
        "def print_graphs_tuple(graphs_tuple):\n",
        "  print(\"Shapes of `GraphsTuple`'s fields:\")\n",
        "  print(graphs_tuple.map(lambda x: x if x is None else x.shape, fields=gn.graphs.ALL_FIELDS))\n",
        "  print(\"\\nData contained in `GraphsTuple`'s fields:\")\n",
        "  print(\"globals:\\n{}\".format(graphs_tuple.globals))\n",
        "  print(\"nodes:\\n{}\".format(graphs_tuple.nodes))\n",
        "  print(\"edges:\\n{}\".format(graphs_tuple.edges))\n",
        "  print(\"senders:\\n{}\".format(graphs_tuple.senders))\n",
        "  print(\"receivers:\\n{}\".format(graphs_tuple.receivers))\n",
        "  print(\"n_node:\\n{}\".format(graphs_tuple.n_node))\n",
        "  print(\"n_edge:\\n{}\".format(graphs_tuple.n_edge))\n",
        "\n",
        "#print_graphs_tuple(graphs_tuple)"
      ],
      "execution_count": 207,
      "outputs": []
    }
  ]
}