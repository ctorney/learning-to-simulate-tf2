{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProcessFishData.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPZOGt2EBczX/zbvOn2Pz35",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ctorney/learning-to-simulate-tf2/blob/main/test-files/ProcessFishData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Gpg2iae3jpzk"
      },
      "outputs": [],
      "source": [
        "#@title ### Imports { form-width: \"30%\" }\n",
        "\n",
        "import os, sys\n",
        "import numpy as np\n",
        "from math import *\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation, rc\n",
        "import datetime\n",
        "import json\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import scipy as sp\n",
        "from scipy import stats\n",
        "\n",
        "import pickle\n",
        "\n",
        "import functools\n",
        "from tqdm import tqdm\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Input, Concatenate\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "from spektral.layers import ECCConv, GlobalAvgPool, MessagePassing, XENetConv, GlobalAttentionPool, GlobalMaxPool, GlobalSumPool,GlobalAttnSumPool\n",
        "tfpl = tfp.layers\n",
        "tfd = tfp.distributions\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "plt.style.use('seaborn-paper') \n",
        "plt.style.use('seaborn-whitegrid')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with open('test-files/fish_and_frames_alpha (1).mat') as mat_file:\n",
        "mat = sp.io.loadmat('test-files/fish_and_frames_alpha (1).mat')\n",
        "full_data = mat['frames']"
      ],
      "metadata": {
        "id": "HZ4CkjBDjvgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vxs = []\n",
        "for d in full_data['speed'][2:]:\n",
        "    for i in d:\n",
        "        vxs.extend(np.squeeze(i))\n",
        "print(len(vxs))\n",
        "plt.hist(vxs,bins=20)"
      ],
      "metadata": {
        "id": "VUtqINKojxsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### Metadata { form-width: \"30%\" }\n",
        "n_frames = len(full_data['px'])\n",
        "\n",
        "max_id = 0\n",
        "min_id = 100000000\n",
        "for d in tqdm(full_data['onfish'][2:],total=n_frames):\n",
        "    for i in np.squeeze(d[0]):\n",
        "        if i> max_id: max_id = i\n",
        "        if i< min_id: min_id = i\n",
        "\n",
        "total_id = max_id-min_id+1\n",
        "\n",
        "print(f'max_id = {max_id}, min_id = {min_id}, total_id = {total_id}')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qzxJD2g-jzce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### Datatypes { form-width: \"30%\" }\n",
        "\n",
        "for key in ['px','py','vx','vy','speed']:\n",
        "    for timestep, layer in tqdm(enumerate(full_data[key][2:]), total=n_frames-2):\n",
        "        for i, val in enumerate(np.squeeze(layer[0])):\n",
        "            if full_data[key][2+timestep][0][i,0]==None: full_data[key][2+timestep][0][i,0]=np.nan\n",
        "            full_data[key][2+timestep][0][i,0] = float(full_data[key][2+timestep][0][i,0])\n",
        "            if key=='speed':\n",
        "                if abs(full_data[key][2+timestep][0][i,0])>5: full_data[key][2+timestep][0][i,0]=np.nan"
      ],
      "metadata": {
        "cellView": "form",
        "id": "surHJ-LDj3xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WINDOW_SIZE=3\n",
        "REPEATS=10\n",
        "BATCH_SIZE=128\n",
        "DOMAIN_SIZE=(1920,1080)\n",
        "domain_max = max(DOMAIN_SIZE)\n",
        "#append_speeds=True\n",
        "reject_length = 40\n",
        "\n",
        "def wind(data,i,w,append_speed=True):\n",
        "    indices=[]\n",
        "    for j in range(w):\n",
        "        index_j=np.squeeze(data['onfish'][2+i+j][0])\n",
        "        indices.append(index_j)\n",
        "        \n",
        "    true_indices=functools.reduce(np.intersect1d,indices)\n",
        "    n=len(true_indices)\n",
        "    arr=np.zeros((w,n,4))\n",
        "    for j in range(w):\n",
        "        idx=np.searchsorted(np.squeeze(data['onfish'][2+i+j][0]),true_indices)\n",
        "        arr[j,:,0]=np.squeeze(data['px'][2+i+j][0])[idx]\n",
        "        arr[j,:,1]=np.squeeze(data['py'][2+i+j][0])[idx]\n",
        "        arr[j,:,2]=np.squeeze(data['vx'][2+i+j][0])[idx]\n",
        "        arr[j,:,3]=np.squeeze(data['vy'][2+i+j][0])[idx]\n",
        "        arr[j,:,2]*=np.squeeze(data['speed'][2+i+j][0])[idx]\n",
        "        arr[j,:,3]*=np.squeeze(data['speed'][2+i+j][0])[idx]\n",
        "    \n",
        "    arr=arr[:,~np.isnan(arr).any(axis=(0,2)),:]\n",
        "                \n",
        "    return arr\n",
        "\n",
        "windows_as_list=[]\n",
        "for i in tqdm(range(n_frames-WINDOW_SIZE-1),total=n_frames-WINDOW_SIZE-1):\n",
        "    x=wind(full_data,i,WINDOW_SIZE)\n",
        "    #x=tf.convert_to_tensor(x)\n",
        "    if x.shape[1]>=reject_length: windows_as_list.append(x)\n",
        "    \n",
        "# windows=[]\n",
        "# for i in tqdm(windows_as_list):\n",
        "#     windows.append(tf.ragged.constant(i))"
      ],
      "metadata": {
        "id": "KV-aSI4ej7LP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c=0\n",
        "lens = np.zeros(len(windows_as_list))\n",
        "for i, window in tqdm(enumerate(windows_as_list),total=n_frames-WINDOW_SIZE-1):\n",
        "    lens[i]=window.shape[1]\n",
        "        #print(i, window.shape)\n",
        "plt.hist(lens,bins=20)"
      ],
      "metadata": {
        "id": "OoRqMlzhj94j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "windows_as_list = [window for window in windows_as_list if not window.shape[1]<reject_length]"
      ],
      "metadata": {
        "id": "ZkKwaX2dkBw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "4R22PC9HkEXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### Naive Parse Graph Function { form-width: \"30%\"}\n",
        "#@markdown Edge distance not normalised to L\n",
        "\n",
        "#@tf.function\n",
        "def naive_pre_process_function(X, V, _Lx=DOMAIN_SIZE[0], _Ly=DOMAIN_SIZE[1], interaction_radius=100, train_mode=True, add_noise=True):\n",
        "        \n",
        "        max_dist=max(_Lx,_Ly)\n",
        "        #X, V = inputs\n",
        "        # input shape [batch, steps, num. agents, dims]\n",
        "        # node features xpos, ypos, xvel, yvel\n",
        "        # edge features distance, rel angle to receiver\n",
        "        #print(X.shape, V.shape)\n",
        "        X_current = X[:,-1:,:,:]\n",
        "        V_current = V[:,-1:,:,:]\n",
        "\n",
        "        X_current = X_current.merge_dims(1,2)\n",
        "        V_current = V_current.merge_dims(1,2)\n",
        "\n",
        "        #X_current = tf.strided_slice(X, [0,-1,0,0], [])\n",
        "        #print(X_current.shape)\n",
        "        \n",
        "        def transpose_tensors_pos(i):\n",
        "            #print(i)\n",
        "            #i = i.to_tensor()\n",
        "            ii = tf.expand_dims(i[...,0], -1)\n",
        "            dx = tf.linalg.matrix_transpose(ii)-ii\n",
        "            dx = tf.where(dx>0.5*_Lx, dx-_Lx, dx)\n",
        "            dx = tf.where(dx<-0.5*_Lx, dx+_Lx, dx)\n",
        "            dx = tf.RaggedTensor.from_tensor(dx)\n",
        "\n",
        "            jj = tf.expand_dims(i[...,1], -1)\n",
        "            dy = tf.linalg.matrix_transpose(jj)-jj\n",
        "            dy = tf.where(dy>0.5*_Ly, dy-_Ly, dy)\n",
        "            dy = tf.where(dy<-0.5*_Ly, dy+_Ly, dy)\n",
        "            dy = tf.RaggedTensor.from_tensor(dy)\n",
        "\n",
        "            dd = tf.math.sqrt(tf.square(dx)+tf.square(dy))\n",
        "\n",
        "            return dx, dy, dd\n",
        "\n",
        "        def transpose_tensors_vel(i):\n",
        "            #i = i.to_tensor()\n",
        "            ii = tf.expand_dims(i[...,0], -1)\n",
        "            dx = tf.linalg.matrix_transpose(ii)-ii\n",
        "            dx = tf.RaggedTensor.from_tensor(dx)\n",
        "\n",
        "            jj = tf.expand_dims(i[...,1], -1)\n",
        "            dy = tf.linalg.matrix_transpose(jj)-jj\n",
        "            dy = tf.RaggedTensor.from_tensor(dy)\n",
        "\n",
        "            dnorm = tf.math.sqrt(tf.square(dx)+tf.square(dy))\n",
        "\n",
        "            dx = tf.math.divide_no_nan(dx,dnorm)\n",
        "            dy = tf.math.divide_no_nan(dy,dnorm)\n",
        "\n",
        "            return dx, dy\n",
        "\n",
        "        dx, dy, dist = tf.map_fn(fn=transpose_tensors_pos,\n",
        "                          elems=X_current,\n",
        "                          fn_output_signature=(tf.RaggedTensorSpec(shape=[None, None],\n",
        "                                                                   ragged_rank=1, \n",
        "                                                                   dtype=tf.float64),\n",
        "                                               tf.RaggedTensorSpec(shape=[None, None], \n",
        "                                                                   ragged_rank=1, \n",
        "                                                                   dtype=tf.float64),\n",
        "                                               tf.RaggedTensorSpec(shape=[None, None], \n",
        "                                                                   ragged_rank=1, \n",
        "                                                                   dtype=tf.float64))\n",
        "                          )\n",
        "\n",
        "        dvx, dvy = tf.map_fn(fn=transpose_tensors_vel,\n",
        "                          elems=V_current,\n",
        "                          fn_output_signature=(tf.RaggedTensorSpec(shape=[None, None], \n",
        "                                                                   ragged_rank=1, \n",
        "                                                                   dtype=tf.float64),\n",
        "                                               tf.RaggedTensorSpec(shape=[None, None], \n",
        "                                                                   ragged_rank=1, \n",
        "                                                                   dtype=tf.float64))\n",
        "                          )\n",
        "        \n",
        "        bx = _Lx - X_current[...,0:1]\n",
        "        by = _Ly - X_current[...,1:2]\n",
        "        boundary_dists = tf.concat([X_current, bx, by], axis=-1)\n",
        "        boundary_dists = boundary_dists/max_dist\n",
        "\n",
        "        def angles(x):\n",
        "            #x = x.to_tensor()\n",
        "            return tf.expand_dims(tf.math.atan2(x[...,1],x[...,0]),-1)\n",
        "\n",
        "        angles = tf.map_fn(fn=angles,\n",
        "                           elems=V_current,\n",
        "                           fn_output_signature=tf.RaggedTensorSpec(shape=[None, None], \n",
        "                                                                   ragged_rank=0, \n",
        "                                                                   dtype=tf.float64)\n",
        "                           )\n",
        "        angle_to_neigh = tf.math.atan2(dy, dx)\n",
        "\n",
        "        rel_angle_to_neigh = angle_to_neigh - angles\n",
        "\n",
        "        #dist = tf.math.sqrt(tf.square(dx)+tf.square(dy))\n",
        "\n",
        "        #interaction_radius = 25.0# tf.reduce_mean(dist,axis=[1,2],keepdims=True)\n",
        "        \n",
        "        def set_diag_func(x):\n",
        "            x = x.to_tensor()\n",
        "            x = tf.linalg.set_diag(x, tf.zeros(tf.shape(x)[0],dtype=tf.int32))\n",
        "            return tf.RaggedTensor.from_tensor(x)\n",
        "        \n",
        "        adj_matrix = tf.where(dist<interaction_radius, tf.ones_like(dist,dtype=tf.int32), tf.zeros_like(dist,dtype=tf.int32))\n",
        "        #adj_matrix = tf.linalg.set_diag(adj_matrix, tf.zeros(tf.shape(adj_matrix)[:2],dtype=tf.int32))\n",
        "        adj_matrix = tf.map_fn(fn=set_diag_func,\n",
        "                               elems=adj_matrix,\n",
        "                               fn_output_signature=(tf.RaggedTensorSpec(shape=[None,None],\n",
        "                                                                        ragged_rank=1, \n",
        "                                                                        dtype=tf.int32)))\n",
        "        \n",
        "        sender_recv_list = tf.where(adj_matrix)\n",
        "        n_edge = tf.reduce_sum(adj_matrix, axis=[1,2])\n",
        "        #print('sr list', sender_recv_list)\n",
        "        # def count_nodes(x):\n",
        "        #     return tf.shape(x)[-1]\n",
        "\n",
        "        # #n_node = tf.ones_like(n_edge)*tf.shape(adj_matrix)[-1]\n",
        "        # n_node = tf.map_fn(fn=count_nodes,\n",
        "        #                    elems=adj_matrix,\n",
        "        #                    fn_output_signature=tf.RaggedTensorSpec(shape=[None,None], ragged_rank=1, dtype=tf.int32))\n",
        "        n_node = adj_matrix.row_lengths(axis=1)\n",
        "        #print(tf.range(adj_matrix.get_shape()[0]))\n",
        "        #output_i = tf.repeat(tf.range(adj_matrix.get_shape()[0]),n_node)\n",
        "\n",
        "        # Finds batch indices (s1) for each edge and multiplies by \n",
        "        # the number of nodes in each graph (n_node) to determine the \n",
        "        # shift along the sparse matrix axes for sender and receiver indices\n",
        "\n",
        "        s1 = tf.squeeze(tf.slice(sender_recv_list,(0,0),size=(-1,1)))\n",
        "        s2 = tf.squeeze(tf.slice(sender_recv_list,(0,1),size=(-1,1)))\n",
        "        s3 = tf.squeeze(tf.slice(sender_recv_list,(0,2),size=(-1,1)))\n",
        "        # print('s1, s2, s3', s1.shape, s2.shape, s3.shape)\n",
        "        # print('nodes', n_node)#.shape)\n",
        "\n",
        "        def indice_func(n):\n",
        "            return n_node[n]\n",
        "        \n",
        "        indice_update = tf.map_fn(fn=indice_func,\n",
        "                                  elems=s1\n",
        "                                  #fn_output_signature=\n",
        "                                  )\n",
        "        senders = s2 + indice_update\n",
        "        receivers = s3 + indice_update\n",
        "\n",
        "        # def send_func(n):\n",
        "        #     return s2 + s1*n\n",
        "      \n",
        "        # def rec_func(n):\n",
        "        #     return s3 + s1*n\n",
        "        \n",
        "        # senders = tf.map_fn(fn=send_func,\n",
        "        #                     elems=n_node,\n",
        "        #                     fn_output_signature=tf.TensorSpec(shape=[None], dtype=tf.int64)\n",
        "        #                     )\n",
        "        # receivers = tf.map_fn(fn=send_func,\n",
        "        #                     elems=n_node,\n",
        "        #                     fn_output_signature=tf.TensorSpec(shape=[None], dtype=tf.int64)\n",
        "        #                     )\n",
        "        #print(senders.shape)\n",
        "        # senders = tf.squeeze(tf.slice(sender_recv_list,(0,1),size=(-1,1))) + tf.squeeze(tf.slice(sender_recv_list,(0,0),size=(-1,1)))*n_node\n",
        "        # receivers = tf.squeeze(tf.slice(sender_recv_list,(0,2),size=(-1,1))) + tf.squeeze(tf.slice(sender_recv_list,(0,0),size=(-1,1)))*tf.shape(adj_matrix,out_type=tf.int64)[-1]\n",
        "        total_nodes = tf.reduce_sum(n_node, axis=0)\n",
        "        output_a = tf.sparse.SparseTensor(indices=tf.stack([senders,receivers],axis=1), values = tf.ones_like(senders),dense_shape=[total_nodes, total_nodes])\n",
        "        edge_distance = tf.expand_dims(tf.gather_nd(dist/max_dist,sender_recv_list),-1)\n",
        "        #print(\"ed\", edge_distance.shape)\n",
        "        edge_x_distance = tf.expand_dims(tf.gather_nd(tf.math.cos(rel_angle_to_neigh),sender_recv_list),-1)  # neigbour position relative to sender heading\n",
        "        edge_y_distance = tf.expand_dims(tf.gather_nd(tf.math.sin(rel_angle_to_neigh),sender_recv_list),-1)  # neigbour position relative to sender heading\n",
        "        edge_x_orientation = tf.expand_dims(tf.gather_nd(dvx,sender_recv_list),-1)  # neigbour velocity relative to sender heading\n",
        "        edge_y_orientation = tf.expand_dims(tf.gather_nd(dvy,sender_recv_list),-1)  # neigbour velocity relative to sender heading\n",
        "\n",
        "\n",
        "        output_e = tf.concat([edge_distance,edge_x_distance,edge_y_distance,edge_x_orientation,edge_y_orientation],axis=-1)\n",
        "        #edges = tf.concat([edge_distance,edge_x_distance,edge_y_distance],axis=-1)\n",
        "        #node_velocities = tf.transpose(V, perm=[0,2,1,3])\n",
        "        def vel_transpose_func(x):\n",
        "            x = x.to_tensor()\n",
        "            x = tf.transpose(x, perm=[1,0,2])\n",
        "            return tf.reshape(x, (-1, 2*(WINDOW_SIZE-1)))\n",
        "        \n",
        "        #print(V.shape)\n",
        "        node_velocities = tf.map_fn(fn=vel_transpose_func,\n",
        "                                    elems=V,\n",
        "                                    fn_output_signature=tf.RaggedTensorSpec(shape=[None, None],\n",
        "                                                                            ragged_rank=0, \n",
        "                                                                            dtype=tf.float64),\n",
        "                                    infer_shape=False\n",
        "                                    )\n",
        "\n",
        "        #node_accelerations = tf.transpose(A, perm=[0,2,1,3])\n",
        "        output_x = tf.concat([node_velocities, boundary_dists], axis=-1)\n",
        "        output_x = output_x.merge_dims(0,1)\n",
        "        #output_x = tf.reshape(node_velocities,(-1,2*(WINDOW_SIZE-1)))\n",
        "\n",
        "        return output_x, output_a, output_e#, output_i\n",
        "\n",
        "naive_ppfunc = functools.partial(naive_pre_process_function,\n",
        "                           _Lx=DOMAIN_SIZE[0],\n",
        "                           _Ly=DOMAIN_SIZE[1]\n",
        "                           )"
      ],
      "metadata": {
        "cellView": "form",
        "id": "x4lriFfTkGsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in windows_as_list:\n",
        "    print(i.shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "Sf2NoyENkIrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_targets(x, w=WINDOW_SIZE):\n",
        "    inputs = (x[0:w-1,:,0:2], x[0:w-1,:,2:4])\n",
        "    targets = x[None,-1,:,2:4]\n",
        "    return (inputs, targets)\n",
        "\n",
        "split_with_window = functools.partial(\n",
        "  split_targets,\n",
        "  w=WINDOW_SIZE)  \n",
        "\n",
        "train_total = 500\n",
        "valid_total = 20\n",
        "\n",
        "#windows_list = random.sample(windows_as_list,train_total+valid_total)\n",
        "window_dataset = tf.data.Dataset.from_generator(lambda: windows_as_list,\n",
        "                                                    output_signature=tf.TensorSpec(shape=(WINDOW_SIZE,None,4), dtype=tf.float64))\n",
        "    \n",
        "for i in window_dataset:\n",
        "    print(i.shape)\n",
        "    break\n",
        "#window_dataset.map(lambda x: tf.data.Dataset.from_tensor_slices(x))\n",
        "\n",
        "# #window_dataset = tf.data.Dataset.from_tensor_slices(windows_list)\n",
        "window_dataset = window_dataset.map(split_with_window)\n",
        "window_dataset = window_dataset.repeat(REPEATS)\n",
        "window_dataset = window_dataset.shuffle(10000, reshuffle_each_iteration=True)\n",
        "window_dataset = window_dataset.apply(tf.data.experimental.dense_to_ragged_batch(BATCH_SIZE, drop_remainder=True))\n",
        "window_dataset = window_dataset.take(train_total+valid_total)\n",
        "window_dataset = window_dataset.map(lambda x, y: (naive_ppfunc(x[0], x[1]), y))\n",
        "train_dataset = window_dataset.skip(valid_total)\n",
        "valid_dataset = window_dataset.take(valid_total)\n",
        "# for i in window_dataset:\n",
        "#     print(i[1])\n",
        "#     break\n",
        "\n",
        "#print(len(list(window_dataset)))\n",
        "\n",
        "#valid_dataset = window_dataset.take(valid_total)\n",
        "#train_dataset = window_dataset.skip(valid_total)\n",
        "#train_total = train_dataset.reduce(0, lambda x, _: x + 1)"
      ],
      "metadata": {
        "id": "oQ9CPooQkLTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### Model { form-width: \"30%\" }\n",
        "\n",
        "min_lr = 1e-6\n",
        "lr = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-3 - min_lr,\n",
        "                                decay_steps=int(5e6),\n",
        "                                decay_rate=0.1) #+ min_lr\n",
        "MLP_SIZE=32\n",
        "n_out=2\n",
        "\n",
        "class GNNNet(Model):\n",
        "    def __init__(self,n_out=4,mp_steps=2):\n",
        "        super().__init__()\n",
        "        #self.preprocess = PreprocessingLayer(DOMAIN_SIZE)\n",
        "        self.encoder = XENetConv([MLP_SIZE,MLP_SIZE], MLP_SIZE, 2*MLP_SIZE, use_bias=False, node_activation=\"tanh\", edge_activation=\"tanh\") #ECCConv(32, activation=\"tanh\")\n",
        "        \n",
        "        self.process = XENetConv([MLP_SIZE,MLP_SIZE], MLP_SIZE, 2*MLP_SIZE, use_bias=False, node_activation=\"relu\", edge_activation=\"relu\")# MessagePassing(aggregate='mean')# 32, activation=\"relu\")\n",
        "        \n",
        "        #self.decoder = Dense(tfpl.IndependentNormal.params_size(n_out), activation=\"tanh\",use_bias=False) #ECCConv(n_out, activation=\"tanh\")\n",
        "        \n",
        "        self.mean_decoder = Dense(n_out, activation=\"linear\",use_bias=False) #ECCConv(n_out, activation=\"tanh\")\n",
        "        self.std_decoder = Dense(n_out, activation=\"linear\",kernel_initializer=tf.keras.initializers.Zeros(),bias_initializer=tf.keras.initializers.Constant(5.),activity_regularizer=tf.keras.regularizers.l2(1000)) #ECCConv(n_out, activation=\"tanh\")\n",
        "\n",
        "        self.distribution = tfp.layers.IndependentNormal(n_out)#lambda (mu, std): tfd.Normal(loc=mu, scale=std))\n",
        "        #self.global_pool = GlobalAvgPool()\n",
        "        #self.dense = Dense(32, activation=\"relu\")\n",
        "        #self.final = Dense(n_out, activation=\"sigmoid\")\n",
        "        self.mp_steps=mp_steps\n",
        "    \n",
        "    #@tf.function()\n",
        "    def call(self, inputs, train_mode=True):\n",
        "        #x, a, e, i = self.preprocess(inputs, train_mode=train_mode)\n",
        "        #x.set_shape([None,None])\n",
        "        #print(x.shape, a.shape, e.shape, i.shape)\n",
        "        x, a, e = inputs\n",
        "        x, e = self.encoder([x, a, e])\n",
        "        #x = self.node_encoder(x)\n",
        "        #e = self.edge_encoder(e)\n",
        "        for _ in range(self.mp_steps):\n",
        "            x, e = self.process([x, a, e])         \n",
        "        mu = self.mean_decoder(x)\n",
        "        std = self.std_decoder(x)\n",
        "        x = tf.keras.layers.concatenate([mu,std])\n",
        "#        x = self.distribution([mu,std])\n",
        "#        x = self.decoder(x)\n",
        "        x = self.distribution(x)\n",
        "        #print('model', x)\n",
        "        #x = self.global_pool([x, i])\n",
        "        #x = self.dense(x)\n",
        "        #x = self.final(x)\n",
        "\n",
        "        #x = tf.reshape(x, (-1, N, 2))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "model = GNNNet(n_out=n_out)\n",
        "optimizer = Adam(lr)\n",
        "#loss_fn = MeanSquaredError()\n",
        "\n",
        "def loss_fn(target,predicted_dist):\n",
        "    return -tf.reduce_sum(predicted_dist.log_prob(target.merge_dims(0,2)))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ffJSgYtdkMxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('fish_weights_1')"
      ],
      "metadata": {
        "id": "pTjEntmJkPSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### Train Step\n",
        "\n",
        "#@tf.function()#input_signature=[[tf.TensorSpec(shape=(None,None,2), dtype=tf.float32),tf.TensorSpec(shape=(None,None,2), dtype=tf.float32),tf.TensorSpec(shape=(None,None,2), dtype=tf.float32)],tf.TensorSpec(shape=(None,4), dtype=tf.float32)], experimental_relax_shapes=True)\n",
        "def train_step(inputs, target):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inputs, training=True)\n",
        "        #print('predtar', predictions, target)\n",
        "        target = tf.cast(target, dtype=tf.float32)\n",
        "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
        "        #print(sum(model.losses))\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss, sum(model.losses)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lTt6rQUJkRXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### Train Model { form-width: \"30%\" }\n",
        "\n",
        "step = losses = 0\n",
        "epochs = 30  # Number of training epochs\n",
        "batch_size = BATCH_SIZE  # Batch size\n",
        "print(f\"total = {train_total}, batch size = {batch_size}\")\n",
        "divisor=20\n",
        "loss_values = np.zeros((epochs,train_total))\n",
        "reg_values = np.zeros((epochs,train_total))\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "    pbar = tqdm(enumerate(train_dataset), total=train_total)\n",
        "    \n",
        "    for step, batch in pbar:\n",
        "        inputs, target = batch\n",
        "        losses, regs = train_step(inputs,target)\n",
        "        loss_values[epoch,step]=losses.numpy()\n",
        "        reg_values[epoch,step]=regs.numpy()\n",
        "        if step%divisor==0:\n",
        "            pbar.set_description(f\"loss {tf.reduce_mean(loss_values[epoch,max(0,step-divisor):step+1]).numpy()}, \\\n",
        "            regs {tf.reduce_mean(reg_values[epoch,max(0,step-divisor):step+1]).numpy()}\")\n",
        "    print(f'Epoch Averages: -Loss {tf.reduce_mean(loss_values[epoch,0:step+1]).numpy()} \\\n",
        "    -Regs {tf.reduce_mean(reg_values[epoch,0:step+1]).numpy()}')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nsIfHUWOkTL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights('fish_weights_2')"
      ],
      "metadata": {
        "id": "tGeZgFV2kU0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### Loss Plot\n",
        "\n",
        "compression = 100\n",
        "\n",
        "loss = np.ndarray.flatten(loss_values)\n",
        "reg = np.ndarray.flatten(reg_values)\n",
        "#loss = np.nanmean(np.pad(loss.astype(float), (0, compression - loss.size%compression), mode='constant', constant_values=np.NaN).reshape(-1, compression), axis=1)\n",
        "plt.plot(loss[100:])\n",
        "plt.plot(reg[100:])\n",
        "plt.yscale('log')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "C9-p-O8QkXDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### Validation\n",
        "\n",
        "pred_list = []\n",
        "true_values = []\n",
        "valid_loss=0\n",
        "c=0\n",
        "\n",
        "for databatch in tqdm(valid_dataset, total=valid_total):\n",
        "\n",
        "    target = databatch[1]\n",
        "    true_values.append(tf.expand_dims(target.merge_dims(0,-2),0).numpy())\n",
        "\n",
        "    predictions = model(databatch[0],train_mode=False)\n",
        "    pred_list.append((predictions.sample(1).numpy()))\n",
        "    target = tf.cast(target, dtype=tf.float32)\n",
        "    #loss_value = tf.keras.losses.MeanSquaredError()(target,predictions).numpy()\n",
        "    loss_value = loss_fn(target, predictions).numpy()\n",
        "    valid_loss+= loss_value\n",
        "    c+=1\n",
        "    if c>=50:\n",
        "        break\n",
        "\n",
        "print('validation loss', valid_loss/c)\n",
        "#print(pred_list)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TL2GP2PhkZ7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.pylabtools import figsize\n",
        "#@title ### Sample Predictions v Targets Plot { form-width: \"30%\" }\n",
        "# plt.plot(tf.expand_dims(target.merge_dims(0,-2),0).numpy()[0,:,1],\n",
        "#          predictions.sample(1).numpy()[0,:,1],\n",
        "#          '.')\n",
        "# plt.ylim((-1,1))\n",
        "#target.numpy().shape\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(10,4))\n",
        "ax1.plot(tf.expand_dims(target.merge_dims(0,-2),0).numpy()[0,:,0],\n",
        "         predictions.sample(1).numpy()[0,:,0],\n",
        "         '.')\n",
        "ax2.plot(tf.expand_dims(target.merge_dims(0,-2),0).numpy()[0,:,1],\n",
        "         predictions.sample(1).numpy()[0,:,1],\n",
        "         '.')\n",
        "ax1.set_xlim(-5,5)\n",
        "ax1.set_ylim(-5,5)\n",
        "ax2.set_xlim(-5,5)\n",
        "#ax2.set_ylim(-5,5)\n",
        "ax1.set_title('X-coords')\n",
        "ax2.set_title('Y-coords')\n",
        "#ax2.scatter(x, y)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KjfFQg98kcwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### Predictions v Targets Mean\n",
        "\n",
        "fig, axs = plt.subplots(1,2, figsize=(16, 8), facecolor='w', edgecolor='k')  \n",
        "print(pred_list[0].shape)\n",
        "axs = axs.ravel()\n",
        "for pred_i in range(2):\n",
        "    #pred_vals = np.array([pp[:,:,pred_i] for pp in pred_list],dtype=object).flatten()\n",
        "    #true_vals = np.array([tt[:,:,pred_i] for tt in true_values]).flatten()\n",
        "\n",
        "    pred_vals = np.concatenate([(np.tanh(pp[:,:,pred_i])/0.7).squeeze() for pp in pred_list])\n",
        "    true_vals = np.concatenate([(np.tanh(pp[:,:,pred_i])/0.7).squeeze() for pp in true_values])\n",
        "\n",
        "    bin_means, bin_edges, binnumber = stats.binned_statistic(true_vals, pred_vals,bins=100,range=(-1,1))\n",
        "    bin_width = (bin_edges[1] - bin_edges[0])\n",
        "    bin_centers = bin_edges[1:] - bin_width/2\n",
        "\n",
        "    bin_stds, bin_edges, binnumber = stats.binned_statistic(true_vals, pred_vals,statistic='std',bins=100,range=(-1,1))\n",
        "\n",
        "\n",
        "    axs[pred_i].plot(bin_centers,bin_means,c='C0')\n",
        "\n",
        "    axs[pred_i].fill_between(bin_centers,bin_means-bin_stds,bin_means+bin_stds,color='C0',alpha=0.5)\n",
        "\n",
        "    xx = np.linspace(bin_edges.min(),bin_edges.max(),10)\n",
        "    axs[pred_i].plot(xx,xx,c='k',ls='--')\n",
        "\n",
        "    axs[pred_i].set_ylabel('GNN prediction of parameter')\n",
        "    axs[pred_i].set_xlabel('True parameter that generated the microstate')\n",
        "    #axs[pred_i].set_xlim(-1,1)\n",
        "    #axs[pred_i].set_ylim(-1,1)\n",
        "\n",
        "\n",
        "#plt.savefig('gnn_' + str(run) + '.png',dpi=300)\n",
        "#plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cX-VfSXOkfJ1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}