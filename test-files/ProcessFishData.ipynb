{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d96b1a67",
   "metadata": {
    "collapsed": true,
    "gradient": {
     "editing": false,
     "id": "d96b1a67",
     "kernelId": "4db5fd37-e23d-456f-8ba8-726fa1089326"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/distributions/distribution.py:265: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/distributions/bernoulli.py:169: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    }
   ],
   "source": [
    "#@title ### Imports { form-width: \"30%\" }\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "from math import *\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation, rc\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "\n",
    "import pickle\n",
    "\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from spektral.layers import ECCConv, GlobalAvgPool, MessagePassing, XENetConv, GlobalAttentionPool, GlobalMaxPool, GlobalSumPool,GlobalAttnSumPool\n",
    "tfpl = tfp.layers\n",
    "tfd = tfp.distributions\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.style.use('seaborn-paper') \n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa80dca2",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "fa80dca2",
     "kernelId": "4db5fd37-e23d-456f-8ba8-726fa1089326"
    }
   },
   "outputs": [],
   "source": [
    "#with open('test-files/fish_and_frames_alpha (1).mat') as mat_file:\n",
    "mat = sp.io.loadmat('test-files/fish_and_frames_alpha (1).mat')\n",
    "full_data = mat['frames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a92c2401",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "a92c2401",
     "kernelId": "4db5fd37-e23d-456f-8ba8-726fa1089326"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  6  8 10 11 12 15 16 17 19 21 22 23 25 26 27 31 34 35 36 38 39 40 41\n",
      "  43 44 45 46 47 48]]\n"
     ]
    }
   ],
   "source": [
    "for d in full_data['onfish'][2]:\n",
    "    print(d)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0e882c4",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "a0e882c4",
     "kernelId": "4db5fd37-e23d-456f-8ba8-726fa1089326"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 100838/100840 [00:01<00:00, 56343.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_id = 37647, min_id = 3, total_id = 37645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#@title ### Metadata { form-width: \"30%\" }\n",
    "n_frames = len(full_data['px'])\n",
    "\n",
    "max_id = 0\n",
    "min_id = 100000000\n",
    "for d in tqdm(full_data['onfish'][2:],total=n_frames):\n",
    "    for i in np.squeeze(d[0]):\n",
    "        if i> max_id: max_id = i\n",
    "        if i< min_id: min_id = i\n",
    "\n",
    "total_id = max_id-min_id+1\n",
    "\n",
    "print(f'max_id = {max_id}, min_id = {min_id}, total_id = {total_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ede27a4c",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "ede27a4c",
     "kernelId": "4db5fd37-e23d-456f-8ba8-726fa1089326"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100838/100838 [00:14<00:00, 7018.79it/s]\n",
      "100%|██████████| 100838/100838 [00:14<00:00, 6959.31it/s]\n",
      "100%|██████████| 100838/100838 [00:14<00:00, 7043.83it/s]\n",
      "100%|██████████| 100838/100838 [00:14<00:00, 7022.32it/s]\n"
     ]
    }
   ],
   "source": [
    "#@title ### Datatypes { form-width: \"30%\" }\n",
    "\n",
    "for key in ['px','py','vx','vy']:\n",
    "    for timestep, layer in tqdm(enumerate(full_data[key][2:]), total=n_frames-2):\n",
    "        for i, val in enumerate(np.squeeze(layer[0])):\n",
    "            if full_data[key][2+timestep][0][i,0]==None: full_data[key][2+timestep][0][i,0]=np.nan\n",
    "            full_data[key][2+timestep][0][i,0] = float(full_data[key][2+timestep][0][i,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccb2da6e",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "ccb2da6e",
     "kernelId": "4db5fd37-e23d-456f-8ba8-726fa1089326"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100836/100836 [00:14<00:00, 6728.51it/s]\n",
      "  0%|          | 0/100836 [00:00<?, ?it/s]2022-02-21 19:00:21.672010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-21 19:00:21.685785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-21 19:00:21.686792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-21 19:00:21.689647: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-21 19:00:21.690617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-21 19:00:21.691540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "  0%|          | 1/100836 [00:01<39:20:17,  1.40s/it]tor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-21 19:00:22.686036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-21 19:00:22.686989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-21 19:00:22.687861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-21 19:00:22.688717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14793 MB memory:  -> device: 0, name: Quadro RTX 5000, pci bus id: 0000:00:05.0, compute capability: 7.5\n",
      " 10%|█         | 10549/100836 [00:23<03:17, 457.59it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_69/2971162719.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mwindows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindows_as_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mwindows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mragged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/ragged/ragged_factory_ops.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(pylist, dtype, ragged_rank, inner_shape, name, row_splits_dtype)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"RaggedConstant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     return _constant_value(ragged_factory, constant_op.constant, pylist, dtype,\n\u001b[0m\u001b[1;32m     87\u001b[0m                            ragged_rank, inner_shape)\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/ragged/ragged_factory_ops.py\u001b[0m in \u001b[0;36m_constant_value\u001b[0;34m(ragged_factory, inner_factory, pylist, dtype, ragged_rank, inner_shape)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcatenated_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m   values = inner_factory(\n\u001b[0m\u001b[1;32m    240\u001b[0m       values, dtype=dtype, shape=(len(values),) + inner_shape, name=\"values\")\n\u001b[1;32m    241\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mrow_splits\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m   \"\"\"\n\u001b[0;32m--> 271\u001b[0;31m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[1;32m    272\u001b[0m                         allow_broadcast=True)\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    281\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m   \u001b[0;34m\"\"\"Creates a constant on the current device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "WINDOW_SIZE=3\n",
    "REPEATS=10\n",
    "BATCH_SIZE=128\n",
    "DOMAIN_SIZE=(1920,1080)\n",
    "domain_max = max(DOMAIN_SIZE)\n",
    "\n",
    "def wind(data,i,w):\n",
    "    indices=[]\n",
    "    for j in range(w):\n",
    "        index_j=np.squeeze(data['onfish'][2+i+j][0])\n",
    "        indices.append(index_j)\n",
    "        \n",
    "    true_indices=functools.reduce(np.intersect1d,indices)\n",
    "    n=len(true_indices)\n",
    "    \n",
    "    arr=np.zeros((w,n,4))\n",
    "    for j in range(w):\n",
    "        idx=np.searchsorted(np.squeeze(data['onfish'][2+i+j][0]),true_indices)\n",
    "        arr[j,:,0]=np.squeeze(data['px'][2+i+j][0])[idx]/domain_max\n",
    "        arr[j,:,1]=np.squeeze(data['py'][2+i+j][0])[idx]/domain_max\n",
    "        arr[j,:,2]=np.squeeze(data['vx'][2+i+j][0])[idx]\n",
    "        arr[j,:,3]=np.squeeze(data['vy'][2+i+j][0])[idx]\n",
    "        arr[j,:,2]*=np.squeeze(data['speed'][2+i+j][0])[idx]\n",
    "        arr[j,:,3]*=np.squeeze(data['speed'][2+i+j][0])[idx]\n",
    "    \n",
    "    arr=arr[:,~np.isnan(arr).any(axis=(0,2)),:]\n",
    "                \n",
    "    return arr\n",
    "\n",
    "windows_as_list=[]\n",
    "for i in tqdm(range(n_frames-WINDOW_SIZE-1),total=n_frames-WINDOW_SIZE-1):\n",
    "    x=wind(full_data,i,WINDOW_SIZE)\n",
    "    #x=tf.convert_to_tensor(x)\n",
    "    windows_as_list.append(x)\n",
    "    \n",
    "# windows=[]\n",
    "# for i in tqdm(windows_as_list):\n",
    "#     windows.append(tf.ragged.constant(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfcab220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb 21 19:00:53 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 5000     Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 33%   32C    P2    37W / 230W |  15076MiB / 16125MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8fc519c",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 13,
     "id": "a8fc519c",
     "kernelId": "4db5fd37-e23d-456f-8ba8-726fa1089326"
    }
   },
   "outputs": [],
   "source": [
    "#@title ### Naive Parse Graph Function { form-width: \"30%\"}\n",
    "#@markdown Edge distance not normalised to L\n",
    "\n",
    "#@tf.function\n",
    "def naive_pre_process_function(X, V, _Lx=DOMAIN_SIZE[0], _Ly=DOMAIN_SIZE[1], interaction_radius=100, train_mode=True, add_noise=True):\n",
    "        \n",
    "        #X, V = inputs\n",
    "        # input shape [batch, steps, num. agents, dims]\n",
    "        # node features xpos, ypos, xvel, yvel\n",
    "        # edge features distance, rel angle to receiver\n",
    "        #print(X.shape, V.shape)\n",
    "        X_current = X[:,-1:,:,:]\n",
    "        V_current = V[:,-1:,:,:]\n",
    "\n",
    "        X_current = X_current.merge_dims(1,2)\n",
    "        V_current = V_current.merge_dims(1,2)\n",
    "\n",
    "        #X_current = tf.strided_slice(X, [0,-1,0,0], [])\n",
    "        #print(X_current.shape)\n",
    "        \n",
    "        def transpose_tensors_pos(i):\n",
    "            #print(i)\n",
    "            #i = i.to_tensor()\n",
    "            ii = tf.expand_dims(i[...,0], -1)\n",
    "            dx = tf.linalg.matrix_transpose(ii)-ii\n",
    "            dx = tf.where(dx>0.5*_Lx, dx-_Lx, dx)\n",
    "            dx = tf.where(dx<-0.5*_Lx, dx+_Lx, dx)\n",
    "            dx = tf.RaggedTensor.from_tensor(dx)\n",
    "\n",
    "            jj = tf.expand_dims(i[...,1], -1)\n",
    "            dy = tf.linalg.matrix_transpose(jj)-jj\n",
    "            dy = tf.where(dy>0.5*_Ly, dy-_Ly, dy)\n",
    "            dy = tf.where(dy<-0.5*_Ly, dy+_Ly, dy)\n",
    "            dy = tf.RaggedTensor.from_tensor(dy)\n",
    "\n",
    "            dd = tf.math.sqrt(tf.square(dx)+tf.square(dy))\n",
    "\n",
    "            return dx, dy, dd\n",
    "\n",
    "        def transpose_tensors_vel(i):\n",
    "            #i = i.to_tensor()\n",
    "            ii = tf.expand_dims(i[...,0], -1)\n",
    "            dx = tf.linalg.matrix_transpose(ii)-ii\n",
    "            dx = tf.RaggedTensor.from_tensor(dx)\n",
    "\n",
    "            jj = tf.expand_dims(i[...,1], -1)\n",
    "            dy = tf.linalg.matrix_transpose(jj)-jj\n",
    "            dy = tf.RaggedTensor.from_tensor(dy)\n",
    "\n",
    "            dnorm = tf.math.sqrt(tf.square(dx)+tf.square(dy))\n",
    "\n",
    "            dx = tf.math.divide_no_nan(dx,dnorm)\n",
    "            dy = tf.math.divide_no_nan(dy,dnorm)\n",
    "\n",
    "            return dx, dy\n",
    "\n",
    "        dx, dy, dist = tf.map_fn(fn=transpose_tensors_pos,\n",
    "                          elems=X_current,\n",
    "                          fn_output_signature=(tf.RaggedTensorSpec(shape=[None, None],\n",
    "                                                                   ragged_rank=1, \n",
    "                                                                   dtype=tf.float64),\n",
    "                                               tf.RaggedTensorSpec(shape=[None, None], \n",
    "                                                                   ragged_rank=1, \n",
    "                                                                   dtype=tf.float64),\n",
    "                                               tf.RaggedTensorSpec(shape=[None, None], \n",
    "                                                                   ragged_rank=1, \n",
    "                                                                   dtype=tf.float64))\n",
    "                          )\n",
    "\n",
    "        dvx, dvy = tf.map_fn(fn=transpose_tensors_vel,\n",
    "                          elems=V_current,\n",
    "                          fn_output_signature=(tf.RaggedTensorSpec(shape=[None, None], \n",
    "                                                                   ragged_rank=1, \n",
    "                                                                   dtype=tf.float64),\n",
    "                                               tf.RaggedTensorSpec(shape=[None, None], \n",
    "                                                                   ragged_rank=1, \n",
    "                                                                   dtype=tf.float64))\n",
    "                          )\n",
    "        \n",
    "        bx = _Lx - X_current[...,0:1]\n",
    "        by = _Ly - X_current[...,1:2]\n",
    "        boundary_dists = tf.concat([X_current, bx, by], axis=-1)\n",
    "\n",
    "        def angles(x):\n",
    "            #x = x.to_tensor()\n",
    "            return tf.expand_dims(tf.math.atan2(x[...,1],x[...,0]),-1)\n",
    "\n",
    "        angles = tf.map_fn(fn=angles,\n",
    "                           elems=V_current,\n",
    "                           fn_output_signature=tf.RaggedTensorSpec(shape=[None, None], \n",
    "                                                                   ragged_rank=0, \n",
    "                                                                   dtype=tf.float64)\n",
    "                           )\n",
    "        angle_to_neigh = tf.math.atan2(dy, dx)\n",
    "\n",
    "        rel_angle_to_neigh = angle_to_neigh - angles\n",
    "\n",
    "        #dist = tf.math.sqrt(tf.square(dx)+tf.square(dy))\n",
    "\n",
    "        #interaction_radius = 25.0# tf.reduce_mean(dist,axis=[1,2],keepdims=True)\n",
    "        \n",
    "        def set_diag_func(x):\n",
    "            x = x.to_tensor()\n",
    "            x = tf.linalg.set_diag(x, tf.zeros(tf.shape(x)[0],dtype=tf.int32))\n",
    "            return tf.RaggedTensor.from_tensor(x)\n",
    "        \n",
    "        adj_matrix = tf.where(dist<interaction_radius, tf.ones_like(dist,dtype=tf.int32), tf.zeros_like(dist,dtype=tf.int32))\n",
    "        #adj_matrix = tf.linalg.set_diag(adj_matrix, tf.zeros(tf.shape(adj_matrix)[:2],dtype=tf.int32))\n",
    "        adj_matrix = tf.map_fn(fn=set_diag_func,\n",
    "                               elems=adj_matrix,\n",
    "                               fn_output_signature=(tf.RaggedTensorSpec(shape=[None,None],\n",
    "                                                                        ragged_rank=1, \n",
    "                                                                        dtype=tf.int32)))\n",
    "        \n",
    "        sender_recv_list = tf.where(adj_matrix)\n",
    "        n_edge = tf.reduce_sum(adj_matrix, axis=[1,2])\n",
    "        #print('sr list', sender_recv_list)\n",
    "        # def count_nodes(x):\n",
    "        #     return tf.shape(x)[-1]\n",
    "\n",
    "        # #n_node = tf.ones_like(n_edge)*tf.shape(adj_matrix)[-1]\n",
    "        # n_node = tf.map_fn(fn=count_nodes,\n",
    "        #                    elems=adj_matrix,\n",
    "        #                    fn_output_signature=tf.RaggedTensorSpec(shape=[None,None], ragged_rank=1, dtype=tf.int32))\n",
    "        n_node = adj_matrix.row_lengths(axis=1)\n",
    "        #print(tf.range(adj_matrix.get_shape()[0]))\n",
    "        #output_i = tf.repeat(tf.range(adj_matrix.get_shape()[0]),n_node)\n",
    "\n",
    "        # Finds batch indices (s1) for each edge and multiplies by \n",
    "        # the number of nodes in each graph (n_node) to determine the \n",
    "        # shift along the sparse matrix axes for sender and receiver indices\n",
    "\n",
    "        s1 = tf.squeeze(tf.slice(sender_recv_list,(0,0),size=(-1,1)))\n",
    "        s2 = tf.squeeze(tf.slice(sender_recv_list,(0,1),size=(-1,1)))\n",
    "        s3 = tf.squeeze(tf.slice(sender_recv_list,(0,2),size=(-1,1)))\n",
    "        # print('s1, s2, s3', s1.shape, s2.shape, s3.shape)\n",
    "        # print('nodes', n_node)#.shape)\n",
    "\n",
    "        def indice_func(n):\n",
    "            return n_node[n]\n",
    "        \n",
    "        indice_update = tf.map_fn(fn=indice_func,\n",
    "                                  elems=s1\n",
    "                                  #fn_output_signature=\n",
    "                                  )\n",
    "        senders = s2 + indice_update\n",
    "        receivers = s3 + indice_update\n",
    "\n",
    "        # def send_func(n):\n",
    "        #     return s2 + s1*n\n",
    "      \n",
    "        # def rec_func(n):\n",
    "        #     return s3 + s1*n\n",
    "        \n",
    "        # senders = tf.map_fn(fn=send_func,\n",
    "        #                     elems=n_node,\n",
    "        #                     fn_output_signature=tf.TensorSpec(shape=[None], dtype=tf.int64)\n",
    "        #                     )\n",
    "        # receivers = tf.map_fn(fn=send_func,\n",
    "        #                     elems=n_node,\n",
    "        #                     fn_output_signature=tf.TensorSpec(shape=[None], dtype=tf.int64)\n",
    "        #                     )\n",
    "        #print(senders.shape)\n",
    "        # senders = tf.squeeze(tf.slice(sender_recv_list,(0,1),size=(-1,1))) + tf.squeeze(tf.slice(sender_recv_list,(0,0),size=(-1,1)))*n_node\n",
    "        # receivers = tf.squeeze(tf.slice(sender_recv_list,(0,2),size=(-1,1))) + tf.squeeze(tf.slice(sender_recv_list,(0,0),size=(-1,1)))*tf.shape(adj_matrix,out_type=tf.int64)[-1]\n",
    "        total_nodes = tf.reduce_sum(n_node, axis=0)\n",
    "        output_a = tf.sparse.SparseTensor(indices=tf.stack([senders,receivers],axis=1), values = tf.ones_like(senders),dense_shape=[total_nodes, total_nodes])\n",
    "        edge_distance = tf.expand_dims(tf.gather_nd(dist/max(_Lx, _Ly),sender_recv_list),-1)\n",
    "        #print(\"ed\", edge_distance.shape)\n",
    "        edge_x_distance = tf.expand_dims(tf.gather_nd(tf.math.cos(rel_angle_to_neigh),sender_recv_list),-1)  # neigbour position relative to sender heading\n",
    "        edge_y_distance = tf.expand_dims(tf.gather_nd(tf.math.sin(rel_angle_to_neigh),sender_recv_list),-1)  # neigbour position relative to sender heading\n",
    "        edge_x_orientation = tf.expand_dims(tf.gather_nd(dvx,sender_recv_list),-1)  # neigbour velocity relative to sender heading\n",
    "        edge_y_orientation = tf.expand_dims(tf.gather_nd(dvy,sender_recv_list),-1)  # neigbour velocity relative to sender heading\n",
    "\n",
    "\n",
    "        output_e = tf.concat([edge_distance,edge_x_distance,edge_y_distance,edge_x_orientation,edge_y_orientation],axis=-1)\n",
    "        #edges = tf.concat([edge_distance,edge_x_distance,edge_y_distance],axis=-1)\n",
    "        #node_velocities = tf.transpose(V, perm=[0,2,1,3])\n",
    "        def vel_transpose_func(x):\n",
    "            x = x.to_tensor()\n",
    "            x = tf.transpose(x, perm=[1,0,2])\n",
    "            return tf.reshape(x, (-1, 2*(WINDOW_SIZE-1)))\n",
    "        #print(V.shape)\n",
    "        node_velocities = tf.map_fn(fn=vel_transpose_func,\n",
    "                                    elems=V,\n",
    "                                    fn_output_signature=tf.RaggedTensorSpec(shape=[None, None],\n",
    "                                                                            ragged_rank=0, \n",
    "                                                                            dtype=tf.float64),\n",
    "                                    infer_shape=False\n",
    "                                    )\n",
    "\n",
    "        #node_accelerations = tf.transpose(A, perm=[0,2,1,3])\n",
    "        output_x = tf.concat([node_velocities, boundary_dists], axis=-1)\n",
    "        output_x = output_x.merge_dims(0,1)\n",
    "        #output_x = tf.reshape(node_velocities,(-1,2*(WINDOW_SIZE-1)))\n",
    "\n",
    "        return output_x, output_a, output_e#, output_i\n",
    "\n",
    "naive_ppfunc = functools.partial(naive_pre_process_function,\n",
    "                           _Lx=DOMAIN_SIZE[0],\n",
    "                           _Ly=DOMAIN_SIZE[1]\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a143e099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 27, 4)\n"
     ]
    }
   ],
   "source": [
    "for i in windows_as_list:\n",
    "    print(i.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1293d775",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "1293d775",
     "kernelId": "4db5fd37-e23d-456f-8ba8-726fa1089326"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 27, 4)\n"
     ]
    }
   ],
   "source": [
    "def split_targets(x, w=WINDOW_SIZE):\n",
    "    inputs = (x[0:w-1,:,0:2], x[0:w-1,:,2:4])\n",
    "    targets = x[None,-1,:,2:4]\n",
    "    return (inputs, targets)\n",
    "\n",
    "split_with_window = functools.partial(\n",
    "  split_targets,\n",
    "  w=WINDOW_SIZE)  \n",
    "\n",
    "train_total = 100\n",
    "valid_total = 10\n",
    "\n",
    "#windows_list = random.sample(windows_as_list,train_total+valid_total)\n",
    "\n",
    "window_dataset = tf.data.Dataset.from_generator(lambda: windows_as_list,\n",
    "                                                output_signature=tf.TensorSpec(shape=(WINDOW_SIZE,None,4), dtype=tf.float64))\n",
    "for i in window_dataset:\n",
    "    print(i.shape)\n",
    "    break\n",
    "#window_dataset.map(lambda x: tf.data.Dataset.from_tensor_slices(x))\n",
    "\n",
    "# #window_dataset = tf.data.Dataset.from_tensor_slices(windows_list)\n",
    "window_dataset = window_dataset.map(split_with_window)\n",
    "window_dataset = window_dataset.repeat(REPEATS)\n",
    "window_dataset = window_dataset.shuffle(10000, reshuffle_each_iteration=True)\n",
    "window_dataset = window_dataset.apply(tf.data.experimental.dense_to_ragged_batch(BATCH_SIZE, drop_remainder=True))\n",
    "window_dataset = window_dataset.take(train_total+valid_total)\n",
    "window_dataset = window_dataset.map(lambda x, y: (naive_ppfunc(x[0], x[1]), y))\n",
    "train_dataset = window_dataset.skip(valid_total)\n",
    "valid_dataset = window_dataset.take(valid_total)\n",
    "# for i in window_dataset:\n",
    "#     print(i[1])\n",
    "#     break\n",
    "\n",
    "#print(len(list(window_dataset)))\n",
    "\n",
    "#valid_dataset = window_dataset.take(valid_total)\n",
    "#train_dataset = window_dataset.skip(valid_total)\n",
    "#train_total = train_dataset.reduce(0, lambda x, _: x + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e78ba5aa",
   "metadata": {
    "gradient": {
     "execution_count": 15,
     "id": "e78ba5aa",
     "kernelId": "4db5fd37-e23d-456f-8ba8-726fa1089326"
    }
   },
   "outputs": [],
   "source": [
    "#@title ### Model { form-width: \"30%\" }\n",
    "\n",
    "min_lr = 1e-6\n",
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-3 - min_lr,\n",
    "                                decay_steps=int(5e6),\n",
    "                                decay_rate=0.1) #+ min_lr\n",
    "MLP_SIZE=4\n",
    "n_out=2\n",
    "\n",
    "class GNNNet(Model):\n",
    "    def __init__(self,n_out=4,mp_steps=2):\n",
    "        super().__init__()\n",
    "        #self.preprocess = PreprocessingLayer(DOMAIN_SIZE)\n",
    "        self.encoder = XENetConv([MLP_SIZE,MLP_SIZE], MLP_SIZE, 2*MLP_SIZE, use_bias=False, node_activation=\"tanh\", edge_activation=\"tanh\") #ECCConv(32, activation=\"tanh\")\n",
    "        \n",
    "        self.process = XENetConv([MLP_SIZE,MLP_SIZE], MLP_SIZE, 2*MLP_SIZE, use_bias=False, node_activation=\"relu\", edge_activation=\"relu\")# MessagePassing(aggregate='mean')# 32, activation=\"relu\")\n",
    "        \n",
    "        #self.decoder = Dense(tfpl.IndependentNormal.params_size(n_out), activation=\"tanh\",use_bias=False) #ECCConv(n_out, activation=\"tanh\")\n",
    "        \n",
    "        self.mean_decoder = Dense(n_out, activation=\"linear\",use_bias=False) #ECCConv(n_out, activation=\"tanh\")\n",
    "        self.std_decoder = Dense(n_out, activation=\"linear\",kernel_initializer=tf.keras.initializers.Zeros(),bias_initializer=tf.keras.initializers.Constant(-2.)) #ECCConv(n_out, activation=\"tanh\")\n",
    "\n",
    "        self.distribution = tfp.layers.IndependentNormal(n_out)#lambda (mu, std): tfd.Normal(loc=mu, scale=std))\n",
    "        #self.global_pool = GlobalAvgPool()\n",
    "        #self.dense = Dense(32, activation=\"relu\")\n",
    "        #self.final = Dense(n_out, activation=\"sigmoid\")\n",
    "        self.mp_steps=mp_steps\n",
    "    \n",
    "    #@tf.function()\n",
    "    def call(self, inputs, train_mode=True):\n",
    "        #x, a, e, i = self.preprocess(inputs, train_mode=train_mode)\n",
    "        #x.set_shape([None,None])\n",
    "        #print(x.shape, a.shape, e.shape, i.shape)\n",
    "        x, a, e = inputs\n",
    "        x, e = self.encoder([x, a, e])\n",
    "        #x = self.node_encoder(x)\n",
    "        #e = self.edge_encoder(e)\n",
    "        for _ in range(self.mp_steps):\n",
    "            x, e = self.process([x, a, e])         \n",
    "        mu = self.mean_decoder(x)\n",
    "        std = self.std_decoder(x)\n",
    "        x = tf.keras.layers.concatenate([mu,std])\n",
    "#        x = self.distribution([mu,std])\n",
    "#        x = self.decoder(x)\n",
    "        x = self.distribution(x)\n",
    "        #print('model', x)\n",
    "        #x = self.global_pool([x, i])\n",
    "        #x = self.dense(x)\n",
    "        #x = self.final(x)\n",
    "\n",
    "        #x = tf.reshape(x, (-1, N, 2))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "model = GNNNet(n_out=n_out)\n",
    "optimizer = Adam(lr)\n",
    "loss_fn = MeanSquaredError()\n",
    "\n",
    "def loss_fn(target,predicted_dist):\n",
    "    return -tf.reduce_sum(predicted_dist.log_prob(target.merge_dims(0,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e40ddc55",
   "metadata": {
    "gradient": {
     "execution_count": 16,
     "id": "e40ddc55",
     "kernelId": "4db5fd37-e23d-456f-8ba8-726fa1089326"
    }
   },
   "outputs": [],
   "source": [
    "#@title ### Train Step\n",
    "\n",
    "#@tf.function()#input_signature=[[tf.TensorSpec(shape=(None,None,2), dtype=tf.float32),tf.TensorSpec(shape=(None,None,2), dtype=tf.float32),tf.TensorSpec(shape=(None,None,2), dtype=tf.float32)],tf.TensorSpec(shape=(None,4), dtype=tf.float32)], experimental_relax_shapes=True)\n",
    "def train_step(inputs, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        #print('predtar', predictions, target)\n",
    "        target = tf.cast(target, dtype=tf.float32)\n",
    "        loss = loss_fn(target, predictions) #+ sum(model.losses)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cde55d",
   "metadata": {
    "gradient": {
     "id": "77cde55d",
     "kernelId": "4db5fd37-e23d-456f-8ba8-726fa1089326"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total = 100, batch size = 128\n",
      "\n",
      "Start of epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 336485892.226190:  32%|███▏      | 32/100 [03:09<01:46,  1.57s/it]  "
     ]
    }
   ],
   "source": [
    "#@title ### Train Model { form-width: \"30%\" }\n",
    "\n",
    "step = losses = 0\n",
    "epochs = 10  # Number of training epochs\n",
    "batch_size = BATCH_SIZE  # Batch size\n",
    "print(f\"total = {train_total}, batch size = {batch_size}\")\n",
    "divisor=20\n",
    "loss_values = np.zeros((epochs,train_total))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    pbar = tqdm(enumerate(window_dataset), total=train_total)\n",
    "    \n",
    "    for step, batch in pbar:\n",
    "        inputs, target = batch\n",
    "        losses = train_step(inputs,target)\n",
    "        loss_values[epoch,step]=losses.numpy()\n",
    "        if step%divisor==0:\n",
    "            pbar.set_description(\"loss %f\" % (tf.reduce_mean(loss_values[epoch,0:step+1]).numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c31540",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ~/.nv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28710202",
   "metadata": {
    "gradient": {
     "id": "28710202",
     "kernelId": "4db5fd37-e23d-456f-8ba8-726fa1089326"
    }
   },
   "outputs": [],
   "source": [
    "#@title ### Loss Plot\n",
    "\n",
    "compression = 100\n",
    "\n",
    "loss = np.ndarray.flatten(loss_values)\n",
    "#loss = np.nanmean(np.pad(loss.astype(float), (0, compression - loss.size%compression), mode='constant', constant_values=np.NaN).reshape(-1, compression), axis=1)\n",
    "plt.plot(loss[100:])\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3fbc18",
   "metadata": {
    "gradient": {
     "id": "5a3fbc18",
     "kernelId": "4db5fd37-e23d-456f-8ba8-726fa1089326"
    }
   },
   "outputs": [],
   "source": [
    "#@title ### Validation\n",
    "\n",
    "pred_list = []\n",
    "true_values = []\n",
    "valid_loss=0\n",
    "c=0\n",
    "\n",
    "for databatch in tqdm(parsed_train_dataset):\n",
    "\n",
    "    target = databatch[1]\n",
    "    true_values.append(tf.expand_dims(target.merge_dims(0,-2),0).numpy())\n",
    "\n",
    "    predictions = model(databatch[0],train_mode=False)\n",
    "    pred_list.append((predictions.sample(1).numpy()))\n",
    "    target = tf.cast(target, dtype=tf.float32)\n",
    "    #loss_value = tf.keras.losses.MeanSquaredError()(target,predictions).numpy()\n",
    "    loss_value = loss_fn(target, predictions).numpy()\n",
    "    valid_loss+= loss_value\n",
    "    c+=1\n",
    "    if c>=50:\n",
    "        break\n",
    "\n",
    "print('validation loss', valid_loss/c)\n",
    "#print(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76962ab",
   "metadata": {
    "gradient": {
     "id": "d76962ab",
     "kernelId": "4db5fd37-e23d-456f-8ba8-726fa1089326"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.pylabtools import figsize\n",
    "#@title ### Sample Predictions v Targets Plot { form-width: \"30%\" }\n",
    "# plt.plot(tf.expand_dims(target.merge_dims(0,-2),0).numpy()[0,:,1],\n",
    "#          predictions.sample(1).numpy()[0,:,1],\n",
    "#          '.')\n",
    "# plt.ylim((-1,1))\n",
    "#target.numpy().shape\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(10,4))\n",
    "ax1.plot(tf.expand_dims(target.merge_dims(0,-2),0).numpy()[0,:,0],\n",
    "         predictions.sample(1).numpy()[0,:,0],\n",
    "         '.')\n",
    "ax2.plot(tf.expand_dims(target.merge_dims(0,-2),0).numpy()[0,:,1],\n",
    "         predictions.sample(1).numpy()[0,:,1],\n",
    "         '.')\n",
    "ax1.set_ylim(-1,1)\n",
    "ax1.set_title('X-coords')\n",
    "ax2.set_title('Y-coords')\n",
    "#ax2.scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71712c83",
   "metadata": {
    "gradient": {
     "id": "71712c83",
     "kernelId": "4db5fd37-e23d-456f-8ba8-726fa1089326"
    }
   },
   "outputs": [],
   "source": [
    "#@title ### Predictions v Targets Mean\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(16, 8), facecolor='w', edgecolor='k')  \n",
    "print(pred_list[0].shape)\n",
    "axs = axs.ravel()\n",
    "for pred_i in range(2):\n",
    "    #pred_vals = np.array([pp[:,:,pred_i] for pp in pred_list],dtype=object).flatten()\n",
    "    #true_vals = np.array([tt[:,:,pred_i] for tt in true_values]).flatten()\n",
    "\n",
    "    pred_vals = np.concatenate([(np.tanh(pp[:,:,pred_i])/0.7).squeeze() for pp in pred_list])\n",
    "    true_vals = np.concatenate([(np.tanh(pp[:,:,pred_i])/0.7).squeeze() for pp in true_values])\n",
    "\n",
    "    bin_means, bin_edges, binnumber = stats.binned_statistic(true_vals, pred_vals,bins=100,range=(-1,1))\n",
    "    bin_width = (bin_edges[1] - bin_edges[0])\n",
    "    bin_centers = bin_edges[1:] - bin_width/2\n",
    "\n",
    "    bin_stds, bin_edges, binnumber = stats.binned_statistic(true_vals, pred_vals,statistic='std',bins=100,range=(-1,1))\n",
    "\n",
    "\n",
    "    axs[pred_i].plot(bin_centers,bin_means,c='C0')\n",
    "\n",
    "    axs[pred_i].fill_between(bin_centers,bin_means-bin_stds,bin_means+bin_stds,color='C0',alpha=0.5)\n",
    "\n",
    "    xx = np.linspace(bin_edges.min(),bin_edges.max(),10)\n",
    "    axs[pred_i].plot(xx,xx,c='k',ls='--')\n",
    "\n",
    "    axs[pred_i].set_ylabel('GNN prediction of parameter')\n",
    "    axs[pred_i].set_xlabel('True parameter that generated the microstate')\n",
    "    #axs[pred_i].set_xlim(-1,1)\n",
    "    #axs[pred_i].set_ylim(-1,1)\n",
    "\n",
    "\n",
    "#plt.savefig('gnn_' + str(run) + '.png',dpi=300)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c6fd8d",
   "metadata": {
    "gradient": {
     "id": "13c6fd8d",
     "kernelId": "4db5fd37-e23d-456f-8ba8-726fa1089326"
    }
   },
   "outputs": [],
   "source": [
    "#@title ### Rollout Functions\n",
    "\n",
    "INPUT_SEQUENCE_LENGTH=WINDOW_SIZE-1\n",
    "\n",
    "KINEMATIC_PARTICLE_ID = 3\n",
    "def get_kinematic_mask(particle_types):\n",
    "  \"\"\"Returns a boolean mask, set to true for kinematic (obstacle) particles.\"\"\"\n",
    "  return tf.equal(particle_types, KINEMATIC_PARTICLE_ID)\n",
    "\n",
    "def rollout(simulator, features, num_steps):\n",
    "  \"\"\"Rolls out a trajectory by applying the model in sequence.\"\"\"\n",
    "  N = features[0].shape[1]\n",
    "  initial_positions = features[0][tf.newaxis,0:INPUT_SEQUENCE_LENGTH]\n",
    "  initial_velocities = features[1][tf.newaxis,0:INPUT_SEQUENCE_LENGTH]\n",
    "  #initial_accelerations = features['acc'][tf.newaxis,0:INPUT_SEQUENCE_LENGTH]\n",
    "  ground_truth_positions = features[0][tf.newaxis,INPUT_SEQUENCE_LENGTH:]\n",
    "  ground_truth_velocities = features[1][tf.newaxis,INPUT_SEQUENCE_LENGTH:]\n",
    "  #ground_truth_accelerations = features['acc'][tf.newaxis,INPUT_SEQUENCE_LENGTH:]\n",
    "  global_context = None\n",
    "  #print('inits', initial_positions.shape)\n",
    "  def step_fn(step, current_positions, current_velocities, pos_predictions, vel_predictions):\n",
    "    if global_context is None:\n",
    "      global_context_step = None\n",
    "    else:\n",
    "      global_context_step = global_context[\n",
    "          step + INPUT_SEQUENCE_LENGTH - 1][tf.newaxis]\n",
    "    simulator_inputs = ppfunc(current_positions, current_velocities)\n",
    "    next_velocity = simulator(simulator_inputs).sample(1)\n",
    "    #print(1, next_velocity)\n",
    "    #print(2, current_positions[:,-1])\n",
    "    #next_velocity = next_acceleration*dt+current_velocities[:,-1]\n",
    "    next_velocity /= tf.expand_dims(tf.norm(next_velocity, axis=-1),-1)\n",
    "    next_position = next_velocity+current_positions[:,-1]\n",
    "    next_position = tf.where(next_position<0, next_position+DOMAIN_SIZE, next_position)\n",
    "    next_position = tf.where(next_position>DOMAIN_SIZE, next_position-DOMAIN_SIZE, next_position)\n",
    "    #next_acceleration = (next_velocity - current_velocities[:,-1])/dt\n",
    "    # Update kinematic particles from prescribed trajectory.\n",
    "    kinematic_mask = get_kinematic_mask(tf.zeros(N))\n",
    "    next_position_ground_truth = ground_truth_positions[:,step,:,:]\n",
    "    next_velocity_ground_truth = ground_truth_velocities[:,step,:,:]\n",
    "    #next_acceleration_ground_truth = ground_truth_accelerations[:,step,:,:]\n",
    "    next_position = tf.where(tf.expand_dims(kinematic_mask,-1), next_position_ground_truth,\n",
    "                             next_position)\n",
    "    next_velocity = tf.where(tf.expand_dims(kinematic_mask,-1), next_velocity_ground_truth,\n",
    "                             next_velocity)\n",
    "    #next_acceleration = tf.where(tf.expand_dims(kinematic_mask,-1), next_acceleration_ground_truth,\n",
    "    #                         next_acceleration)\n",
    "    updated_pos_predictions = pos_predictions.write(step, next_position)\n",
    "    updated_vel_predictions = vel_predictions.write(step, next_velocity)\n",
    "    #updated_acc_predictions = acc_predictions.write(step, next_acceleration)\n",
    "    # Shift `current_positions`, removing the oldest position in the sequence\n",
    "    # and appending the next position at the end.\n",
    "    next_positions = tf.concat([current_positions[:,1:,:,:],\n",
    "                                next_position[:,tf.newaxis,:,:]], axis=1)\n",
    "    next_velocities = tf.concat([current_velocities[:,1:,:,:],\n",
    "                                next_velocity[:,tf.newaxis,:,:]], axis=1)\n",
    "    #next_accelerations = tf.concat([current_accelerations[:,1:,:,:],\n",
    "    #                            next_acceleration[:,tf.newaxis,:,:]], axis=1)\n",
    "  \n",
    "    return (step + 1, next_positions, next_velocities, updated_pos_predictions, updated_vel_predictions)\n",
    "  \n",
    "  pos_predictions = tf.TensorArray(size=num_steps, dtype=tf.float32)\n",
    "  vel_predictions = tf.TensorArray(size=num_steps, dtype=tf.float32)\n",
    "  #acc_predictions = tf.TensorArray(size=num_steps, dtype=tf.float32)\n",
    "\n",
    "  _, _, _, pos_predictions, vel_predictions = tf.while_loop(\n",
    "      cond=lambda step, state, vel, pos_prediction, vel_prediction: tf.less(step, num_steps),\n",
    "      body=step_fn,\n",
    "      loop_vars=(0, initial_positions, initial_velocities, pos_predictions, vel_predictions),\n",
    "      parallel_iterations=1)\n",
    "  \n",
    "  #pos_predictions = tf.transpose(pos_predictions, perm=[1,0,2,3])\n",
    "\n",
    "  output_dict = {\n",
    "      'initial_positions': initial_positions,\n",
    "      'initial_velocities': initial_velocities,\n",
    "      #'initial_accelerations': initial_accelerations,\n",
    "      'pos_predicted_rollout': pos_predictions.stack(),\n",
    "      'vel_predicted_rollout': vel_predictions.stack(),\n",
    "      #'acc_predicted_rollout': acc_predictions.stack(),\n",
    "      'ground_truth_position_rollout': ground_truth_positions,\n",
    "      'ground_truth_velocity_rollout': ground_truth_velocities,\n",
    "      #'ground_truth_acceleration_rollout': ground_truth_accelerations\n",
    "  }\n",
    "  #print(output_dict['pos_predicted_rollout'].shape)\n",
    "  \n",
    "  for field in ['pos_predicted_rollout','vel_predicted_rollout']:\n",
    "    output_dict[field]=tf.transpose(output_dict[field], perm=[1,0,2,3])\n",
    "\n",
    "\n",
    "  if global_context is not None:\n",
    "    output_dict['global_context'] = global_context\n",
    "  return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fbc574",
   "metadata": {
    "gradient": {
     "id": "c3fbc574",
     "kernelId": "4db5fd37-e23d-456f-8ba8-726fa1089326"
    }
   },
   "outputs": [],
   "source": [
    "#@title ### Generate Rollout { form-width: \"30%\" }\n",
    "\n",
    "#valid_dataset =  tf.data.TFRecordDataset(tf.data.Dataset.list_files([valid_dir + filename for filename in os.listdir(valid_dir)]))\n",
    "\n",
    "def reduce_rollout_windows(x, y):\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    indices = tf.where(tf.math.logical_not(tf.reduce_any(tf.math.logical_or(tf.math.is_nan(x), tf.math.is_nan(y)), axis=(0,2))))[:,0]\n",
    "    x = tf.gather(x, indices, axis=1)\n",
    "    y = tf.gather(y, indices, axis=1)\n",
    "    #print('x2', x)\n",
    "    return tf.data.Dataset.from_tensors((x, y))\n",
    "\n",
    "parsed_rollout_dataset = tf.data.Dataset.from_tensors((pos, vel))\n",
    "#parsed_rollout_dataset = parsed_rollout_dataset.map(lambda x, y: naive_ppfunc(x, y))\n",
    "#parsed_rollout_dataset = make_window_dataset(pos, vel, window_size=1)\n",
    "parsed_rollout_dataset = parsed_rollout_dataset.flat_map(reduce_rollout_windows)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for databatch in parsed_rollout_dataset:\n",
    "    num_steps = databatch[0].shape[0]-INPUT_SEQUENCE_LENGTH\n",
    "    #databatch = naive_ppfunc(tf.RaggedTensor.from_tensor(databatch[0]), tf.RaggedTensor.from_tensor(databatch[1]))\n",
    "    #print('db', databatch)\n",
    "    output = rollout(model,databatch,num_steps)\n",
    "    break\n",
    "\n",
    "#output['metadata'] = metadata\n",
    "filename = './zonal_model.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(output, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e78c364",
   "metadata": {
    "gradient": {
     "id": "4e78c364",
     "kernelId": "4db5fd37-e23d-456f-8ba8-726fa1089326"
    }
   },
   "outputs": [],
   "source": [
    "#@title ### OP Plot\n",
    "\n",
    "pred_v = output['vel_predicted_rollout'][0,...]\n",
    "true_v = output['ground_truth_velocity_rollout'][0,...]\n",
    "\n",
    "pred_op = np.linalg.norm(np.mean(pred_v,axis=1),axis=-1)\n",
    "true_op = np.linalg.norm(np.mean(true_v,axis=1),axis=-1)\n",
    "\n",
    "plt.plot(true_op, label='truths')\n",
    "plt.plot(pred_op, label='predictions')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5d3473",
   "metadata": {
    "gradient": {
     "id": "7c5d3473",
     "kernelId": "4db5fd37-e23d-456f-8ba8-726fa1089326"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
