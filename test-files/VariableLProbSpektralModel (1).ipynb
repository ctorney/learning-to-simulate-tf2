{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "id": "8SPTqZ5bToXO",
    "outputId": "6c6ae6ea-b54a-4c52-cec9-2cbc91105b9c"
   },
   "outputs": [],
   "source": [
    "!pip install spektral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "VfoYzr9AWlXB"
   },
   "outputs": [],
   "source": [
    "#@title ### Imports { form-width: \"30%\" }\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "from math import *\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation, rc\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import pickle\n",
    "\n",
    "import functools\n",
    "#from tqdm import tqdm\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "import shutil\n",
    "\n",
    "from spektral.layers import ECCConv, GlobalAvgPool, MessagePassing, XENetConv, GlobalAttentionPool, GlobalMaxPool, GlobalSumPool, GlobalAttnSumPool, CrystalConv\n",
    "tfpl = tfp.layers\n",
    "tfd = tfp.distributions\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.style.use('seaborn-paper') \n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djgc1AVtrAdy"
   },
   "source": [
    "# Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "tGQdTJz1VIKe"
   },
   "outputs": [],
   "source": [
    "#@title ### Zonal Model { form-width: \"30%\" }\n",
    "\n",
    "def get_record(group_id,pos,vel):\n",
    "    feature = { 'group_id': tf.train.Feature(float_list=tf.train.FloatList(value=[group_id])),\n",
    "                'pos': tf.train.Feature(bytes_list=tf.train.BytesList(value=[pos.numpy()])),\n",
    "                'vel': tf.train.Feature(bytes_list=tf.train.BytesList(value=[vel.numpy()]))\n",
    "                #'acc': tf.train.Feature(bytes_list=tf.train.BytesList(value=[acc.numpy()]))\n",
    "                }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "\n",
    "class zonal_model:\n",
    "    def __init__(self, N, timesteps, discard, repeat, dt, save_interval,train_directory='train_datasets', valid_directory='valid_datasets', disable_progress=False):\n",
    "        self.N = N\n",
    "        self.timesteps = timesteps\n",
    "        self.discard = discard\n",
    "        self.B = repeat  # repeat for B batches\n",
    "        self.L = 0\n",
    "        self.dt = dt\n",
    "        self.save_interval = save_interval\n",
    "        \n",
    "        self.micro_state = np.zeros((self.B, (self.timesteps - self.discard)//self.save_interval, N, 4),dtype=np.float32)\n",
    "\n",
    "        self.sim_counter=0\n",
    "\n",
    "        if not os.path.exists(train_directory):\n",
    "            os.makedirs(train_directory)\n",
    "\n",
    "        if not os.path.exists(valid_directory):\n",
    "            os.makedirs(valid_directory)\n",
    "\n",
    "        self.train_directory = train_directory\n",
    "        self.valid_directory = valid_directory\n",
    "\n",
    "        # turn progress bar on or off\n",
    "        self.disable_progress = disable_progress\n",
    "\n",
    "        self.valid_fraction = 0.1\n",
    "        \n",
    "    def initialise_state(self, L):\n",
    "\n",
    "        #self.positions = tf.random.uniform((self.B,self.N,2),0.5*self.L, 0.5*self.L+20) #0,self.L)\n",
    "        self.positions = tf.random.uniform((self.B,self.N,2),0, L) \n",
    "        self.angles = tf.random.uniform((self.B,self.N,1), 0, 2*pi) #\n",
    "        \n",
    "\n",
    "\n",
    "    def run_sim(self, *params):\n",
    "\n",
    "        L, eta, Ra, Ro, Rr, vs, va, sigma = params\n",
    "\n",
    "        self.initialise_state(L)\n",
    "        self.L=L\n",
    "        \n",
    "        record_file = self.train_directory + '/microstates-' + str(self.sim_counter) + '.tfrecords'\n",
    "        self.writer = tf.io.TFRecordWriter(record_file) \n",
    "        \n",
    "        valid_file = self.valid_directory + '/microstates-' + str(self.sim_counter) + '.tfrecords'\n",
    "        self.validwriter = tf.io.TFRecordWriter(valid_file)\n",
    "        \n",
    "        # tensorflow function to run an update step\n",
    "        @tf.function\n",
    "        def update_tf(X, A):\n",
    "            cos_A = tf.math.cos(A)\n",
    "            sin_A = tf.math.sin(A)\n",
    "\n",
    "\n",
    "            Xx = tf.expand_dims(X[...,0],-1)\n",
    "            dx = -Xx + tf.linalg.matrix_transpose(Xx)\n",
    "            dx = tf.where(dx>0.5*L, dx-L, dx)\n",
    "            dx = tf.where(dx<-0.5*L, dx+L, dx)\n",
    "\n",
    "            Xy = tf.expand_dims(X[...,1],-1)\n",
    "            dy = -Xy + tf.linalg.matrix_transpose(Xy)\n",
    "            dy = tf.where(dy>0.5*L, dy-L, dy)\n",
    "            dy = tf.where(dy<-0.5*L, dy+L, dy)\n",
    "\n",
    "\n",
    "            angle_to_neigh = tf.math.atan2(dy, dx)\n",
    "            cos_N = tf.math.cos(angle_to_neigh)\n",
    "            sin_N = tf.math.sin(angle_to_neigh)\n",
    "            rel_angle_to_neigh = angle_to_neigh - A\n",
    "            rel_angle_to_neigh = tf.math.atan2(tf.math.sin(rel_angle_to_neigh), tf.math.cos(rel_angle_to_neigh))\n",
    "            \n",
    "            dist = tf.math.sqrt(tf.square(dx)+tf.square(dy))\n",
    "    \n",
    "            # repulsion \n",
    "            rep_x = tf.where(dist<=Rr, -dx, tf.zeros_like(dx))\n",
    "            rep_x = tf.where(rel_angle_to_neigh<0.5*va, rep_x, tf.zeros_like(rep_x))\n",
    "            rep_x = tf.where(rel_angle_to_neigh>-0.5*va, rep_x, tf.zeros_like(rep_x))\n",
    "            rep_x = tf.math.divide_no_nan(rep_x,tf.math.square(dist))\n",
    "            rep_x = tf.reduce_sum(rep_x,axis=2)\n",
    "\n",
    "            rep_y = tf.where(dist<=Rr, -dy, tf.zeros_like(dy))\n",
    "            rep_y = tf.where(rel_angle_to_neigh<0.5*va, rep_y, tf.zeros_like(rep_y))\n",
    "            rep_y = tf.where(rel_angle_to_neigh>-0.5*va, rep_y, tf.zeros_like(rep_y))\n",
    "            rep_y = tf.math.divide_no_nan(rep_y,tf.math.square(dist))\n",
    "            rep_y = tf.reduce_sum(rep_y,axis=2)\n",
    "\n",
    "            # alignment \n",
    "            align_x = tf.where(dist<=Ro, cos_A, tf.zeros_like(cos_A))\n",
    "            align_x = tf.where(rel_angle_to_neigh<0.5*va, align_x, tf.zeros_like(align_x))\n",
    "            align_x = tf.where(rel_angle_to_neigh>-0.5*va, align_x, tf.zeros_like(align_x))\n",
    "            align_x = tf.reduce_sum(align_x,axis=1)\n",
    "            \n",
    "            align_y = tf.where(dist<=Ro, sin_A, tf.zeros_like(sin_A))\n",
    "            align_y = tf.where(rel_angle_to_neigh<0.5*va, align_y, tf.zeros_like(align_y))\n",
    "            align_y = tf.where(rel_angle_to_neigh>-0.5*va, align_y, tf.zeros_like(align_y))\n",
    "            align_y = tf.reduce_sum(align_y,axis=1)\n",
    "\n",
    "            al_norm = tf.math.sqrt(align_x**2+align_y**2)\n",
    "            align_x = tf.math.divide_no_nan(align_x,al_norm)\n",
    "            align_y = tf.math.divide_no_nan(align_y,al_norm)\n",
    "\n",
    "            # attractive interactions\n",
    "            attr_x = tf.where(dist<=Ra, dx, tf.zeros_like(dx))\n",
    "            attr_x = tf.where(rel_angle_to_neigh<0.5*va, attr_x, tf.zeros_like(attr_x))\n",
    "            attr_x = tf.where(rel_angle_to_neigh>-0.5*va, attr_x, tf.zeros_like(attr_x))\n",
    "            attr_x = tf.reduce_sum(attr_x,axis=2)\n",
    "\n",
    "            attr_y = tf.where(dist<=Ra, dy, tf.zeros_like(dy))\n",
    "            attr_y = tf.where(rel_angle_to_neigh<0.5*va, attr_y, tf.zeros_like(attr_y))\n",
    "            attr_y = tf.where(rel_angle_to_neigh>-0.5*va, attr_y, tf.zeros_like(attr_y))\n",
    "            attr_y = tf.reduce_sum(attr_y,axis=2)\n",
    "\n",
    "            at_norm = tf.math.sqrt(attr_x**2+attr_y**2)\n",
    "            attr_x = tf.math.divide_no_nan(attr_x,at_norm)\n",
    "            attr_y = tf.math.divide_no_nan(attr_y,at_norm)\n",
    "\n",
    "            # combine angles and convert to desired angle change\n",
    "            social_x = rep_x + align_x + attr_x\n",
    "            social_y = rep_y + align_y + attr_y\n",
    "\n",
    "            d_angle = tf.math.atan2(social_y,social_x)\n",
    "            d_angle = tf.expand_dims(d_angle,-1)\n",
    "\n",
    "            \n",
    "            d_angle = tf.math.atan2((1-eta)*tf.math.sin(d_angle) + eta*sin_A, (1-eta)*tf.math.cos(d_angle) + eta*cos_A)\n",
    "\n",
    "            d_angle = d_angle - A\n",
    "            d_angle = tf.where(d_angle>pi, d_angle-2*pi, d_angle)\n",
    "            d_angle = tf.where(d_angle<-pi, d_angle+2*pi, d_angle)\n",
    "\n",
    "\n",
    "            # add perception noise\n",
    "            noise = sigma*tf.random.uniform(shape=(self.B,self.N,1),minval=-pi/2,maxval=pi/2)\n",
    "            d_angle = d_angle + noise\n",
    "            \n",
    "            # restrict to maximum turning angle\n",
    "            #d_angle = tf.where(tf.math.abs(d_angle)>eta*self.dt, tf.math.sign(d_angle)*eta*self.dt, d_angle)\n",
    "            \n",
    "            # rotate headings\n",
    "            A = A + d_angle\n",
    "            \n",
    "            # update positions\n",
    "            velocity = self.dt*vs*tf.concat([tf.cos(A),tf.sin(A)],axis=-1)\n",
    "            X += velocity\n",
    "\n",
    "            # add periodic boundary conditions\n",
    "            A = tf.where(A<-pi,  A+2*pi, A)\n",
    "            A = tf.where(A>pi, A-2*pi, A)\n",
    "\n",
    "            X = tf.where(X>L, X-L, X)\n",
    "            X = tf.where(X<0, X+L, X)\n",
    "\n",
    "            X = tf.where(X>L, X-L, X)\n",
    "            X = tf.where(X<0, X+L, X)\n",
    "\n",
    "            return X, A\n",
    "            \n",
    "        self.initialise_state(L)\n",
    "\n",
    "        counter=0\n",
    "        for i in tqdm(range(self.timesteps),disable=self.disable_progress):\n",
    "            self.positions, self.angles = update_tf(self.positions,  self.angles)\n",
    "            if i>=self.discard:\n",
    "                if i%self.save_interval==0:\n",
    "                    # store in an array in case we want to visualise\n",
    "                    self.micro_state[:,counter,:,0:2] = self.positions.numpy()\n",
    "                    self.micro_state[:,counter,:,2:3] = np.cos(self.angles.numpy())\n",
    "                    self.micro_state[:,counter,:,3:4] = np.sin(self.angles.numpy())\n",
    "                    \n",
    "                        \n",
    "                    \n",
    "\n",
    "                    counter = counter + 1\n",
    "\n",
    "        for b in range(self.B):\n",
    "            self.save_tf_record(b,L)\n",
    "\n",
    "        self.writer.close()\n",
    "        self.validwriter.close()\n",
    "        self.sim_counter+=1\n",
    "        return \n",
    "\n",
    "    def save_tf_record(self, b, L):\n",
    "        pos =  tf.io.serialize_tensor(self.micro_state[b,:,:,0:2])\n",
    "        vel =  tf.io.serialize_tensor(self.micro_state[b,:,:,2:4])\n",
    "        #acc =  tf.io.serialize_tensor(np.gradient(self.micro_state[b,:,:,2:4], axis=0))\n",
    "\n",
    "        tf_record = get_record(L,pos,vel)\n",
    "        if b> self.B*self.valid_fraction:\n",
    "            self.writer.write(tf_record.SerializeToString())\n",
    "        else:\n",
    "            self.validwriter.write(tf_record.SerializeToString())\n",
    "\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "MR2QG75OVIKk"
   },
   "outputs": [],
   "source": [
    "#@title ### Params\n",
    "\n",
    "n_points=50 #10\n",
    "\n",
    "param_values = np.linspace(0,25,n_points)\n",
    "L= 10\n",
    "N= 100\n",
    "repeat = 5\n",
    "discard = 0\n",
    "timesteps = 500\n",
    "save_interval=1\n",
    "dt=1\n",
    "\n",
    "\n",
    "latt=0  # adapt\n",
    "lrep= 0 # adapt\n",
    "lali= 1 # adapt\n",
    "eta=0.5 # adapt\n",
    "va=2*pi # adapt\n",
    "vs=0.03 # fix \n",
    "sigma=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "3bVMKzNyGD6R"
   },
   "outputs": [],
   "source": [
    "#@title ### Clear Data\n",
    "\n",
    "shutil.rmtree('train_datasets')\n",
    "shutil.rmtree('valid_datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WCWi7dRNVIKl",
    "outputId": "1c99f362-bc54-4bf8-f0ba-f9f1d2d10049"
   },
   "outputs": [],
   "source": [
    "#@title ### Generate Data\n",
    "\n",
    "sim = zonal_model(N,timesteps=timesteps+discard,discard=discard,repeat=repeat, dt=dt,save_interval=save_interval,disable_progress=True)\n",
    "def evaluate_zonal_model():\n",
    "    sim.run_sim(eta, latt, lali, lrep, vs, va, sigma)\n",
    "    return\n",
    "\n",
    "def create_simulation(N=N,L=L,eta=eta, latt=latt, lali=lali, lrep=lrep, vs=vs, va=va, sigma=sigma, dp=True):\n",
    "    sim.run_sim(L, eta, latt, lali, lrep, vs, va, sigma)\n",
    "    return\n",
    "\n",
    "#evaluate_zonal_model()\n",
    "rand_Ls=np.random.randint(3,50,size=100)\n",
    "for i in tqdm(rand_Ls):\n",
    "    create_simulation(L=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_Ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "3mIfRX3ZVIKm"
   },
   "outputs": [],
   "source": [
    "#@title ### Animation\n",
    "step_stride = 1\n",
    "block_on_show = True\n",
    "allpos = sim.micro_state[0,:,:,0:2]\n",
    "allvel = sim.micro_state[0,:,:,2:4]\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "plot_info = []\n",
    "Q={}\n",
    "\n",
    "ax_i = 1\n",
    "ax = axes#[ax_i]\n",
    "\n",
    "bounds = [(0,L), (0,L)]\n",
    "ax.set_xlim(bounds[0][0], bounds[0][1])\n",
    "ax.set_ylim(bounds[1][0], bounds[1][1])\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_aspect(1.)\n",
    "    \n",
    "X, Y = allpos[0,:,0],allpos[0,:,1]\n",
    "U, V = allvel[0,:,0],allvel[0,:,1]\n",
    "Q[f'{ax_i}'] = ax.quiver(X, Y, U, V, pivot='mid', color='b', units='inches')\n",
    "\n",
    "\n",
    "\n",
    "num_steps = allpos.shape[0]\n",
    "#print(pos_trajectory.shape)\n",
    "\n",
    "def update_quiver(num):\n",
    "\n",
    "    U, V = allvel[num,:,0], allvel[num,:,1]\n",
    "    offsets = allpos[num]\n",
    "\n",
    "    Q[f'{ax_i}'].set_offsets(offsets)\n",
    "    Q[f'{ax_i}'].set_UVC(U,V)\n",
    "    return axes\n",
    "\n",
    "anim = animation.FuncAnimation(fig, update_quiver, interval=50, frames=np.arange(0, num_steps, step_stride),blit=False)\n",
    "#plt.show(block=block_on_show)\n",
    "rc('animation', html='jshtml')\n",
    "anim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMGNTWjMWcTv"
   },
   "source": [
    "# Parsing/Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "CDyPgl9V-C3n"
   },
   "outputs": [],
   "source": [
    "#@title ### Parsing Functions\n",
    "\n",
    "def _parse_record(x):\n",
    "  # Parse the input tf.train.Example proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(x, feature_description)\n",
    "\n",
    "def _parse_tensor(x):\n",
    "    output = {'group_id': x['group_id'],\n",
    "              'pos': tf.io.parse_tensor(x['pos'],out_type=tf.float32),\n",
    "              'vel': tf.io.parse_tensor(x['vel'],out_type=tf.float32)\n",
    "              #'acc': tf.io.parse_tensor(x['acc'],out_type=tf.float32)\n",
    "              }\n",
    "    return output\n",
    "\n",
    "def make_window_dataset(x):\n",
    "    # make a dataset from the time series tensor\n",
    "    windows = tf.data.Dataset.from_tensor_slices((x['pos'],x['vel']))\n",
    "    # convert to windows\n",
    "    windows = windows.window(WINDOW_SIZE, shift=1, stride=1)\n",
    "    # take a batch of window size and combine pos, vel, acc to a single dataset\n",
    "    windows = windows.flat_map(lambda pos_ds,vel_ds: tf.data.Dataset.zip((pos_ds.batch(WINDOW_SIZE, drop_remainder=True),vel_ds.batch(WINDOW_SIZE, drop_remainder=True),tf.data.Dataset.from_tensors((x['group_id'])))))\n",
    "    return windows\n",
    "\n",
    "\n",
    "def split_targets(x, y, z, w=7):\n",
    "    inputs = (x[0:w-1], y[0:w-1], z)#, z[0:w-1])\n",
    "    targets = y[None,-1]\n",
    "    return (inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "LFTeLbhTVIKq"
   },
   "outputs": [],
   "source": [
    "#@title ### Create Dataset\n",
    "\n",
    "train_dir = 'train_datasets/'\n",
    "valid_dir = 'valid_datasets/'\n",
    "\n",
    "WINDOW_SIZE=3\n",
    "\n",
    "n_out = 2\n",
    "n_feat_node = (WINDOW_SIZE-1)*4+2\n",
    "n_feat_edge = 5\n",
    "\n",
    "BATCH_SIZE=128\n",
    "DOMAIN_SIZE=L\n",
    "\n",
    "\n",
    "all_file_list = [train_dir + filename for filename in os.listdir(train_dir)]\n",
    "\n",
    "#dataset_size = sum(1 for _ in tf.data.TFRecordDataset(all_file_list[0]))*len(all_file_list)//BATCH_SIZE\n",
    "\n",
    "\n",
    "feature_description = {'group_id': tf.io.FixedLenFeature([], tf.float32),\n",
    "                        'pos': tf.io.FixedLenFeature([], tf.string),\n",
    "                        'vel': tf.io.FixedLenFeature([], tf.string),\n",
    "                        #'acc': tf.io.FixedLenFeature([], tf.string)\n",
    "                        }\n",
    "\n",
    "\n",
    "split_with_window = functools.partial(\n",
    "  split_targets,\n",
    "  w=WINDOW_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset =  tf.data.TFRecordDataset(tf.data.Dataset.list_files([train_dir + filename for filename in os.listdir(train_dir)]))\n",
    "\n",
    "parsed_train_dataset = train_dataset.map(_parse_record)\n",
    "parsed_train_dataset = parsed_train_dataset.map(_parse_tensor)\n",
    "\n",
    "parsed_train_dataset = parsed_train_dataset.flat_map(make_window_dataset)\n",
    "parsed_train_dataset = parsed_train_dataset.map(split_with_window)\n",
    "parsed_train_dataset = parsed_train_dataset.shuffle(10000, reshuffle_each_iteration=True)\n",
    "#parsed_train_dataset = parsed_train_dataset.repeat(EPOCHS)\n",
    "parsed_train_dataset = parsed_train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "#parsed_train_dataset = parsed_train_dataset.map(_parse_graph)\n",
    "\n",
    "\n",
    "valid_dataset =  tf.data.TFRecordDataset(tf.data.Dataset.list_files([valid_dir + filename for filename in os.listdir(valid_dir)]))\n",
    "\n",
    "parsed_valid_dataset = valid_dataset.map(_parse_record)\n",
    "parsed_valid_dataset = parsed_valid_dataset.map(_parse_tensor)\n",
    "parsed_valid_dataset = parsed_valid_dataset.flat_map(make_window_dataset)\n",
    "parsed_valid_dataset = parsed_valid_dataset.map(split_with_window)\n",
    "parsed_valid_dataset = parsed_valid_dataset.shuffle(10000, reshuffle_each_iteration=True)\n",
    "#parsed_valid_dataset = parsed_valid_dataset.repeat(EPOCHS)\n",
    "parsed_valid_dataset = parsed_valid_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "#parsed_valid_dataset = parsed_valid_dataset.map(_parse_graph)\n",
    "\n",
    "total=len(list(parsed_train_dataset))\n",
    "valid_total=len(list(parsed_valid_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlzzQSMkAeqx"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "hOId7yMy2XWp"
   },
   "outputs": [],
   "source": [
    "#@title ### Preprocessing Layer\n",
    "\n",
    "class PreprocessingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(PreprocessingLayer, self).__init__()\n",
    "        #self._L = L\n",
    "        \n",
    "\n",
    "    #@tf.function(input_signature=([tf.TensorSpec(shape=(None,WINDOW_SIZE-1,N,2), dtype=tf.float32),tf.TensorSpec(shape=(None,WINDOW_SIZE-1,N,2), dtype=tf.float32),tf.TensorSpec(shape=(None,WINDOW_SIZE-1,N,2), dtype=tf.float32)]))\n",
    "    def call(self, inputs, train_mode=True, add_noise=True):\n",
    "        \n",
    "        X, V, L = inputs\n",
    "        L = tf.expand_dims(L,-1)\n",
    "        L = tf.expand_dims(L,-1)\n",
    "        if train_mode==True:\n",
    "            x_rand = 0.03\n",
    "            v_rand = 0.05\n",
    "            X += x_rand*tf.random.normal(X.shape, mean=0, stddev=1)\n",
    "            V += v_rand*tf.random.normal(V.shape, mean=0, stddev=1)\n",
    "        # input shape [batch, steps, num. agents, dims]\n",
    "        # node features xpos, ypos, xvel, yvel\n",
    "        # edge features distance, rel angle to receiver\n",
    "        X_current = X[:,-1]\n",
    "        V_current = V[:,-1]\n",
    "\n",
    "        Xx = tf.expand_dims(X_current[...,0],-1)\n",
    "        dx = -Xx + tf.linalg.matrix_transpose(Xx)\n",
    "        dx = tf.where(dx>0.5*L, dx-L, dx)\n",
    "        dx = tf.where(dx<-0.5*L, dx+L, dx)\n",
    "        #x_boundary_dist = tf.concat([Xx, self._L-Xx], axis=-1)\n",
    "\n",
    "        Xy = tf.expand_dims(X_current[...,1],-1)\n",
    "        dy = -Xy + tf.linalg.matrix_transpose(Xy)\n",
    "        dy = tf.where(dy>0.5*L, dy-L, dy)\n",
    "        dy = tf.where(dy<-0.5*L, dy+L, dy)\n",
    "        #y_boundary_dist = tf.concat([Xy, self._L-Xy], axis=-1)\n",
    "\n",
    "        #boundary_dist = tf.concat([x_boundary_dist, y_boundary_dist], axis=-1)\n",
    "\n",
    "        Vx = tf.expand_dims(V_current[...,0],-1)\n",
    "        dvx = -Vx + tf.linalg.matrix_transpose(Vx)\n",
    "\n",
    "        Vy = tf.expand_dims(V_current[...,1],-1)\n",
    "        dvy = -Vy + tf.linalg.matrix_transpose(Vy)\n",
    "        \n",
    "        dvnorm = tf.math.sqrt(dvx**2+dvy**2)\n",
    "        dvx = tf.math.divide_no_nan(dvx,dvnorm)\n",
    "        dvy = tf.math.divide_no_nan(dvy,dvnorm)\n",
    "\n",
    "        angles = tf.expand_dims(tf.math.atan2(V_current[...,1],V_current[...,0]),-1)\n",
    "        angle_to_neigh = tf.math.atan2(dy, dx)\n",
    "\n",
    "        rel_angle_to_neigh = angle_to_neigh - angles\n",
    "\n",
    "        dist = tf.math.sqrt(tf.square(dx)+tf.square(dy))\n",
    "\n",
    "        interaction_radius = 1.0# tf.reduce_mean(dist,axis=[1,2],keepdims=True)\n",
    "        adj_matrix = tf.where(dist<interaction_radius, tf.ones_like(dist,dtype=tf.int32), tf.zeros_like(dist,dtype=tf.int32))\n",
    "        adj_matrix = tf.linalg.set_diag(adj_matrix, tf.zeros(tf.shape(adj_matrix)[:2],dtype=tf.int32))\n",
    "        #print('adj', adj_matrix.shape)\n",
    "        sender_recv_list = tf.where(adj_matrix)\n",
    "        n_edge = tf.reduce_sum(adj_matrix, axis=[1,2])\n",
    "        n_node = tf.ones_like(n_edge)*tf.shape(adj_matrix)[-1]\n",
    "        #print('node shape', n_node.shape)\n",
    "        output_i = tf.repeat(tf.range(tf.shape(adj_matrix)[0]),n_node)\n",
    "        # print(1,tf.range(tf.shape(adj_matrix)[0]).shape)\n",
    "        # print(2,output_i.shape)\n",
    "        #print('sr list', sender_recv_list.shape)\n",
    "        senders = tf.squeeze(tf.slice(sender_recv_list,(0,1),size=(-1,1)))+ tf.squeeze(tf.slice(sender_recv_list,(0,0),size=(-1,1)))*tf.shape(adj_matrix,out_type=tf.int64)[-1]\n",
    "        receivers = tf.squeeze(tf.slice(sender_recv_list,(0,2),size=(-1,1))) + tf.squeeze(tf.slice(sender_recv_list,(0,0),size=(-1,1)))*tf.shape(adj_matrix,out_type=tf.int64)[-1]\n",
    "        #print(senders.shape, receivers.shape)\n",
    "        output_a = tf.sparse.SparseTensor(indices=tf.stack([senders,receivers],axis=1), values = tf.ones_like(senders),dense_shape=[tf.shape(output_i)[0],tf.shape(output_i)[0]])\n",
    "        edge_distance = tf.expand_dims(tf.gather_nd(dist/L,sender_recv_list),-1)\n",
    "        #print(\"ed\", edge_distance.shape)\n",
    "        edge_x_distance = tf.expand_dims(tf.gather_nd(tf.math.cos(rel_angle_to_neigh),sender_recv_list),-1)  # neigbour position relative to sender heading\n",
    "        edge_y_distance = tf.expand_dims(tf.gather_nd(tf.math.sin(rel_angle_to_neigh),sender_recv_list),-1)  # neigbour position relative to sender heading\n",
    "        edge_x_orientation = tf.expand_dims(tf.gather_nd(dvx,sender_recv_list),-1)  # neigbour velocity relative to sender heading\n",
    "        edge_y_orientation = tf.expand_dims(tf.gather_nd(dvy,sender_recv_list),-1)  # neigbour velocity relative to sender heading\n",
    "\n",
    "\n",
    "        output_e = tf.concat([edge_distance,edge_x_distance,edge_y_distance,edge_x_orientation,edge_y_orientation],axis=-1)\n",
    "        #edges = tf.concat([edge_distance,edge_x_distance,edge_y_distance],axis=-1)\n",
    "        node_velocities = tf.transpose(V, perm=[0,2,1,3])\n",
    "        #node_accelerations = tf.transpose(A, perm=[0,2,1,3])\n",
    "        \n",
    "        # shape = node_velocities.get_shape().as_list()\n",
    "        # dim = tf.math.reduce_prod(shape[1:])\n",
    "        output_x = tf.reshape(node_velocities,(-1,2*(WINDOW_SIZE-1)))\n",
    "        #node_accelerations = tf.reshape(node_accelerations,(-1,2*(WINDOW_SIZE-1)))\n",
    "        #node_positions = (X_current - (self._L/2.))/self._L\n",
    "        #node_positions = tf.reshape(node_positions,(-1,2))\n",
    "        #boundary_dist = tf.reshape(boundary_dist, (-1,4))\n",
    "        #output_x = tf.concat([node_velocities],axis=-1)\n",
    "        #         if train_mode==True and add_noise==True:\n",
    "        #             noise_std = 6.7e-4\n",
    "        #             num_velocities = WINDOW_SIZE-1\n",
    "        #             output_x+=tf.random.normal(tf.shape(output_x),\n",
    "        #                                       stddev=noise_std / num_velocities ** 0.5,\n",
    "        #                                       dtype=output_x.dtype)\n",
    "        #dist = tf.linalg.set_diag(dist, 25.0*tf.ones(tf.shape(dist)[:2],dtype=tf.float32))\n",
    "\n",
    "        #dist_out =  tf.reduce_mean(tf.reduce_min(dist,axis=[2]),axis=[1])\n",
    "        #X_T, V_T, A_T = targets\n",
    "        #node_targets = tf.concat([X_T, V_T, A_T], axis=-1)\n",
    "        #target = A_T\n",
    "        #output_x.set_shape((None,n_feat_node))\n",
    "        #output_a.set_shape((None,N))\n",
    "        #output_e.set_shape((None,5))\n",
    "        #input_graphs = utils_tf.set_zero_global_features(input_graphs,self._output_size)\n",
    "        return output_x, output_a, output_e, output_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "W4pqnbtbGG7u"
   },
   "outputs": [],
   "source": [
    "#@title ### Model\n",
    "\n",
    "min_lr = 1e-6\n",
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-3 - min_lr,\n",
    "                                decay_steps=int(5e6),\n",
    "                                decay_rate=0.1) #+ min_lr\n",
    "MLP_SIZE=4\n",
    "\n",
    "class GNNNet(Model):\n",
    "    def __init__(self,n_out=4,mp_steps=2):\n",
    "        super().__init__()\n",
    "        self.preprocess = PreprocessingLayer()\n",
    "        self.encoder = XENetConv([MLP_SIZE,MLP_SIZE], MLP_SIZE, 2*MLP_SIZE, node_activation=\"tanh\", edge_activation=\"tanh\", use_bias=False) #ECCConv(32, activation=\"tanh\")\n",
    "        \n",
    "        self.process = CrystalConv(MLP_SIZE, activation=\"relu\", use_bias=False)# MessagePassing(aggregate='mean')# 32, activation=\"relu\")\n",
    "        \n",
    "        #self.decoder = Dense(tfpl.IndependentNormal.params_size(n_out), activation=\"tanh\",use_bias=False) #ECCConv(n_out, activation=\"tanh\")\n",
    "        \n",
    "        self.mean_decoder = Dense(n_out, activation=\"tanh\",use_bias=False) #ECCConv(n_out, activation=\"tanh\")\n",
    "        self.std_decoder = Dense(n_out, activation=\"linear\",kernel_initializer=tf.keras.initializers.Zeros(),bias_initializer=tf.keras.initializers.Constant(-2.)) #ECCConv(n_out, activation=\"tanh\")\n",
    "\n",
    "        self.distribution = tfp.layers.IndependentNormal(n_out)#lambda (mu, std): tfd.Normal(loc=mu, scale=std))\n",
    "        #self.global_pool = GlobalAvgPool()\n",
    "        #self.dense = Dense(32, activation=\"relu\")\n",
    "        #self.final = Dense(n_out, activation=\"sigmoid\")\n",
    "        self.mp_steps=mp_steps\n",
    "    \n",
    "    def call(self, inputs, train_mode=True):\n",
    "        x, a, e, i = self.preprocess(inputs, train_mode=train_mode)\n",
    "        #x.set_shape([None,None])\n",
    "        #print(x.shape, a.shape, e.shape)#, i.shape)\n",
    "        x, e = self.encoder([x, a, e])\n",
    "        \n",
    "        #x = self.node_encoder(x)\n",
    "        #e = self.edge_encoder(e)\n",
    "        for _ in range(self.mp_steps):\n",
    "            x = self.process([x, a, e])         \n",
    "        \n",
    "        mu = self.mean_decoder(x)\n",
    "        std = self.std_decoder(x)\n",
    "        x = tf.keras.layers.concatenate([mu,std])\n",
    "#        x = self.distribution([mu,std])\n",
    "#        x = self.decoder(x)\n",
    "        x = self.distribution(x)\n",
    "        #x = self.global_pool([x, i])\n",
    "        #x = self.dense(x)\n",
    "        #x = self.final(x)\n",
    "\n",
    "        #x = tf.reshape(x, (-1, N, 2))\n",
    "        \n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "model = GNNNet(n_out=n_out)\n",
    "optimizer = Adam(lr)\n",
    "loss_fn = MeanSquaredError()\n",
    "\n",
    "def loss_fn(target,predicted_dist):\n",
    "    return -tf.reduce_sum(predicted_dist.log_prob(tf.reshape(target,(-1,2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "IPbjfwN1H9vy"
   },
   "outputs": [],
   "source": [
    "#@title ### Train Step\n",
    "\n",
    "#@tf.function()#input_signature=[[tf.TensorSpec(shape=(None,None,2), dtype=tf.float32),tf.TensorSpec(shape=(None,None,2), dtype=tf.float32),tf.TensorSpec(shape=(None,None,2), dtype=tf.float32)],tf.TensorSpec(shape=(None,4), dtype=tf.float32)], experimental_relax_shapes=True)\n",
    "def train_step(inputs, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, train_mode=True)\n",
    "        loss = loss_fn(target, predictions) #+ sum(model.losses)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def validation_step(ds):\n",
    "    predictions = model(inputs, train_mode=False)\n",
    "    loss = loss_fn(target, predictions)\n",
    "    return loss\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "PFV3nFtKICJc"
   },
   "outputs": [],
   "source": [
    "#@title ### Train Model { form-width: \"30%\" }\n",
    "step = losses = 0\n",
    "validate=False\n",
    "#total = 1000\n",
    "#parsed_train_dataset = parsed_train_dataset.take(total)\n",
    "epochs = 20  # Number of training epochs\n",
    "batch_size = BATCH_SIZE  # Batch size\n",
    "print(f\"total = {total}, batch size = {batch_size}\")\n",
    "divisor=20\n",
    "loss_values = np.zeros((epochs,total))\n",
    "if validate==True:\n",
    "    valid_loss_values = np.zeros((epochs,total))\n",
    "    ops = np.zeros(epochs)\n",
    "\n",
    "log_dir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "summary_writer = tf.summary.create_file_writer(logdir=log_dir)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "    epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    pbar = tqdm(enumerate(parsed_train_dataset), total=total)\n",
    "    if validate==True: val_bar = tqdm(enumerate(parsed_valid_dataset), total=valid_total)\n",
    "    \n",
    "    for step, batch in pbar:\n",
    "        inputs, target = batch\n",
    "        #print(inputs[2])\n",
    "        losses = train_step(inputs,target)\n",
    "        loss_values[epoch,step]=losses.numpy()\n",
    "        #epoch_loss_avg(losses)\n",
    "        #epoch_accuracy(y, model())\n",
    "        if step%divisor==0:\n",
    "            pbar.set_description(\"loss %f\" % (tf.reduce_mean(loss_values[epoch,0:step+1]).numpy()))\n",
    "    \n",
    "    if validate==True:\n",
    "        for step, batch in val_bar:\n",
    "            inputs, target = batch\n",
    "            #inputs = layer(inputs)\n",
    "            losses = validation_step(inputs,target)\n",
    "            valid_loss_values[epoch,step]=losses.numpy()\n",
    "            #epoch_loss_avg(losses)\n",
    "            #epoch_accuracy(y, model())\n",
    "            if step%divisor==0:\n",
    "                val_bar.set_description(\"loss %f\" % (tf.reduce_mean(valid_loss_values[epoch,0:step+1]).numpy()))\n",
    "        output_list = []\n",
    "        counter = 0\n",
    "        for databatch in tqdm(parsed_valid_dataset):\n",
    "            num_steps = databatch['pos'].shape[0]-INPUT_SEQUENCE_LENGTH\n",
    "            output = rollout(databatch,model,num_steps,N)\n",
    "            output_list.append(output)\n",
    "            counter+=1\n",
    "\n",
    "        pred_v_array = np.zeros((counter, num_steps, N, 2))\n",
    "        true_v_array = np.zeros((counter, num_steps, N, 2))\n",
    "        for i in range(counter):\n",
    "            pred_v_array[i]=output_list[i]['vel_predicted_rollout'][0,...]\n",
    "            true_v_array[i]=output_list[i]['ground_truth_velocity_rollout'][0,...]\n",
    "        op = np.mean(pred_v_array-true_v_array)\n",
    "        ops[epoch]=op\n",
    "        print(f'Order parameter difference at epoch {epoch} is {op}')\n",
    "\n",
    "    \n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('epoch_loss_avg', epoch_loss_avg.result(), step=optimizer.iterations)\n",
    "        tf.summary.scalar('epoch_accuracy', epoch_accuracy.result(), step=optimizer.iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfutD0vdvZZl"
   },
   "source": [
    "# Outputs/Rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "n7l4QZmaAa0U",
    "outputId": "15e07ee8-1f57-431a-b0bd-314fc08ff48a"
   },
   "outputs": [],
   "source": [
    "#@title ### Loss Plot\n",
    "\n",
    "compression = 10\n",
    "\n",
    "loss = np.ndarray.flatten(loss_values)\n",
    "loss = np.nanmean(np.pad(loss.astype(float), (0, compression - loss.size%compression), mode='constant', constant_values=np.NaN).reshape(-1, compression), axis=1)\n",
    "plt.plot(loss[100:])\n",
    "#plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VlfyVholCPXm",
    "outputId": "2dea6499-f882-46d3-c751-af3f233945e0"
   },
   "outputs": [],
   "source": [
    "#@title ### Validation\n",
    "\n",
    "pred_list = []\n",
    "true_values = []\n",
    "valid_loss=0\n",
    "c=0\n",
    "for databatch in tqdm(parsed_valid_dataset, total=valid_total):\n",
    "\n",
    "    target = databatch[1]\n",
    "    true_values.append(tf.reshape(target,(1,-1,2)).numpy())\n",
    "\n",
    "    predictions = model(databatch[0],train_mode=False)\n",
    "    pred_list.append((predictions.sample(1).numpy()))\n",
    "\n",
    "    #loss_value = tf.keras.losses.MeanSquaredError()(target,predictions).numpy()\n",
    "    loss_value = loss_fn(target, predictions).numpy()\n",
    "    valid_loss+= loss_value\n",
    "    c+=1\n",
    "\n",
    "print('validation loss', valid_loss/c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('test-files/saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "VurE2iDFVIK3",
    "outputId": "15f73d32-573a-489c-c765-5bb6483f445c"
   },
   "outputs": [],
   "source": [
    "#@title ### Sample Predictions v Targets Plot\n",
    "plt.plot(predictions.sample(1).numpy()[0,:,0],tf.reshape(target,(1,-1,2)).numpy()[0,:,0],'.')\n",
    "#target.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "id": "wu2nHUstKEqX",
    "outputId": "8ff6ee0c-0d84-49c5-da18-3c4c3f2fbc48"
   },
   "outputs": [],
   "source": [
    "#@title ### Predictions v Targets Mean\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(16, 8), facecolor='w', edgecolor='k')  \n",
    "\n",
    "axs = axs.ravel()\n",
    "for pred_i in range(2):\n",
    "    pred_vals = np.array([pp[:,:,pred_i] for pp in pred_list]).flatten()\n",
    "    true_vals = np.array([tt[:,:,pred_i] for tt in true_values]).flatten()\n",
    "\n",
    "    bin_means, bin_edges, binnumber = stats.binned_statistic(true_vals, pred_vals,bins=100,range=(-1,1))\n",
    "    bin_width = (bin_edges[1] - bin_edges[0])\n",
    "    bin_centers = bin_edges[1:] - bin_width/2\n",
    "\n",
    "    bin_stds, bin_edges, binnumber = stats.binned_statistic(true_vals, pred_vals,statistic='std',bins=100,range=(-1,1))\n",
    "\n",
    "\n",
    "    axs[pred_i].plot(bin_centers,bin_means,c='C0')\n",
    "\n",
    "    axs[pred_i].fill_between(bin_centers,bin_means-bin_stds,bin_means+bin_stds,color='C0',alpha=0.5)\n",
    "\n",
    "    xx = np.linspace(bin_edges.min(),bin_edges.max(),10)\n",
    "    axs[pred_i].plot(xx,xx,c='k',ls='--')\n",
    "\n",
    "    axs[pred_i].set_ylabel('GNN prediction of parameter')\n",
    "    axs[pred_i].set_xlabel('True parameter that generated the microstate')\n",
    "\n",
    "\n",
    "\n",
    "#plt.savefig('gnn_' + str(run) + '.png',dpi=300)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "7AawmXKLZEgJ"
   },
   "outputs": [],
   "source": [
    "#@title ### Rollout Functions\n",
    "\n",
    "INPUT_SEQUENCE_LENGTH=WINDOW_SIZE-1\n",
    "\n",
    "KINEMATIC_PARTICLE_ID = 3\n",
    "def get_kinematic_mask(particle_types):\n",
    "  \"\"\"Returns a boolean mask, set to true for kinematic (obstacle) particles.\"\"\"\n",
    "  return tf.equal(particle_types, KINEMATIC_PARTICLE_ID)\n",
    "\n",
    "def rollout(features, simulator, num_steps, N=N):\n",
    "  \"\"\"Rolls out a trajectory by applying the model in sequence.\"\"\"\n",
    "  \n",
    "  length = features['group_id']\n",
    "  initial_positions = features['pos'][tf.newaxis,0:INPUT_SEQUENCE_LENGTH]\n",
    "  initial_velocities = features['vel'][tf.newaxis,0:INPUT_SEQUENCE_LENGTH]\n",
    "  #initial_accelerations = features['acc'][tf.newaxis,0:INPUT_SEQUENCE_LENGTH]\n",
    "  ground_truth_positions = features['pos'][tf.newaxis,INPUT_SEQUENCE_LENGTH:]\n",
    "  ground_truth_velocities = features['vel'][tf.newaxis,INPUT_SEQUENCE_LENGTH:]\n",
    "  #ground_truth_accelerations = features['acc'][tf.newaxis,INPUT_SEQUENCE_LENGTH:]\n",
    "  global_context = None\n",
    "  \n",
    "  def step_fn(step, current_positions, current_velocities, pos_predictions, vel_predictions):\n",
    "    if global_context is None:\n",
    "      global_context_step = None\n",
    "    else:\n",
    "      global_context_step = global_context[\n",
    "          step + INPUT_SEQUENCE_LENGTH - 1][tf.newaxis]\n",
    "\n",
    "    next_velocity = simulator((current_positions, current_velocities, length),train_mode=False).sample(1)\n",
    "    #next_velocity = next_acceleration*dt+current_velocities[:,-1]\n",
    "    next_velocity /= tf.expand_dims(tf.norm(next_velocity, axis=-1),-1)\n",
    "    next_position = next_velocity*dt*vs+current_positions[:,-1]\n",
    "    next_position = tf.where(next_position<0, next_position+length, next_position)\n",
    "    next_position = tf.where(next_position>length, next_position-length, next_position)\n",
    "    #next_acceleration = (next_velocity - current_velocities[:,-1])/dt\n",
    "    # Update kinematic particles from prescribed trajectory.\n",
    "    kinematic_mask = get_kinematic_mask(tf.zeros(N))\n",
    "    next_position_ground_truth = ground_truth_positions[:,step,:,:]\n",
    "    next_velocity_ground_truth = ground_truth_velocities[:,step,:,:]\n",
    "    #next_acceleration_ground_truth = ground_truth_accelerations[:,step,:,:]\n",
    "    next_position = tf.where(tf.expand_dims(kinematic_mask,-1), next_position_ground_truth,\n",
    "                             next_position)\n",
    "    next_velocity = tf.where(tf.expand_dims(kinematic_mask,-1), next_velocity_ground_truth,\n",
    "                             next_velocity)\n",
    "    #next_acceleration = tf.where(tf.expand_dims(kinematic_mask,-1), next_acceleration_ground_truth,\n",
    "    #                         next_acceleration)\n",
    "    updated_pos_predictions = pos_predictions.write(step, next_position)\n",
    "    updated_vel_predictions = vel_predictions.write(step, next_velocity)\n",
    "    #updated_acc_predictions = acc_predictions.write(step, next_acceleration)\n",
    "    # Shift `current_positions`, removing the oldest position in the sequence\n",
    "    # and appending the next position at the end.\n",
    "    next_positions = tf.concat([current_positions[:,1:,:,:],\n",
    "                                next_position[:,tf.newaxis,:,:]], axis=1)\n",
    "    next_velocities = tf.concat([current_velocities[:,1:,:,:],\n",
    "                                next_velocity[:,tf.newaxis,:,:]], axis=1)\n",
    "    #next_accelerations = tf.concat([current_accelerations[:,1:,:,:],\n",
    "    #                            next_acceleration[:,tf.newaxis,:,:]], axis=1)\n",
    "  \n",
    "    return (step + 1, next_positions, next_velocities, updated_pos_predictions, updated_vel_predictions)\n",
    "  \n",
    "  pos_predictions = tf.TensorArray(size=num_steps, dtype=tf.float32)\n",
    "  vel_predictions = tf.TensorArray(size=num_steps, dtype=tf.float32)\n",
    "  #acc_predictions = tf.TensorArray(size=num_steps, dtype=tf.float32)\n",
    "\n",
    "  _, _, _, pos_predictions, vel_predictions = tf.while_loop(\n",
    "      cond=lambda step, state, vel, pos_prediction, vel_prediction: tf.less(step, num_steps),\n",
    "      body=step_fn,\n",
    "      loop_vars=(0, initial_positions, initial_velocities, pos_predictions, vel_predictions),\n",
    "      parallel_iterations=1)\n",
    "  \n",
    "  #pos_predictions = tf.transpose(pos_predictions, perm=[1,0,2,3])\n",
    "\n",
    "  output_dict = {\n",
    "      'initial_positions': initial_positions,\n",
    "      'initial_velocities': initial_velocities,\n",
    "      #'initial_accelerations': initial_accelerations,\n",
    "      'pos_predicted_rollout': pos_predictions.stack(),\n",
    "      'vel_predicted_rollout': vel_predictions.stack(),\n",
    "      #'acc_predicted_rollout': acc_predictions.stack(),\n",
    "      'ground_truth_position_rollout': ground_truth_positions,\n",
    "      'ground_truth_velocity_rollout': ground_truth_velocities,\n",
    "      #'ground_truth_acceleration_rollout': ground_truth_accelerations\n",
    "  }\n",
    "\n",
    "  \n",
    "  for field in ['pos_predicted_rollout','vel_predicted_rollout']:\n",
    "    output_dict[field]=tf.transpose(output_dict[field], perm=[1,0,2,3])\n",
    "\n",
    "\n",
    "  if global_context is not None:\n",
    "    output_dict['global_context'] = global_context\n",
    "  return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xd_ie1uYf8PZ",
    "outputId": "3a5aa0c2-8d5f-4b35-afe4-65781e904f1f"
   },
   "outputs": [],
   "source": [
    "#@title ### Generate Rollout Dataset\n",
    "\n",
    "evaluate_rollout_zonal_model(Num=N, Len=20, timesteps=timesteps, repeat=10, dir='rollout_datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OUUPqEPEbcWb"
   },
   "outputs": [],
   "source": [
    "#@title ### Generate Rollout\n",
    "rollout_dir='rollout_datasets/'\n",
    "rollout_dataset =  tf.data.TFRecordDataset(tf.data.Dataset.list_files([rollout_dir + filename for filename in os.listdir(rollout_dir)]))\n",
    "\n",
    "parsed_rollout_dataset = rollout_dataset.map(_parse_record)\n",
    "parsed_rollout_dataset = parsed_rollout_dataset.map(_parse_tensor)\n",
    "\n",
    "for databatch in parsed_rollout_dataset:\n",
    "    num_steps = databatch['pos'].shape[0]-INPUT_SEQUENCE_LENGTH\n",
    "    output = (rollout(databatch, model, num_steps, N),databatch['group_id'])\n",
    "    break\n",
    "\n",
    "# #output['metadata'] = metadata\n",
    "# filename = './zonal_model.pkl'\n",
    "# with open(filename, 'wb') as file:\n",
    "#     pickle.dump(output, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "kByzy_kDxSg1",
    "outputId": "a6316e7e-fbda-4c37-dc89-c63bbf108a6d"
   },
   "outputs": [],
   "source": [
    "#@title ### OP Plot\n",
    "\n",
    "pred_v = output[0]['vel_predicted_rollout'][0,...]\n",
    "true_v = output[0]['ground_truth_velocity_rollout'][0,...]\n",
    "\n",
    "pred_op = np.linalg.norm(np.mean(pred_v,axis=1),axis=-1)\n",
    "true_op = np.linalg.norm(np.mean(true_v,axis=1),axis=-1)\n",
    "\n",
    "plt.plot(true_op, label='truths')\n",
    "plt.plot(pred_op, label='predictions')\n",
    "plt.legend()\n",
    "\n",
    "# pred_av_velocity = np.mean(,axis=1)\n",
    "# true_av_velocity = np.mean(,axis=1)\n",
    "# pred_std = np.std(output['vel_predicted_rollout'][0,...],axis=1)\n",
    "# true_std = np.std(output['ground_truth_velocity_rollout'][0,...],axis=1)\n",
    "\n",
    "# pred_order_parameter = np.linalg.norm(pred_av_velocity,axis=1)/vs\n",
    "# true_order_parameter = np.linalg.norm(true_av_velocity,axis=1)/vs\n",
    "\n",
    "# line = np.linspace(0,max(pred_order_parameter),10)\n",
    "# #plt.scatter(true_order_parameter, pred_order_parameter)\n",
    "# #plt.plot(line, line)\n",
    "\n",
    "# order = np.argsort(true_order_parameter)\n",
    "# true_order_parameter = true_order_parameter[order]\n",
    "# pred_order_parameter = pred_order_parameter[order]\n",
    "\n",
    "# bin_means, bin_edges, binnumber = stats.binned_statistic(true_order_parameter, pred_order_parameter,bins=100)\n",
    "# bin_width = (bin_edges[1] - bin_edges[0])\n",
    "# bin_centers = bin_edges[1:] - bin_width/2\n",
    "\n",
    "# bin_stds, bin_edges, binnumber = stats.binned_statistic(true_order_parameter, pred_order_parameter,statistic='std',bins=100)\n",
    "# max_val=max([np.amax(pred_order_parameter), np.amax(true_order_parameter)])\n",
    "# xx = np.linspace(0,max_val,10)\n",
    "# plt.plot(bin_centers,bin_means,c='C0')\n",
    "# plt.plot(xx,xx,c='k',ls='--')\n",
    "# plt.xlabel(\"Ground Truth Order Parameter\")\n",
    "# plt.ylabel(\"Predicted Order Parameter\")\n",
    "# plt.fill_between(bin_centers,bin_means-bin_stds,bin_means+bin_stds,color='C0',alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LIz0eLBpyxXo",
    "outputId": "c9472540-1b66-4b25-904e-8f702acd1378"
   },
   "outputs": [],
   "source": [
    "#@title ### Avg OP Plot\n",
    "\n",
    "output_list = []\n",
    "counter = 0\n",
    "for databatch in tqdm(parsed_rollout_dataset):\n",
    "    num_steps = databatch['pos'].shape[0]-INPUT_SEQUENCE_LENGTH\n",
    "    output = (rollout(databatch,model,num_steps,N), databatch['group_id'])\n",
    "    output_list.append(output)\n",
    "    counter+=1\n",
    "\n",
    "pred_v_array = np.zeros((counter, num_steps, N, 2))\n",
    "true_v_array = np.zeros((counter, num_steps, N, 2))\n",
    "for i in range(counter):\n",
    "    pred_v_array[i]=output_list[i][0]['vel_predicted_rollout'][0,...]\n",
    "    true_v_array[i]=output_list[i][0]['ground_truth_velocity_rollout'][0,...]\n",
    "\n",
    "av_pred_v = np.mean(pred_v_array, axis=2)\n",
    "av_true_v = np.mean(true_v_array, axis=2)\n",
    "\n",
    "pred_op = np.linalg.norm(av_pred_v,axis=-1)\n",
    "true_op = np.linalg.norm(av_true_v,axis=-1)\n",
    "\n",
    "av_pred_op = np.mean(pred_op,axis=0)\n",
    "av_true_op = np.mean(true_op,axis=0)\n",
    "\n",
    "plt.plot(av_true_op, label='truths')\n",
    "plt.plot(av_pred_op, label='predictions')\n",
    "plt.xlabel('Timestep')\n",
    "plt.ylabel('Order Parameter')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "oXXw2vikafId"
   },
   "outputs": [],
   "source": [
    "#@title ### Efficient Avg OP Plot\n",
    "for databatch in parsed_rollout_dataset:\n",
    "    num_steps = databatch['pos'].shape[0]-INPUT_SEQUENCE_LENGTH\n",
    "    break\n",
    "rollout_func = functools.partial(rollout,\n",
    "                                 simulator=model,\n",
    "                                 num_steps=num_steps,\n",
    "                                 N=N\n",
    "                                 )\n",
    "\n",
    "counter = len(list(parsed_rollout_dataset))\n",
    "rollouts_dataset = parsed_rollout_dataset.apply(rollout_func)\n",
    "\n",
    "pred_v_array = np.zeros((counter, num_steps, N, 2))\n",
    "true_v_array = np.zeros((counter, num_steps, N, 2))\n",
    "for i in range(counter):\n",
    "    pred_v_array[i]=output_list[i]['vel_predicted_rollout'][0,...]\n",
    "    true_v_array[i]=output_list[i]['ground_truth_velocity_rollout'][0,...]\n",
    "\n",
    "av_pred_v = np.mean(pred_v_array, axis=2)\n",
    "av_true_v = np.mean(true_v_array, axis=2)\n",
    "\n",
    "pred_op = np.linalg.norm(av_pred_v,axis=-1)\n",
    "true_op = np.linalg.norm(av_true_v,axis=-1)\n",
    "\n",
    "av_pred_op = np.mean(pred_op,axis=0)\n",
    "av_true_op = np.mean(true_op,axis=0)\n",
    "\n",
    "plt.plot(av_true_op, label='truths')\n",
    "plt.plot(av_pred_op, label='predictions')\n",
    "plt.xlabel('Timestep')\n",
    "plt.ylabel('Order Parameter')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "background_save": true
    },
    "id": "9mUczFe6qHkC"
   },
   "outputs": [],
   "source": [
    "#@title ### Rollout Animation\n",
    "\n",
    "TYPE_TO_COLOR = {\n",
    "    3: \"black\",  # Boundary particles.\n",
    "    0: \"green\",  # Rigid solids.\n",
    "    7: \"magenta\",  # Goop.\n",
    "    6: \"gold\",  # Sand.\n",
    "    5: \"blue\",  # Water.\n",
    "}\n",
    "\n",
    "step_stride = 1\n",
    "block_on_show = True\n",
    "\n",
    "# with open(filename, \"rb\") as file:\n",
    "#   rollout_data = pickle.load(file)\n",
    "rollout_data, length = output\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "plot_info = []\n",
    "Q={}\n",
    "for ax_i, (label, (rollout_field_1, rollout_field_2)) in enumerate(\n",
    "    [(\"Ground truth\", (\"ground_truth_position_rollout\", \"ground_truth_velocity_rollout\")),\n",
    "      (\"Prediction\", (\"pos_predicted_rollout\", \"vel_predicted_rollout\"))]):\n",
    "  # Append the initial positions to get the full trajectory.\n",
    "  pos_trajectory = np.concatenate([\n",
    "      rollout_data[\"initial_positions\"][0,...],\n",
    "      rollout_data[rollout_field_1][0,...]], axis=0)\n",
    "  vel_trajectory = np.concatenate([\n",
    "      rollout_data[\"initial_velocities\"][0,...],\n",
    "      rollout_data[rollout_field_2][0,...]], axis=0)\n",
    "  ax = axes[ax_i]\n",
    "  ax.set_title(label)\n",
    "  bounds = [(0,length), (0,length)]\n",
    "  ax.set_xlim(bounds[0][0], bounds[0][1])\n",
    "  ax.set_ylim(bounds[1][0], bounds[1][1])\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "  ax.set_aspect(1.)\n",
    "  points = {\n",
    "      particle_type: ax.plot([], [], \"o\", ms=2, color=color)[0]\n",
    "      for particle_type, color in TYPE_TO_COLOR.items()}\n",
    "  X, Y, U, V = pos_trajectory[0,:,0], pos_trajectory[0,:,1], vel_trajectory[0,:,0], vel_trajectory[0,:,1]\n",
    "  Q[f'{ax_i}'] = ax.quiver(X, Y, U, V, pivot='mid', color='b', units='inches')\n",
    "  plot_info.append((ax, pos_trajectory, vel_trajectory, points))\n",
    "\n",
    "\n",
    "num_steps = pos_trajectory.shape[0]\n",
    "#print(pos_trajectory.shape)\n",
    "\n",
    "def update_quiver(num):\n",
    "  for i, (ax, pos_trajectory, vel_trajectory, _) in enumerate(plot_info):\n",
    "    U, V = vel_trajectory[num,:,0], vel_trajectory[num,:,1]\n",
    "    offsets = pos_trajectory[num]\n",
    "\n",
    "    Q[f'{i}'].set_offsets(offsets)\n",
    "    Q[f'{i}'].set_UVC(U,V)\n",
    "  return axes\n",
    "\n",
    "anim = animation.FuncAnimation(fig, update_quiver,\n",
    "                               interval=50, frames=np.arange(0, num_steps, step_stride),blit=False)\n",
    "#plt.show(block=block_on_show)\n",
    "rc('animation', html='jshtml')\n",
    "anim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "AC5l9RH_vLG4"
   },
   "outputs": [],
   "source": [
    "#@title ### Single Animation\n",
    "\n",
    "_, allpos, allvel, _ = plot_info[1]\n",
    "#allvel = simulation_cls.micro_state[0,:,:,2:]\n",
    "        \n",
    "X, Y = allpos[0,:,0],allpos[0,:,1]\n",
    "U, V = allvel[0,:,0],allvel[0,:,1]\n",
    "skip=50\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,8))\n",
    "plt.close()\n",
    "Q = ax.quiver(X, Y, U, V, pivot='mid', color='b', units='inches')\n",
    "ax.axis('equal')\n",
    "ax.set_xlim(0,length)\n",
    "ax.set_ylim(0,length)\n",
    "\n",
    "def update_quiver(num, Q, X, Y):\n",
    "    \"\"\"updates the horizontal and vertical vector components by a\n",
    "    fixed increment on each frame\n",
    "    \"\"\"\n",
    "    U, V = allvel[num,:,0],allvel[num,:,1]\n",
    "    offsets = allpos[num]\n",
    "\n",
    "    Q.set_offsets(offsets)\n",
    "    Q.set_UVC(U,V)\n",
    "\n",
    "    return Q,\n",
    "\n",
    "# you need to set blit=False, or the first set of arrows never gets\n",
    "# cleared on subsequent frames\n",
    "anim = animation.FuncAnimation(fig, update_quiver, fargs=(Q, X, Y),\n",
    "                               interval=50, frames=num_steps,blit=False)\n",
    "\n",
    "rc('animation', html='jshtml')\n",
    "anim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVW_a6JC30dh"
   },
   "source": [
    "# Scaled Rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "gQZ7jmH_24-D"
   },
   "outputs": [],
   "source": [
    "#@title ### Scaled Zonal Model { form-width: \"30%\" }\n",
    "\n",
    "def get_record(group_id,pos,vel):\n",
    "    feature = { 'group_id': tf.train.Feature(float_list=tf.train.FloatList(value=[group_id])),\n",
    "                'pos': tf.train.Feature(bytes_list=tf.train.BytesList(value=[pos.numpy()])),\n",
    "                'vel': tf.train.Feature(bytes_list=tf.train.BytesList(value=[vel.numpy()])),\n",
    "                #'acc': tf.train.Feature(bytes_list=tf.train.BytesList(value=[acc.numpy()]))\n",
    "                }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "\n",
    "class scaled_zonal_model:\n",
    "    def __init__(self, N, timesteps, discard, repeat, L, dt, save_interval, scaled_directory='scaled_datasets', disable_progress=False):\n",
    "        self.N = N\n",
    "        self.timesteps = timesteps\n",
    "        self.discard = discard\n",
    "        self.B = repeat  # repeat for B batches\n",
    "        self.L = L\n",
    "        self.dt = dt\n",
    "        self.save_interval = save_interval\n",
    "        \n",
    "        self.micro_state = np.zeros((self.B, (self.timesteps - self.discard)//self.save_interval, N, 4),dtype=np.float32)\n",
    "\n",
    "        self.sim_counter=0\n",
    "\n",
    "        if not os.path.exists(scaled_directory):\n",
    "            os.makedirs(scaled_directory)\n",
    "\n",
    "        self.scaled_directory = scaled_directory\n",
    "\n",
    "        # turn progress bar on or off\n",
    "        self.disable_progress = disable_progress\n",
    "\n",
    "    def initialise_state(self):\n",
    "\n",
    "        #self.positions = tf.random.uniform((self.B,self.N,2),0.5*self.L, 0.5*self.L+20) #0,self.L)\n",
    "        self.positions = tf.random.uniform((self.B,self.N,2),0, self.L) \n",
    "        self.angles = tf.random.uniform((self.B,self.N,1), 0, 2*pi) #\n",
    "        \n",
    "\n",
    "\n",
    "    def run_sim(self, *params):\n",
    "\n",
    "        eta, Ra, Ro, Rr, vs, va, sigma = params\n",
    "        \n",
    "        record_file = self.scaled_directory + '/microstates-' + str(self.sim_counter) + '.tfrecords'\n",
    "        self.writer = tf.io.TFRecordWriter(record_file) \n",
    "        \n",
    "        # tensorflow function to run an update step\n",
    "        @tf.function\n",
    "        def update_tf(X, A):\n",
    "            cos_A = tf.math.cos(A)\n",
    "            sin_A = tf.math.sin(A)\n",
    "\n",
    "\n",
    "            Xx = tf.expand_dims(X[...,0],-1)\n",
    "            dx = -Xx + tf.linalg.matrix_transpose(Xx)\n",
    "            dx = tf.where(dx>0.5*self.L, dx-self.L, dx)\n",
    "            dx = tf.where(dx<-0.5*self.L, dx+self.L, dx)\n",
    "\n",
    "            Xy = tf.expand_dims(X[...,1],-1)\n",
    "            dy = -Xy + tf.linalg.matrix_transpose(Xy)\n",
    "            dy = tf.where(dy>0.5*self.L, dy-self.L, dy)\n",
    "            dy = tf.where(dy<-0.5*self.L, dy+self.L, dy)\n",
    "\n",
    "\n",
    "            angle_to_neigh = tf.math.atan2(dy, dx)\n",
    "            cos_N = tf.math.cos(angle_to_neigh)\n",
    "            sin_N = tf.math.sin(angle_to_neigh)\n",
    "            rel_angle_to_neigh = angle_to_neigh - A\n",
    "            rel_angle_to_neigh = tf.math.atan2(tf.math.sin(rel_angle_to_neigh), tf.math.cos(rel_angle_to_neigh))\n",
    "            \n",
    "            dist = tf.math.sqrt(tf.square(dx)+tf.square(dy))\n",
    "    \n",
    "            # repulsion \n",
    "            rep_x = tf.where(dist<=Rr, -dx, tf.zeros_like(dx))\n",
    "            rep_x = tf.where(rel_angle_to_neigh<0.5*va, rep_x, tf.zeros_like(rep_x))\n",
    "            rep_x = tf.where(rel_angle_to_neigh>-0.5*va, rep_x, tf.zeros_like(rep_x))\n",
    "            rep_x = tf.math.divide_no_nan(rep_x,tf.math.square(dist))\n",
    "            rep_x = tf.reduce_sum(rep_x,axis=2)\n",
    "\n",
    "            rep_y = tf.where(dist<=Rr, -dy, tf.zeros_like(dy))\n",
    "            rep_y = tf.where(rel_angle_to_neigh<0.5*va, rep_y, tf.zeros_like(rep_y))\n",
    "            rep_y = tf.where(rel_angle_to_neigh>-0.5*va, rep_y, tf.zeros_like(rep_y))\n",
    "            rep_y = tf.math.divide_no_nan(rep_y,tf.math.square(dist))\n",
    "            rep_y = tf.reduce_sum(rep_y,axis=2)\n",
    "\n",
    "            # alignment \n",
    "            align_x = tf.where(dist<=Ro, cos_A, tf.zeros_like(cos_A))\n",
    "            align_x = tf.where(rel_angle_to_neigh<0.5*va, align_x, tf.zeros_like(align_x))\n",
    "            align_x = tf.where(rel_angle_to_neigh>-0.5*va, align_x, tf.zeros_like(align_x))\n",
    "            align_x = tf.reduce_sum(align_x,axis=1)\n",
    "            \n",
    "            align_y = tf.where(dist<=Ro, sin_A, tf.zeros_like(sin_A))\n",
    "            align_y = tf.where(rel_angle_to_neigh<0.5*va, align_y, tf.zeros_like(align_y))\n",
    "            align_y = tf.where(rel_angle_to_neigh>-0.5*va, align_y, tf.zeros_like(align_y))\n",
    "            align_y = tf.reduce_sum(align_y,axis=1)\n",
    "\n",
    "            al_norm = tf.math.sqrt(align_x**2+align_y**2)\n",
    "            align_x = tf.math.divide_no_nan(align_x,al_norm)\n",
    "            align_y = tf.math.divide_no_nan(align_y,al_norm)\n",
    "\n",
    "            # attractive interactions\n",
    "            attr_x = tf.where(dist<=Ra, dx, tf.zeros_like(dx))\n",
    "            attr_x = tf.where(rel_angle_to_neigh<0.5*va, attr_x, tf.zeros_like(attr_x))\n",
    "            attr_x = tf.where(rel_angle_to_neigh>-0.5*va, attr_x, tf.zeros_like(attr_x))\n",
    "            attr_x = tf.reduce_sum(attr_x,axis=2)\n",
    "\n",
    "            attr_y = tf.where(dist<=Ra, dy, tf.zeros_like(dy))\n",
    "            attr_y = tf.where(rel_angle_to_neigh<0.5*va, attr_y, tf.zeros_like(attr_y))\n",
    "            attr_y = tf.where(rel_angle_to_neigh>-0.5*va, attr_y, tf.zeros_like(attr_y))\n",
    "            attr_y = tf.reduce_sum(attr_y,axis=2)\n",
    "\n",
    "            at_norm = tf.math.sqrt(attr_x**2+attr_y**2)\n",
    "            attr_x = tf.math.divide_no_nan(attr_x,at_norm)\n",
    "            attr_y = tf.math.divide_no_nan(attr_y,at_norm)\n",
    "\n",
    "            # combine angles and convert to desired angle change\n",
    "            social_x = rep_x + align_x + attr_x\n",
    "            social_y = rep_y + align_y + attr_y\n",
    "\n",
    "            d_angle = tf.math.atan2(social_y,social_x)\n",
    "            d_angle = tf.expand_dims(d_angle,-1)\n",
    "\n",
    "            \n",
    "            d_angle = tf.math.atan2((1-eta)*tf.math.sin(d_angle) + eta*sin_A, (1-eta)*tf.math.cos(d_angle) + eta*cos_A)\n",
    "\n",
    "            d_angle = d_angle - A\n",
    "            d_angle = tf.where(d_angle>pi, d_angle-2*pi, d_angle)\n",
    "            d_angle = tf.where(d_angle<-pi, d_angle+2*pi, d_angle)\n",
    "\n",
    "\n",
    "            # add perception noise\n",
    "            noise = tf.random.normal(shape=(self.B,self.N,1),mean=0,stddev=sigma*(self.dt**0.5))\n",
    "            d_angle = d_angle + noise\n",
    "            \n",
    "            # restrict to maximum turning angle\n",
    "            #d_angle = tf.where(tf.math.abs(d_angle)>eta*self.dt, tf.math.sign(d_angle)*eta*self.dt, d_angle)\n",
    "            \n",
    "            # rotate headings\n",
    "            A = A + d_angle\n",
    "            \n",
    "            # update positions\n",
    "            velocity = self.dt*vs*tf.concat([tf.cos(A),tf.sin(A)],axis=-1)\n",
    "            X += velocity\n",
    "\n",
    "            # add periodic boundary conditions\n",
    "            A = tf.where(A<-pi,  A+2*pi, A)\n",
    "            A = tf.where(A>pi, A-2*pi, A)\n",
    "\n",
    "            X = tf.where(X>self.L, X-self.L, X)\n",
    "            X = tf.where(X<0, X+self.L, X)\n",
    "\n",
    "            X = tf.where(X>self.L, X-self.L, X)\n",
    "            X = tf.where(X<0, X+self.L, X)\n",
    "\n",
    "            return X, A\n",
    "            \n",
    "        self.initialise_state()\n",
    "\n",
    "        counter=0\n",
    "        for i in tqdm(range(self.timesteps),disable=self.disable_progress):\n",
    "            self.positions, self.angles = update_tf(self.positions,  self.angles)\n",
    "            if i>=self.discard:\n",
    "                if i%self.save_interval==0:\n",
    "                    # store in an array in case we want to visualise\n",
    "                    self.micro_state[:,counter,:,0:2] = self.positions.numpy()\n",
    "                    self.micro_state[:,counter,:,2:3] = np.cos(self.angles.numpy())\n",
    "                    self.micro_state[:,counter,:,3:4] = np.sin(self.angles.numpy())\n",
    "                    \n",
    "                        \n",
    "                    \n",
    "\n",
    "                    counter = counter + 1\n",
    "\n",
    "        for b in range(self.B):\n",
    "            self.save_tf_record(b,self.L)\n",
    "\n",
    "        self.writer.close()\n",
    "        self.sim_counter+=1\n",
    "        return \n",
    "\n",
    "    def save_tf_record(self, b, L):\n",
    "        pos =  tf.io.serialize_tensor(self.micro_state[b,:,:,0:2])\n",
    "        vel =  tf.io.serialize_tensor(self.micro_state[b,:,:,2:4])\n",
    "        #acc =  tf.io.serialize_tensor(np.gradient(self.micro_state[b,:,:,2:4], axis=0))\n",
    "\n",
    "        tf_record = get_record(L,pos,vel)\n",
    "        self.writer.write(tf_record.SerializeToString())\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FF4B67Eo3eYK",
    "outputId": "b2bc55ea-c723-488b-9189-d1561fb346c4"
   },
   "outputs": [],
   "source": [
    "#@title ### Generate Scaled Data\n",
    "\n",
    "scaling_factor=5\n",
    "scaled_timesteps=500\n",
    "\n",
    "def evaluate_rollout_zonal_model(Num=N, Len=L, timesteps=timesteps, discard=discard, repeat=repeat, dir='scaled_datasets', dp=False):\n",
    "    sim = scaled_zonal_model(Num,timesteps=timesteps+discard,discard=discard,L=Len,repeat=repeat, dt=dt,save_interval=save_interval,scaled_directory=dir,disable_progress=dp)\n",
    "    sim.run_sim(eta, latt, lali, lrep, vs, va, sigma)\n",
    "    return\n",
    "\n",
    "evaluate_rollout_zonal_model(Num=scaling_factor*N, Len=20, timesteps=scaled_timesteps, repeat=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-IS4Syl-2nyO"
   },
   "outputs": [],
   "source": [
    "#@title ### Generate Scaled Rollout\n",
    "\n",
    "scaled_dir = 'scaled_datasets/'\n",
    "scaled_dataset =  tf.data.TFRecordDataset(tf.data.Dataset.list_files([scaled_dir + filename for filename in os.listdir(scaled_dir)]))\n",
    "\n",
    "parsed_scaled_rollout_dataset = scaled_dataset.map(_parse_record)\n",
    "parsed_scaled_rollout_dataset = parsed_scaled_rollout_dataset.map(_parse_tensor)\n",
    "\n",
    "for databatch in parsed_scaled_rollout_dataset:\n",
    "    num_steps = databatch['pos'].shape[0]-INPUT_SEQUENCE_LENGTH\n",
    "    scaled_output = (rollout(databatch,model,num_steps,scaling_factor*N),databatch['group_id'])\n",
    "    break\n",
    "\n",
    "#output['metadata'] = metadata\n",
    "# filename = './zonal_model.pkl'\n",
    "# with open(filename, 'wb') as file:\n",
    "#     pickle.dump(output, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "rDtmMbWm9hZt",
    "outputId": "aa482731-a2b1-44ef-f757-ed55592f794d"
   },
   "outputs": [],
   "source": [
    "#@title ### OP Plot\n",
    "\n",
    "scaled_pred_v = scaled_output[0]['vel_predicted_rollout'][0,...]\n",
    "scaled_true_v = scaled_output[0]['ground_truth_velocity_rollout'][0,...]\n",
    "\n",
    "scaled_pred_op = np.linalg.norm(np.mean(scaled_pred_v,axis=1),axis=-1)\n",
    "scaled_true_op = np.linalg.norm(np.mean(scaled_true_v,axis=1),axis=-1)\n",
    "\n",
    "plt.plot(scaled_true_op, label='truths')\n",
    "plt.plot(scaled_pred_op, label='predictions')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "fx03KdQi9-tm",
    "outputId": "5b7d29f6-1cd4-4fec-8885-4213db345022"
   },
   "outputs": [],
   "source": [
    "#@title ### Scaled Avg OP Plot\n",
    "\n",
    "output_list = []\n",
    "counter = 0\n",
    "for databatch in tqdm(parsed_scaled_rollout_dataset):\n",
    "    num_steps = databatch['pos'].shape[0]-INPUT_SEQUENCE_LENGTH\n",
    "    output = (rollout(databatch,model,num_steps,scaling_factor*N), databatch['group_id'])\n",
    "    output_list.append(output)\n",
    "    counter+=1\n",
    "\n",
    "pred_v_array = np.zeros((counter, num_steps, scaling_factor*N, 2))\n",
    "true_v_array = np.zeros((counter, num_steps, scaling_factor*N, 2))\n",
    "for i in range(counter):\n",
    "    pred_v_array[i]=output_list[i][0]['vel_predicted_rollout'][0,...]\n",
    "    true_v_array[i]=output_list[i][0]['ground_truth_velocity_rollout'][0,...]\n",
    "\n",
    "av_pred_v = np.mean(pred_v_array, axis=2)\n",
    "av_true_v = np.mean(true_v_array, axis=2)\n",
    "\n",
    "pred_op = np.linalg.norm(av_pred_v,axis=-1)\n",
    "true_op = np.linalg.norm(av_true_v,axis=-1)\n",
    "\n",
    "av_pred_op = np.mean(pred_op,axis=0)\n",
    "av_true_op = np.mean(true_op,axis=0)\n",
    "\n",
    "plt.plot(av_true_op, label='truths')\n",
    "plt.plot(av_pred_op, label='predictions')\n",
    "plt.xlabel('Timestep')\n",
    "plt.ylabel('Order Parameter')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 777
    },
    "id": "JmHWPw9G-6ZJ",
    "outputId": "691f8404-5e72-4341-96f2-0cab52258c49"
   },
   "outputs": [],
   "source": [
    "#@title ### Rollout Animation\n",
    "\n",
    "TYPE_TO_COLOR = {\n",
    "    3: \"black\",  # Boundary particles.\n",
    "    0: \"green\",  # Rigid solids.\n",
    "    7: \"magenta\",  # Goop.\n",
    "    6: \"gold\",  # Sand.\n",
    "    5: \"blue\",  # Water.\n",
    "}\n",
    "\n",
    "step_stride = 1\n",
    "block_on_show = True\n",
    "\n",
    "# with open(filename, \"rb\") as file:\n",
    "#   rollout_data = pickle.load(file)\n",
    "rollout_data, length = scaled_output\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "plot_info = []\n",
    "Q={}\n",
    "for ax_i, (label, (rollout_field_1, rollout_field_2)) in enumerate(\n",
    "    [(\"Ground truth\", (\"ground_truth_position_rollout\", \"ground_truth_velocity_rollout\")),\n",
    "      (\"Prediction\", (\"pos_predicted_rollout\", \"vel_predicted_rollout\"))]):\n",
    "  # Append the initial positions to get the full trajectory.\n",
    "  pos_trajectory = np.concatenate([\n",
    "      scaled_output[0][\"initial_positions\"][0,...],\n",
    "      scaled_output[0][rollout_field_1][0,...]], axis=0)\n",
    "  vel_trajectory = np.concatenate([\n",
    "      scaled_output[0][\"initial_velocities\"][0,...],\n",
    "      scaled_output[0][rollout_field_2][0,...]], axis=0)\n",
    "  ax = axes[ax_i]\n",
    "  ax.set_title(label)\n",
    "  bounds = [(0,length), (0,length)]\n",
    "  ax.set_xlim(bounds[0][0], bounds[0][1])\n",
    "  ax.set_ylim(bounds[1][0], bounds[1][1])\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "  ax.set_aspect(1.)\n",
    "  points = {\n",
    "      particle_type: ax.plot([], [], \"o\", ms=2, color=color)[0]\n",
    "      for particle_type, color in TYPE_TO_COLOR.items()}\n",
    "  X, Y, U, V = pos_trajectory[0,:,0], pos_trajectory[0,:,1], vel_trajectory[0,:,0], vel_trajectory[0,:,1]\n",
    "  Q[f'{ax_i}'] = ax.quiver(X, Y, U, V, pivot='mid', color='b', units='inches')\n",
    "  plot_info.append((ax, pos_trajectory, vel_trajectory, points))\n",
    "\n",
    "\n",
    "num_steps = pos_trajectory.shape[0]\n",
    "#print(pos_trajectory.shape)\n",
    "\n",
    "def update_quiver(num):\n",
    "  for i, (ax, pos_trajectory, vel_trajectory, _) in enumerate(plot_info):\n",
    "    U, V = vel_trajectory[num,:,0], vel_trajectory[num,:,1]\n",
    "    offsets = pos_trajectory[num]\n",
    "\n",
    "    Q[f'{i}'].set_offsets(offsets)\n",
    "    Q[f'{i}'].set_UVC(U,V)\n",
    "  return axes\n",
    "\n",
    "anim = animation.FuncAnimation(fig, update_quiver,\n",
    "                               interval=50, frames=np.arange(0, num_steps, step_stride),blit=False)\n",
    "#plt.show(block=block_on_show)\n",
    "rc('animation', html='jshtml')\n",
    "anim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#@title ### Phase Transition\n",
    "\n",
    "Num=500\n",
    "#Len=10\n",
    "ts=1000\n",
    "disc=500\n",
    "rep=5\n",
    "\n",
    "num_points=25\n",
    "l_low=5\n",
    "l_high=60\n",
    "l_list=np.logspace(np.log10(l_low),np.log10(l_high),num_points)\n",
    "truths = np.zeros(num_points)\n",
    "preds = np.zeros(num_points)\n",
    "pbar = tqdm(enumerate(l_list),total=num_points,desc='Overall Progress')\n",
    "for step, l in pbar:\n",
    "    pbar.set_description(f'Step:{step}, Length:{l}')\n",
    "    transition_dir = 'transition_datasets/'\n",
    "    evaluate_rollout_zonal_model(Num=Num, Len=l, timesteps=ts, discard=disc, repeat=rep, dir=transition_dir, dp=True)\n",
    "    transition_dataset =  tf.data.TFRecordDataset(tf.data.Dataset.list_files([transition_dir + filename for filename in os.listdir(transition_dir)]))\n",
    "\n",
    "    parsed_transition_dataset = transition_dataset.map(_parse_record)\n",
    "    parsed_transition_dataset = parsed_transition_dataset.map(_parse_tensor)\n",
    "    output_list = []\n",
    "    #inner_bar = tqdm(range(rep),total=rep,desc=f'Step {step} Progress', leave = False)\n",
    "    for databatch in parsed_transition_dataset:\n",
    "        #inner_bar.update(1)\n",
    "        num_steps = databatch['pos'].shape[0]-INPUT_SEQUENCE_LENGTH\n",
    "        output = (rollout(databatch,model,num_steps,Num), databatch['group_id'])\n",
    "        output_list.append(output)\n",
    "    #inner_bar.reset()\n",
    "    pred_v_array = np.zeros((rep, num_steps, Num, 2))\n",
    "    true_v_array = np.zeros((rep, num_steps, Num, 2))\n",
    "    for i in range(rep):\n",
    "        pred_v_array[i]=output_list[i][0]['vel_predicted_rollout'][0,...]\n",
    "        true_v_array[i]=output_list[i][0]['ground_truth_velocity_rollout'][0,...]\n",
    "    av_pred_v = np.mean(pred_v_array, axis=2)\n",
    "    av_true_v = np.mean(true_v_array, axis=2)\n",
    "\n",
    "    pred_op = np.linalg.norm(av_pred_v,axis=-1)\n",
    "    true_op = np.linalg.norm(av_true_v,axis=-1)\n",
    "\n",
    "    av_pred_op = np.mean(pred_op)\n",
    "    av_true_op = np.mean(true_op)\n",
    "    \n",
    "    preds[step]=av_pred_op\n",
    "    truths[step]=av_true_op\n",
    "    print(f'Prediction: {av_pred_op}, Truth: {av_true_op}')\n",
    "\n",
    "plt.scatter(l_list, preds)\n",
    "plt.scatter(l_list, truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(l_list, preds, label='predictions')\n",
    "plt.scatter(l_list, truths, label='truths')\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Order Parameter')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(l_list, preds, label='predictions')\n",
    "plt.plot(l_list, truths, label='truths')\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Order Parameter')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade tqdm"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ProbSpektralModel.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
