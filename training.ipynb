{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "993a70ad-9bef-468d-99b1-c0d2aaa0d57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-12 17:34:20.315064: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import learned_simulator\n",
    "import noise_utils\n",
    "import reading_utils\n",
    "\n",
    "import collections\n",
    "import functools\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import tree\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "216a8b27-c8e4-4731-ba58-867a1cb99aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-10-12 17:07:14--  https://storage.googleapis.com/learning-to-simulate-complex-physics/Datasets/WaterDrop/metadata.json\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.200.16, 142.250.200.48, 172.217.16.240, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.200.16|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 361 [application/octet-stream]\n",
      "Saving to: ‘/tmp/datasets/metadata.json’\n",
      "\n",
      "/tmp/datasets/metad 100%[===================>]     361  --.-KB/s    in 0s      \n",
      "\n",
      "2021-10-12 17:07:14 (216 MB/s) - ‘/tmp/datasets/metadata.json’ saved [361/361]\n",
      "\n",
      "--2021-10-12 17:07:15--  https://storage.googleapis.com/learning-to-simulate-complex-physics/Datasets/WaterDrop/train.tfrecord\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.200.16, 142.250.200.48, 172.217.16.240, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.200.16|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4541246980 (4.2G) [application/octet-stream]\n",
      "Saving to: ‘/tmp/datasets/train.tfrecord’\n",
      "\n",
      "/tmp/datasets/train 100%[===================>]   4.23G  66.0MB/s    in 74s     \n",
      "\n",
      "2021-10-12 17:08:30 (58.2 MB/s) - ‘/tmp/datasets/train.tfrecord’ saved [4541246980/4541246980]\n",
      "\n",
      "--2021-10-12 17:08:30--  https://storage.googleapis.com/learning-to-simulate-complex-physics/Datasets/WaterDrop/valid.tfrecord\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.200.16, 142.250.200.48, 172.217.16.240, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.200.16|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 137190408 (131M) [application/octet-stream]\n",
      "Saving to: ‘/tmp/datasets/valid.tfrecord’\n",
      "\n",
      "/tmp/datasets/valid 100%[===================>] 130.83M  67.7MB/s    in 1.9s    \n",
      "\n",
      "2021-10-12 17:08:32 (67.7 MB/s) - ‘/tmp/datasets/valid.tfrecord’ saved [137190408/137190408]\n",
      "\n",
      "--2021-10-12 17:08:33--  https://storage.googleapis.com/learning-to-simulate-complex-physics/Datasets/WaterDrop/test.tfrecord\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.200.16, 142.250.200.48, 172.217.16.240, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.200.16|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 136885800 (131M) [application/octet-stream]\n",
      "Saving to: ‘/tmp/datasets/test.tfrecord’\n",
      "\n",
      "/tmp/datasets/test. 100%[===================>] 130.54M  62.4MB/s    in 2.1s    \n",
      "\n",
      "2021-10-12 17:08:35 (62.4 MB/s) - ‘/tmp/datasets/test.tfrecord’ saved [136885800/136885800]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "!mkdir -p '/tmp/datasets'\n",
    "!wget -O /tmp/datasets/metadata.json https://storage.googleapis.com/learning-to-simulate-complex-physics/Datasets/WaterDrop/metadata.json\n",
    "!wget -O /tmp/datasets/train.tfrecord https://storage.googleapis.com/learning-to-simulate-complex-physics/Datasets/WaterDrop/train.tfrecord\n",
    "!wget -O /tmp/datasets/valid.tfrecord https://storage.googleapis.com/learning-to-simulate-complex-physics/Datasets/WaterDrop/valid.tfrecord\n",
    "!wget -O /tmp/datasets/test.tfrecord https://storage.googleapis.com/learning-to-simulate-complex-physics/Datasets/WaterDrop/test.tfrecord\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c08155e-e9f2-4264-aa46-2569938b8f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "INPUT_SEQUENCE_LENGTH = 6  # So we can calculate the last 5 velocities.\n",
    "NUM_PARTICLE_TYPES = 9\n",
    "KINEMATIC_PARTICLE_ID = 3\n",
    "\n",
    "batch_size=2\n",
    "\n",
    "def _read_metadata(data_path):\n",
    "    with open(os.path.join(data_path, 'metadata.json'), 'rt') as fp:\n",
    "        return json.loads(fp.read())\n",
    "\n",
    "def prepare_inputs(tensor_dict):\n",
    "  \"\"\"Prepares a single stack of inputs by calculating inputs and targets.\n",
    "\n",
    "  Computes n_particles_per_example, which is a tensor that contains information\n",
    "  about how to partition the axis - i.e. which nodes belong to which graph.\n",
    "\n",
    "  Adds a batch axis to `n_particles_per_example` and `step_context` so they can\n",
    "  later be batched using `batch_concat`. This batch will be the same as if the\n",
    "  elements had been batched via stacking.\n",
    "\n",
    "  Note that all other tensors have a variable size particle axis,\n",
    "  and in this case they will simply be concatenated along that\n",
    "  axis.\n",
    "\n",
    "\n",
    "\n",
    "  Args:\n",
    "    tensor_dict: A dict of tensors containing positions, and step context (\n",
    "    if available).\n",
    "\n",
    "  Returns:\n",
    "    A tuple of input features and target positions.\n",
    "\n",
    "  \"\"\"\n",
    "  # Position is encoded as [sequence_length, num_particles, dim] but the model\n",
    "  # expects [num_particles, sequence_length, dim].\n",
    "  pos = tensor_dict['position']\n",
    "  pos = tf.transpose(pos, perm=[1, 0, 2])\n",
    "\n",
    "  # The target position is the final step of the stack of positions.\n",
    "  target_position = pos[:, -1]\n",
    "\n",
    "  # Remove the target from the input.\n",
    "  tensor_dict['position'] = pos[:, :-1]\n",
    "\n",
    "  # Compute the number of particles per example.\n",
    "  num_particles = tf.shape(pos)[0]\n",
    "  # Add an extra dimension for stacking via concat.\n",
    "  tensor_dict['n_particles_per_example'] = num_particles[tf.newaxis]\n",
    "\n",
    "  if 'step_context' in tensor_dict:\n",
    "    # Take the input global context. We have a stack of global contexts,\n",
    "    # and we take the penultimate since the final is the target.\n",
    "    tensor_dict['step_context'] = tensor_dict['step_context'][-2]\n",
    "    # Add an extra dimension for stacking via concat.\n",
    "    tensor_dict['step_context'] = tensor_dict['step_context'][tf.newaxis]\n",
    "  return tensor_dict, target_position\n",
    "\n",
    "def batch_concat(dataset, batch_size):\n",
    "  \"\"\"We implement batching as concatenating on the leading axis.\"\"\"\n",
    "\n",
    "  # We create a dataset of datasets of length batch_size.\n",
    "  windowed_ds = dataset.window(batch_size)\n",
    "\n",
    "  # The plan is then to reduce every nested dataset by concatenating. We can\n",
    "  # do this using tf.data.Dataset.reduce. This requires an initial state, and\n",
    "  # then incrementally reduces by running through the dataset\n",
    "\n",
    "  # Get initial state. In this case this will be empty tensors of the\n",
    "  # correct shape.\n",
    "  initial_state = tree.map_structure(\n",
    "      lambda spec: tf.zeros(  # pylint: disable=g-long-lambda\n",
    "          shape=[0] + spec.shape.as_list()[1:], dtype=spec.dtype),\n",
    "      dataset.element_spec)\n",
    "\n",
    "  # We run through the nest and concatenate each entry with the previous state.\n",
    "  def reduce_window(initial_state, ds):\n",
    "    return ds.reduce(initial_state, lambda x, y: tf.concat([x, y], axis=0))\n",
    "\n",
    "  return windowed_ds.map(\n",
    "      lambda *x: tree.map_structure(reduce_window, initial_state, x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dca61597-9862-4d9f-a0f7-218538136fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-12 17:34:25.924826: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-10-12 17:34:25.927156: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-10-12 17:34:25.978632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-12 17:34:25.979165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: Quadro P1000 computeCapability: 6.1\n",
      "coreClock: 1.4805GHz coreCount: 5 deviceMemorySize: 3.92GiB deviceMemoryBandwidth: 74.65GiB/s\n",
      "2021-10-12 17:34:25.979200: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-10-12 17:34:25.982095: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2021-10-12 17:34:25.982132: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2021-10-12 17:34:25.983635: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-10-12 17:34:25.984319: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-10-12 17:34:25.985819: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-10-12 17:34:25.986926: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-10-12 17:34:25.989523: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-10-12 17:34:25.989611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-12 17:34:25.989850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-12 17:34:25.990009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-10-12 17:34:25.990351: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-12 17:34:25.990932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-12 17:34:25.991152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: Quadro P1000 computeCapability: 6.1\n",
      "coreClock: 1.4805GHz coreCount: 5 deviceMemorySize: 3.92GiB deviceMemoryBandwidth: 74.65GiB/s\n",
      "2021-10-12 17:34:25.991163: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-10-12 17:34:25.991176: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2021-10-12 17:34:25.991185: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2021-10-12 17:34:25.991194: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-10-12 17:34:25.991203: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-10-12 17:34:25.991212: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-10-12 17:34:25.991221: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-10-12 17:34:25.991231: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-10-12 17:34:25.991263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-12 17:34:25.991446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-12 17:34:25.991602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-10-12 17:34:25.991621: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-10-12 17:34:26.284479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-10-12 17:34:26.284499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2021-10-12 17:34:26.284503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2021-10-12 17:34:26.284648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-12 17:34:26.284863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-12 17:34:26.285100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-12 17:34:26.285269: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2659 MB memory) -> physical GPU (device: 0, name: Quadro P1000, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "2021-10-12 17:34:26.285399: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function prepare_inputs at 0x7f40cefd7af0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: invalid syntax (tmpla9lxp2j.py, line 13)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function prepare_inputs at 0x7f40cefd7af0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: invalid syntax (tmpla9lxp2j.py, line 13)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:Entity <function _yield_value at 0x7f40d21588b0> appears to be a generator function. It will not be converted by AutoGraph.\n",
      "WARNING: Entity <function _yield_value at 0x7f40d21588b0> appears to be a generator function. It will not be converted by AutoGraph.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_path = '/tmp/datasets'\n",
    "\n",
    "# Loads the metadata of the dataset.\n",
    "metadata = _read_metadata(data_path)\n",
    "#Create a tf.data.Dataset from the TFRecord.\n",
    "\n",
    "ds = tf.data.TFRecordDataset([os.path.join(data_path, 'train.tfrecord')])\n",
    "ds = ds.map(functools.partial(reading_utils.parse_serialized_simulation_example, metadata=metadata))\n",
    "# Splits an entire trajectory into chunks of 7 steps.\n",
    "# Previous 5 velocities, current velocity and target.\n",
    "split_with_window = functools.partial(\n",
    "    reading_utils.split_trajectory,\n",
    "    window_length=INPUT_SEQUENCE_LENGTH + 1)\n",
    "ds = ds.flat_map(split_with_window)\n",
    "# Splits a chunk into input steps and target steps\n",
    "ds = ds.map(prepare_inputs)\n",
    "# If in train mode, repeat dataset forever and shuffle.\n",
    "ds = ds.repeat()\n",
    "ds = ds.shuffle(512)\n",
    "# Custom batching on the leading axis.\n",
    "ds = batch_concat(ds, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a10819f3-4431-4a67-845b-b3499b488991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-12 17:34:28.593657: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-10-12 17:34:28.613138: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3299990000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'particle_type': array([5, 5, 5, ..., 5, 5, 5]), 'position': array([[[0.7813939 , 0.1687931 ],\n",
      "        [0.7813903 , 0.16892076],\n",
      "        [0.7813452 , 0.16894704],\n",
      "        [0.7813512 , 0.16882114],\n",
      "        [0.7814686 , 0.16851303],\n",
      "        [0.78166634, 0.16801745]],\n",
      "\n",
      "       [[0.7599475 , 0.16612233],\n",
      "        [0.75968677, 0.1659991 ],\n",
      "        [0.75944555, 0.1657685 ],\n",
      "        [0.7593131 , 0.16538875],\n",
      "        [0.75928825, 0.16482893],\n",
      "        [0.7592968 , 0.16407833]],\n",
      "\n",
      "       [[0.760049  , 0.16113883],\n",
      "        [0.7598054 , 0.16108072],\n",
      "        [0.7595709 , 0.16093822],\n",
      "        [0.75943136, 0.16062841],\n",
      "        [0.7593891 , 0.16010247],\n",
      "        [0.7593813 , 0.15936947]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.13055547, 0.22743459],\n",
      "        [0.13101469, 0.22861966],\n",
      "        [0.13148275, 0.22974853],\n",
      "        [0.13195878, 0.23081811],\n",
      "        [0.13243766, 0.23182389],\n",
      "        [0.13291474, 0.23276502]],\n",
      "\n",
      "       [[0.2745001 , 0.12609279],\n",
      "        [0.27160683, 0.1259225 ],\n",
      "        [0.26871043, 0.12575364],\n",
      "        [0.26581138, 0.1256107 ],\n",
      "        [0.2629117 , 0.12545495],\n",
      "        [0.2600105 , 0.12527406]],\n",
      "\n",
      "       [[0.88648427, 0.19880651],\n",
      "        [0.8864657 , 0.1937169 ],\n",
      "        [0.8864472 , 0.18856594],\n",
      "        [0.88642865, 0.18335363],\n",
      "        [0.8864101 , 0.17808002],\n",
      "        [0.8863896 , 0.17277339]]], dtype=float32), 'n_particles_per_example': array([678, 678], dtype=int32)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-12 17:34:29.323316: W tensorflow/core/framework/dataset.cc:477] Input of WindowDataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    }
   ],
   "source": [
    "for databatch in ds.as_numpy_iterator():\n",
    "    print(databatch[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14cb6ad8-950e-48e6-8d65-a080c6d4a277",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_std=6.7e-4\n",
    "latent_size=128\n",
    "hidden_size=128\n",
    "hidden_layers=2\n",
    "message_passing_steps=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85213a69-192d-47c4-8e60-86a7114e3ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Gets one step model for training simulation.\"\"\"\n",
    "metadata = _read_metadata(data_path)\n",
    "model_kwargs = dict(\n",
    "      latent_size=latent_size,\n",
    "      mlp_hidden_size=hidden_size,\n",
    "      mlp_num_hidden_layers=hidden_layers,\n",
    "      num_message_passing_steps=message_passing_steps)\n",
    "def _combine_std(std_x, std_y):\n",
    "  return np.sqrt(std_x**2 + std_y**2)\n",
    "\n",
    "Stats = collections.namedtuple('Stats', ['mean', 'std'])\n",
    "vel_noise_std=noise_std\n",
    "acc_noise_std=noise_std\n",
    "\"\"\"Instantiates the simulator.\"\"\"\n",
    "# Cast statistics to numpy so they are arrays when entering the model.\n",
    "cast = lambda v: np.array(v, dtype=np.float32)\n",
    "acceleration_stats = Stats(cast(metadata['acc_mean']), _combine_std(cast(metadata['acc_std']), acc_noise_std))\n",
    "velocity_stats = Stats(cast(metadata['vel_mean']),_combine_std(cast(metadata['vel_std']), vel_noise_std))\n",
    "normalization_stats = {'acceleration': acceleration_stats, 'velocity': velocity_stats}\n",
    "\n",
    "\n",
    "\n",
    "if 'context_mean' in metadata:\n",
    "    context_stats = Stats(cast(metadata['context_mean']), cast(metadata['context_std']))\n",
    "    normalization_stats['context'] = context_stats\n",
    "\n",
    "\n",
    "simulator = learned_simulator.LearnedSimulator(\n",
    "      num_dimensions=metadata['dim'],\n",
    "      connectivity_radius=metadata['default_connectivity_radius'],\n",
    "      graph_network_kwargs=model_kwargs,\n",
    "      boundaries=metadata['bounds'],\n",
    "      num_particle_types=NUM_PARTICLE_TYPES,\n",
    "      normalization_stats=normalization_stats,\n",
    "      particle_type_embedding_size=16)\n",
    "\n",
    "\n",
    "#   def estimator_fn(features, labels, mode):\n",
    "#     target_next_position = labels\n",
    "#     # Sample the noise to add to the inputs to the model during training.\n",
    "#     sampled_noise = noise_utils.get_random_walk_noise_for_position_sequence(\n",
    "#         features['position'], noise_std_last_step=noise_std)\n",
    "#     non_kinematic_mask = tf.logical_not(\n",
    "#         get_kinematic_mask(features['particle_type']))\n",
    "#     noise_mask = tf.cast(\n",
    "#         non_kinematic_mask, sampled_noise.dtype)[:, tf.newaxis, tf.newaxis]\n",
    "#     sampled_noise *= noise_mask\n",
    "\n",
    "#     # Get the predictions and target accelerations.\n",
    "#     pred_target = simulator.get_predicted_and_target_normalized_accelerations(\n",
    "#         next_position=target_next_position,\n",
    "#         position_sequence=features['position'],\n",
    "#         position_sequence_noise=sampled_noise,\n",
    "#         n_particles_per_example=features['n_particles_per_example'],\n",
    "#         particle_types=features['particle_type'],\n",
    "#         global_context=features.get('step_context'))\n",
    "#     pred_acceleration, target_acceleration = pred_target\n",
    "\n",
    "#     # Calculate the loss and mask out loss on kinematic particles/\n",
    "#     loss = (pred_acceleration - target_acceleration)**2\n",
    "\n",
    "#     num_non_kinematic = tf.reduce_sum(\n",
    "#         tf.cast(non_kinematic_mask, tf.float32))\n",
    "#     loss = tf.where(non_kinematic_mask, loss, tf.zeros_like(loss))\n",
    "#     loss = tf.reduce_sum(loss) / tf.reduce_sum(num_non_kinematic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c5faa50-2a08-4eef-8381-9f7bf365651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_positions = databatch[0]['position'][:, 0:INPUT_SEQUENCE_LENGTH]\n",
    "n_particles_per_example=databatch[0]['n_particles_per_example'],\n",
    "particle_types=databatch[0]['particle_type']\n",
    "global_context_step = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e540f52-d016-4df3-857a-621d3358e526",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'get_shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_41779/4128319908.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m simulator(initial_positions,\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0mn_particles_per_example\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_particles_per_example\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mparticle_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparticle_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         global_context=global_context_step)\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/sonnet/src/utils.py\u001b[0m in \u001b[0;36m_decorate_unbound_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0m_decorate_unbound_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mbound_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=attribute-error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_decorate_unbound_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/sonnet/src/base.py\u001b[0m in \u001b[0;36mwrap_with_name_scope\u001b[0;34m(method, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;31m# snt.Module enters the module name scope for all methods. To disable this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;31m# for a particular method annotate it with `@snt.no_name_scope`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/learning-to-simulate-tf2/learned_simulator.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, position_sequence, n_particles_per_example, global_context, particle_types)\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mstep\u001b[0m \u001b[0minto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m     input_graphs_tuple = self._encoder_preprocessor(\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mposition_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_particles_per_example\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         particle_types)\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/sonnet/src/utils.py\u001b[0m in \u001b[0;36m_decorate_unbound_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0m_decorate_unbound_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mbound_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=attribute-error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_decorate_unbound_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/sonnet/src/base.py\u001b[0m in \u001b[0;36mwrap_with_name_scope\u001b[0;34m(method, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;31m# snt.Module enters the module name scope for all methods. To disable this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;31m# for a particular method annotate it with `@snt.no_name_scope`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/learning-to-simulate-tf2/learned_simulator.py\u001b[0m in \u001b[0;36m_encoder_preprocessor\u001b[0;34m(self, position_sequence, n_node, global_context, particle_types)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;31m# Get connectivity of the graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     (senders, receivers, n_edge\n\u001b[0;32m--> 126\u001b[0;31m      \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnectivity_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_connectivity_for_batch_pyfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m          most_recent_position, n_node, self._connectivity_radius)\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/learning-to-simulate-tf2/connectivity_utils.py\u001b[0m in \u001b[0;36mcompute_connectivity_for_batch_pyfunc\u001b[0;34m(positions, n_node, radius, add_self_edges)\u001b[0m\n\u001b[1;32m    116\u001b[0m   \u001b[0msenders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m   \u001b[0mreceivers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m   \u001b[0mn_edge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msenders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreceivers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'get_shape'"
     ]
    }
   ],
   "source": [
    "simulator(initial_positions,\n",
    "        n_particles_per_example=n_particles_per_example,\n",
    "        particle_types=particle_types,\n",
    "        global_context=global_context_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916a58cf-ad67-4e55-b402-4cb027b315e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
